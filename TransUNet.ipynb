{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hlCsd3C1SmI"
      },
      "source": [
        "#Checking GPU\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cf_OYQnXvnA",
        "outputId": "3edb3fa6-d41a-48f1-f86b-a7438bf2e76b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Apr 16 13:34:28 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    45W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yc1EpW6qEsbR"
      },
      "source": [
        "# Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEhw2hVrsun9"
      },
      "source": [
        "Clone the [TransUNet Repository](https://github.com/Beckschen/TransUNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WHlssUCMEwNX",
        "outputId": "f756c299-3817-47d0-997c-899d41ea5a39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'TransUNet'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 95 (delta 47), reused 41 (delta 41), pack-reused 36\u001b[K\n",
            "Unpacking objects: 100% (95/95), 37.43 KiB | 580.00 KiB/s, done.\n",
            "/content/TransUNet\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Beckschen/TransUNet\n",
        "%cd TransUNet\n",
        "# !pip install -r requirements.txt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEi-XXcAs7UG"
      },
      "source": [
        "Download the Google ViT model (R50-ViT-B_16.npz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrP7dV3XNQZi",
        "outputId": "6c97cea0-08ef-4860-aa0a-c916d68963b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-04-13 16:04:19--  https://storage.googleapis.com/vit_models/imagenet21k/R50%2BViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.197.128, 74.125.135.128, 74.125.142.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.197.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 461217452 (440M) [application/octet-stream]\n",
            "Saving to: ‘R50+ViT-B_16.npz’\n",
            "\n",
            "R50+ViT-B_16.npz    100%[===================>] 439.85M  26.7MB/s    in 19s     \n",
            "\n",
            "2023-04-13 16:04:39 (23.7 MB/s) - ‘R50+ViT-B_16.npz’ saved [461217452/461217452]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://storage.googleapis.com/vit_models/imagenet21k/R50%2BViT-B_16.npz\n",
        "!mkdir ../model\n",
        "!mkdir ../model/vit_checkpoint\n",
        "!mkdir ../model/vit_checkpoint/imagenet21k\n",
        "!mv R50+ViT-B_16.npz ../model/vit_checkpoint/imagenet21k/R50+ViT-B_16.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULbqP3oQtEQ9"
      },
      "source": [
        "Install Python Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugxvsODDIMNT",
        "outputId": "3f62becf-cdf9-4fcc-8870-0170f3edaffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first two lines of the requirements.txt file have been changed to torch and torchvision.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 1)) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 2)) (0.15.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (4.65.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 5)) (2.12.1)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting medpy\n",
            "  Downloading MedPy-0.4.0.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.8/151.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.2.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 10)) (1.10.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 11)) (3.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->-r requirements.txt (line 1)) (3.11.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (16.0.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->-r requirements.txt (line 2)) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.40.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.4.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (2.17.2)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.53.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (67.6.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (3.20.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->-r requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX->-r requirements.txt (line 6)) (23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from ml-collections->-r requirements.txt (line 7)) (6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from ml-collections->-r requirements.txt (line 7)) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.9/dist-packages (from ml-collections->-r requirements.txt (line 7)) (0.6.0.post1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (6.2.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 5)) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 5)) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 5)) (3.2.2)\n",
            "Building wheels for collected packages: ml-collections, medpy\n",
            "  Building wheel for ml-collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml-collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94524 sha256=6ff0c57ca1f957878380c634f72bb8bfac102c13131933f4f7df5b0d6ca5a384\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/c2/0d/5d94d95e5875ea17b85a9f1f99b8dd2e50517137c8042c6468\n",
            "  Building wheel for medpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for medpy: filename=MedPy-0.4.0-py3-none-any.whl size=214964 sha256=0e6b71d9cba99aa7d3909cc442a8be38d6953c460b7a82e8f897a0d5f879e02c\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/46/a2/7c585b78f216a3dd8723dbab5f439822fa5dfbff563757a49e\n",
            "Successfully built ml-collections medpy\n",
            "Installing collected packages: SimpleITK, tensorboardX, ml-collections, medpy\n",
            "Successfully installed SimpleITK-2.2.1 medpy-0.4.0 ml-collections-0.1.1 tensorboardX-2.6\n"
          ]
        }
      ],
      "source": [
        "# Read the first two lines of the requirements.txt file\n",
        "!line1=$(head -n 1 requirements.txt)\n",
        "!line2=$(sed -n '2p' requirements.txt)\n",
        "\n",
        "# Replace the first two lines with \"torch\" and \"torchvision\"\n",
        "!echo \"torch\" > requirements_new.txt\n",
        "!echo \"torchvision\" >> requirements_new.txt\n",
        "!tail -n +3 requirements.txt >> requirements_new.txt\n",
        "\n",
        "# Replace the original file with the new file\n",
        "!mv requirements_new.txt requirements.txt\n",
        "\n",
        "!echo \"The first two lines of the requirements.txt file have been changed to torch and torchvision.\"\n",
        "\n",
        "# install requirements\n",
        "!pip install -r requirements.txt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGs8-aXhtJlV"
      },
      "source": [
        "Load the dataset\n",
        "(Temporarily mounting from google drive)\n",
        "\n",
        "***TODO***: Set up a way to download the dataset from cloud "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXAgP71-T9gv",
        "outputId": "00ae6bb5-4dd9-49c4-b2c9-c4baa6c6f7f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mkdir: cannot create directory ‘../data’: File exists\n"
          ]
        }
      ],
      "source": [
        "# Mound Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir ../data\n",
        "\n",
        "# copy the data\n",
        "!cp -R ../drive/MyDrive/TransUNet-data/Synapse ../data\n",
        "# !rsync -a --progress ../drive/MyDrive/TransUNet-data/Synapse ../data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CB1F8SztalC"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v_Fk5t4MIyL",
        "outputId": "69f68476-9d8a-45b0-d82e-d973dcf6b2f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "iteration 8955 : loss : 0.022905, loss_ce: 0.005969\n",
            "iteration 8956 : loss : 0.029605, loss_ce: 0.009190\n",
            "iteration 8957 : loss : 0.027027, loss_ce: 0.011193\n",
            "iteration 8958 : loss : 0.028598, loss_ce: 0.010382\n",
            "iteration 8959 : loss : 0.027750, loss_ce: 0.010629\n",
            "iteration 8960 : loss : 0.026066, loss_ce: 0.007207\n",
            "iteration 8961 : loss : 0.026133, loss_ce: 0.008321\n",
            "iteration 8962 : loss : 0.025903, loss_ce: 0.008503\n",
            "iteration 8963 : loss : 0.030186, loss_ce: 0.010520\n",
            "iteration 8964 : loss : 0.029688, loss_ce: 0.009437\n",
            "iteration 8965 : loss : 0.022159, loss_ce: 0.006415\n",
            "iteration 8966 : loss : 0.030223, loss_ce: 0.012246\n",
            "iteration 8967 : loss : 0.022752, loss_ce: 0.006519\n",
            "iteration 8968 : loss : 0.024407, loss_ce: 0.009096\n",
            "iteration 8969 : loss : 0.023917, loss_ce: 0.010202\n",
            "iteration 8970 : loss : 0.025758, loss_ce: 0.006163\n",
            "iteration 8971 : loss : 0.028281, loss_ce: 0.011642\n",
            "iteration 8972 : loss : 0.029649, loss_ce: 0.009857\n",
            "iteration 8973 : loss : 0.027338, loss_ce: 0.011057\n",
            "iteration 8974 : loss : 0.029301, loss_ce: 0.013483\n",
            "iteration 8975 : loss : 0.022030, loss_ce: 0.008178\n",
            "iteration 8976 : loss : 0.023451, loss_ce: 0.007237\n",
            "iteration 8977 : loss : 0.034544, loss_ce: 0.010493\n",
            "iteration 8978 : loss : 0.023743, loss_ce: 0.008279\n",
            "iteration 8979 : loss : 0.035330, loss_ce: 0.007639\n",
            "iteration 8980 : loss : 0.030762, loss_ce: 0.012296\n",
            "iteration 8981 : loss : 0.031423, loss_ce: 0.006145\n",
            "iteration 8982 : loss : 0.044717, loss_ce: 0.008132\n",
            "iteration 8983 : loss : 0.074571, loss_ce: 0.004696\n",
            "iteration 8984 : loss : 0.030336, loss_ce: 0.013298\n",
            "iteration 8985 : loss : 0.025839, loss_ce: 0.007918\n",
            "iteration 8986 : loss : 0.022970, loss_ce: 0.005809\n",
            "iteration 8987 : loss : 0.027113, loss_ce: 0.008012\n",
            "iteration 8988 : loss : 0.022653, loss_ce: 0.009177\n",
            "iteration 8989 : loss : 0.027230, loss_ce: 0.007690\n",
            "iteration 8990 : loss : 0.038199, loss_ce: 0.011829\n",
            "iteration 8991 : loss : 0.023313, loss_ce: 0.008399\n",
            "iteration 8992 : loss : 0.020908, loss_ce: 0.007226\n",
            "iteration 8993 : loss : 0.026231, loss_ce: 0.008998\n",
            "iteration 8994 : loss : 0.022757, loss_ce: 0.005855\n",
            "iteration 8995 : loss : 0.019468, loss_ce: 0.005188\n",
            "iteration 8996 : loss : 0.029196, loss_ce: 0.008647\n",
            "iteration 8997 : loss : 0.025215, loss_ce: 0.012948\n",
            "iteration 8998 : loss : 0.023185, loss_ce: 0.005995\n",
            "iteration 8999 : loss : 0.023929, loss_ce: 0.003888\n",
            "iteration 9000 : loss : 0.024587, loss_ce: 0.006844\n",
            "iteration 9001 : loss : 0.023589, loss_ce: 0.008855\n",
            "iteration 9002 : loss : 0.024331, loss_ce: 0.011667\n",
            "iteration 9003 : loss : 0.030411, loss_ce: 0.013767\n",
            "iteration 9004 : loss : 0.026329, loss_ce: 0.004811\n",
            "iteration 9005 : loss : 0.026804, loss_ce: 0.011081\n",
            "iteration 9006 : loss : 0.088294, loss_ce: 0.004849\n",
            "iteration 9007 : loss : 0.027265, loss_ce: 0.007785\n",
            "iteration 9008 : loss : 0.075204, loss_ce: 0.005179\n",
            "iteration 9009 : loss : 0.052915, loss_ce: 0.005367\n",
            "iteration 9010 : loss : 0.020801, loss_ce: 0.009525\n",
            "iteration 9011 : loss : 0.026220, loss_ce: 0.012204\n",
            "iteration 9012 : loss : 0.030141, loss_ce: 0.008704\n",
            "iteration 9013 : loss : 0.026899, loss_ce: 0.009214\n",
            "iteration 9014 : loss : 0.032062, loss_ce: 0.016429\n",
            "iteration 9015 : loss : 0.026601, loss_ce: 0.010177\n",
            "iteration 9016 : loss : 0.026530, loss_ce: 0.007742\n",
            "iteration 9017 : loss : 0.024293, loss_ce: 0.011098\n",
            "iteration 9018 : loss : 0.028355, loss_ce: 0.014851\n",
            "iteration 9019 : loss : 0.025848, loss_ce: 0.010666\n",
            "iteration 9020 : loss : 0.020454, loss_ce: 0.005485\n",
            "iteration 9021 : loss : 0.036400, loss_ce: 0.019422\n",
            " 65%|█████████████████▍         | 97/150 [3:32:23<1:55:31, 130.79s/it]iteration 9022 : loss : 0.020760, loss_ce: 0.008939\n",
            "iteration 9023 : loss : 0.031336, loss_ce: 0.009575\n",
            "iteration 9024 : loss : 0.024229, loss_ce: 0.007010\n",
            "iteration 9025 : loss : 0.026619, loss_ce: 0.007148\n",
            "iteration 9026 : loss : 0.032058, loss_ce: 0.007442\n",
            "iteration 9027 : loss : 0.028925, loss_ce: 0.013737\n",
            "iteration 9028 : loss : 0.026779, loss_ce: 0.007447\n",
            "iteration 9029 : loss : 0.023347, loss_ce: 0.011759\n",
            "iteration 9030 : loss : 0.024358, loss_ce: 0.008459\n",
            "iteration 9031 : loss : 0.024413, loss_ce: 0.007631\n",
            "iteration 9032 : loss : 0.024490, loss_ce: 0.006527\n",
            "iteration 9033 : loss : 0.029736, loss_ce: 0.005081\n",
            "iteration 9034 : loss : 0.026696, loss_ce: 0.011155\n",
            "iteration 9035 : loss : 0.028673, loss_ce: 0.006628\n",
            "iteration 9036 : loss : 0.027293, loss_ce: 0.008765\n",
            "iteration 9037 : loss : 0.024280, loss_ce: 0.008970\n",
            "iteration 9038 : loss : 0.024841, loss_ce: 0.007431\n",
            "iteration 9039 : loss : 0.028270, loss_ce: 0.010428\n",
            "iteration 9040 : loss : 0.025968, loss_ce: 0.010484\n",
            "iteration 9041 : loss : 0.027125, loss_ce: 0.006533\n",
            "iteration 9042 : loss : 0.024830, loss_ce: 0.011809\n",
            "iteration 9043 : loss : 0.024005, loss_ce: 0.007939\n",
            "iteration 9044 : loss : 0.039970, loss_ce: 0.010258\n",
            "iteration 9045 : loss : 0.029376, loss_ce: 0.008178\n",
            "iteration 9046 : loss : 0.079148, loss_ce: 0.012847\n",
            "iteration 9047 : loss : 0.021694, loss_ce: 0.006820\n",
            "iteration 9048 : loss : 0.024644, loss_ce: 0.006855\n",
            "iteration 9049 : loss : 0.027426, loss_ce: 0.007377\n",
            "iteration 9050 : loss : 0.029225, loss_ce: 0.011856\n",
            "iteration 9051 : loss : 0.027413, loss_ce: 0.009456\n",
            "iteration 9052 : loss : 0.025994, loss_ce: 0.007462\n",
            "iteration 9053 : loss : 0.027437, loss_ce: 0.008969\n",
            "iteration 9054 : loss : 0.022132, loss_ce: 0.008910\n",
            "iteration 9055 : loss : 0.028918, loss_ce: 0.006183\n",
            "iteration 9056 : loss : 0.025499, loss_ce: 0.009495\n",
            "iteration 9057 : loss : 0.021873, loss_ce: 0.007208\n",
            "iteration 9058 : loss : 0.026068, loss_ce: 0.005750\n",
            "iteration 9059 : loss : 0.022305, loss_ce: 0.006662\n",
            "iteration 9060 : loss : 0.023851, loss_ce: 0.011075\n",
            "iteration 9061 : loss : 0.023919, loss_ce: 0.008547\n",
            "iteration 9062 : loss : 0.028668, loss_ce: 0.007132\n",
            "iteration 9063 : loss : 0.026436, loss_ce: 0.008682\n",
            "iteration 9064 : loss : 0.025345, loss_ce: 0.007784\n",
            "iteration 9065 : loss : 0.025056, loss_ce: 0.005879\n",
            "iteration 9066 : loss : 0.023828, loss_ce: 0.007533\n",
            "iteration 9067 : loss : 0.028071, loss_ce: 0.010393\n",
            "iteration 9068 : loss : 0.023808, loss_ce: 0.009380\n",
            "iteration 9069 : loss : 0.027905, loss_ce: 0.008679\n",
            "iteration 9070 : loss : 0.025346, loss_ce: 0.006236\n",
            "iteration 9071 : loss : 0.027136, loss_ce: 0.011338\n",
            "iteration 9072 : loss : 0.080929, loss_ce: 0.010598\n",
            "iteration 9073 : loss : 0.027631, loss_ce: 0.009567\n",
            "iteration 9074 : loss : 0.044202, loss_ce: 0.010353\n",
            "iteration 9075 : loss : 0.029832, loss_ce: 0.011520\n",
            "iteration 9076 : loss : 0.023297, loss_ce: 0.008025\n",
            "iteration 9077 : loss : 0.029748, loss_ce: 0.009781\n",
            "iteration 9078 : loss : 0.022513, loss_ce: 0.006739\n",
            "iteration 9079 : loss : 0.026386, loss_ce: 0.011241\n",
            "iteration 9080 : loss : 0.078563, loss_ce: 0.004661\n",
            "iteration 9081 : loss : 0.020503, loss_ce: 0.008261\n",
            "iteration 9082 : loss : 0.028221, loss_ce: 0.010143\n",
            "iteration 9083 : loss : 0.029815, loss_ce: 0.017672\n",
            "iteration 9084 : loss : 0.024813, loss_ce: 0.008228\n",
            "iteration 9085 : loss : 0.028427, loss_ce: 0.007385\n",
            "iteration 9086 : loss : 0.025881, loss_ce: 0.007969\n",
            "iteration 9087 : loss : 0.024649, loss_ce: 0.010594\n",
            "iteration 9088 : loss : 0.024016, loss_ce: 0.007166\n",
            "iteration 9089 : loss : 0.027687, loss_ce: 0.004843\n",
            "iteration 9090 : loss : 0.023956, loss_ce: 0.008743\n",
            "iteration 9091 : loss : 0.023655, loss_ce: 0.011156\n",
            "iteration 9092 : loss : 0.019503, loss_ce: 0.005663\n",
            "iteration 9093 : loss : 0.035112, loss_ce: 0.008243\n",
            "iteration 9094 : loss : 0.029698, loss_ce: 0.010207\n",
            "iteration 9095 : loss : 0.029860, loss_ce: 0.008833\n",
            "iteration 9096 : loss : 0.022158, loss_ce: 0.007751\n",
            "iteration 9097 : loss : 0.029057, loss_ce: 0.014367\n",
            "iteration 9098 : loss : 0.028583, loss_ce: 0.012094\n",
            "iteration 9099 : loss : 0.024718, loss_ce: 0.007932\n",
            "iteration 9100 : loss : 0.029246, loss_ce: 0.007128\n",
            "iteration 9101 : loss : 0.026176, loss_ce: 0.009148\n",
            "iteration 9102 : loss : 0.075750, loss_ce: 0.003597\n",
            "iteration 9103 : loss : 0.022233, loss_ce: 0.008743\n",
            "iteration 9104 : loss : 0.072225, loss_ce: 0.011696\n",
            "iteration 9105 : loss : 0.026170, loss_ce: 0.009619\n",
            "iteration 9106 : loss : 0.028815, loss_ce: 0.015259\n",
            "iteration 9107 : loss : 0.025203, loss_ce: 0.006376\n",
            "iteration 9108 : loss : 0.035974, loss_ce: 0.016798\n",
            "iteration 9109 : loss : 0.026364, loss_ce: 0.005058\n",
            "iteration 9110 : loss : 0.025344, loss_ce: 0.008829\n",
            "iteration 9111 : loss : 0.024524, loss_ce: 0.009172\n",
            "iteration 9112 : loss : 0.025957, loss_ce: 0.009694\n",
            "iteration 9113 : loss : 0.026563, loss_ce: 0.004426\n",
            "iteration 9114 : loss : 0.390583, loss_ce: 0.000539\n",
            " 65%|█████████████████▋         | 98/150 [3:34:33<1:53:10, 130.59s/it]iteration 9115 : loss : 0.027087, loss_ce: 0.007329\n",
            "iteration 9116 : loss : 0.029386, loss_ce: 0.012164\n",
            "iteration 9117 : loss : 0.024764, loss_ce: 0.004866\n",
            "iteration 9118 : loss : 0.027273, loss_ce: 0.012068\n",
            "iteration 9119 : loss : 0.034752, loss_ce: 0.011243\n",
            "iteration 9120 : loss : 0.031086, loss_ce: 0.006485\n",
            "iteration 9121 : loss : 0.029562, loss_ce: 0.011304\n",
            "iteration 9122 : loss : 0.030507, loss_ce: 0.007494\n",
            "iteration 9123 : loss : 0.023911, loss_ce: 0.010604\n",
            "iteration 9124 : loss : 0.030398, loss_ce: 0.007940\n",
            "iteration 9125 : loss : 0.024766, loss_ce: 0.009857\n",
            "iteration 9126 : loss : 0.023903, loss_ce: 0.007701\n",
            "iteration 9127 : loss : 0.027973, loss_ce: 0.010270\n",
            "iteration 9128 : loss : 0.022936, loss_ce: 0.006061\n",
            "iteration 9129 : loss : 0.025590, loss_ce: 0.011039\n",
            "iteration 9130 : loss : 0.030337, loss_ce: 0.005594\n",
            "iteration 9131 : loss : 0.025709, loss_ce: 0.012884\n",
            "iteration 9132 : loss : 0.028798, loss_ce: 0.015107\n",
            "iteration 9133 : loss : 0.022821, loss_ce: 0.008002\n",
            "iteration 9134 : loss : 0.027737, loss_ce: 0.008602\n",
            "iteration 9135 : loss : 0.074827, loss_ce: 0.004150\n",
            "iteration 9136 : loss : 0.025435, loss_ce: 0.012114\n",
            "iteration 9137 : loss : 0.027642, loss_ce: 0.011003\n",
            "iteration 9138 : loss : 0.028694, loss_ce: 0.013718\n",
            "iteration 9139 : loss : 0.037326, loss_ce: 0.008460\n",
            "iteration 9140 : loss : 0.027942, loss_ce: 0.010249\n",
            "iteration 9141 : loss : 0.026244, loss_ce: 0.009815\n",
            "iteration 9142 : loss : 0.074046, loss_ce: 0.006149\n",
            "iteration 9143 : loss : 0.022289, loss_ce: 0.007904\n",
            "iteration 9144 : loss : 0.024126, loss_ce: 0.008554\n",
            "iteration 9145 : loss : 0.030628, loss_ce: 0.009653\n",
            "iteration 9146 : loss : 0.025619, loss_ce: 0.010422\n",
            "iteration 9147 : loss : 0.021397, loss_ce: 0.006613\n",
            "iteration 9148 : loss : 0.030511, loss_ce: 0.005426\n",
            "iteration 9149 : loss : 0.023938, loss_ce: 0.009275\n",
            "iteration 9150 : loss : 0.087010, loss_ce: 0.004535\n",
            "iteration 9151 : loss : 0.026285, loss_ce: 0.007622\n",
            "iteration 9152 : loss : 0.043573, loss_ce: 0.007567\n",
            "iteration 9153 : loss : 0.022645, loss_ce: 0.006587\n",
            "iteration 9154 : loss : 0.032587, loss_ce: 0.006401\n",
            "iteration 9155 : loss : 0.025639, loss_ce: 0.009070\n",
            "iteration 9156 : loss : 0.028708, loss_ce: 0.015449\n",
            "iteration 9157 : loss : 0.031519, loss_ce: 0.007993\n",
            "iteration 9158 : loss : 0.023175, loss_ce: 0.008634\n",
            "iteration 9159 : loss : 0.027721, loss_ce: 0.011198\n",
            "iteration 9160 : loss : 0.026843, loss_ce: 0.010871\n",
            "iteration 9161 : loss : 0.083003, loss_ce: 0.006925\n",
            "iteration 9162 : loss : 0.024608, loss_ce: 0.007751\n",
            "iteration 9163 : loss : 0.031701, loss_ce: 0.011203\n",
            "iteration 9164 : loss : 0.023792, loss_ce: 0.009437\n",
            "iteration 9165 : loss : 0.027323, loss_ce: 0.010140\n",
            "iteration 9166 : loss : 0.048403, loss_ce: 0.004532\n",
            "iteration 9167 : loss : 0.023495, loss_ce: 0.006810\n",
            "iteration 9168 : loss : 0.074663, loss_ce: 0.007051\n",
            "iteration 9169 : loss : 0.036819, loss_ce: 0.011188\n",
            "iteration 9170 : loss : 0.027415, loss_ce: 0.006948\n",
            "iteration 9171 : loss : 0.026833, loss_ce: 0.012355\n",
            "iteration 9172 : loss : 0.023798, loss_ce: 0.006774\n",
            "iteration 9173 : loss : 0.025687, loss_ce: 0.006702\n",
            "iteration 9174 : loss : 0.027571, loss_ce: 0.005768\n",
            "iteration 9175 : loss : 0.030195, loss_ce: 0.008019\n",
            "iteration 9176 : loss : 0.028020, loss_ce: 0.010343\n",
            "iteration 9177 : loss : 0.028397, loss_ce: 0.010544\n",
            "iteration 9178 : loss : 0.026120, loss_ce: 0.007790\n",
            "iteration 9179 : loss : 0.033009, loss_ce: 0.010930\n",
            "iteration 9180 : loss : 0.028171, loss_ce: 0.011027\n",
            "iteration 9181 : loss : 0.156751, loss_ce: 0.003934\n",
            "iteration 9182 : loss : 0.041207, loss_ce: 0.008262\n",
            "iteration 9183 : loss : 0.030354, loss_ce: 0.013614\n",
            "iteration 9184 : loss : 0.076317, loss_ce: 0.005503\n",
            "iteration 9185 : loss : 0.023376, loss_ce: 0.009991\n",
            "iteration 9186 : loss : 0.028953, loss_ce: 0.011563\n",
            "iteration 9187 : loss : 0.055781, loss_ce: 0.007449\n",
            "iteration 9188 : loss : 0.028636, loss_ce: 0.009808\n",
            "iteration 9189 : loss : 0.023962, loss_ce: 0.008304\n",
            "iteration 9190 : loss : 0.032110, loss_ce: 0.014186\n",
            "iteration 9191 : loss : 0.028692, loss_ce: 0.008045\n",
            "iteration 9192 : loss : 0.027881, loss_ce: 0.011586\n",
            "iteration 9193 : loss : 0.025986, loss_ce: 0.008006\n",
            "iteration 9194 : loss : 0.030813, loss_ce: 0.010230\n",
            "iteration 9195 : loss : 0.029221, loss_ce: 0.010775\n",
            "iteration 9196 : loss : 0.068968, loss_ce: 0.008177\n",
            "iteration 9197 : loss : 0.028964, loss_ce: 0.010014\n",
            "iteration 9198 : loss : 0.025692, loss_ce: 0.006409\n",
            "iteration 9199 : loss : 0.075948, loss_ce: 0.008769\n",
            "iteration 9200 : loss : 0.032317, loss_ce: 0.008751\n",
            "iteration 9201 : loss : 0.030139, loss_ce: 0.012349\n",
            "iteration 9202 : loss : 0.025783, loss_ce: 0.012128\n",
            "iteration 9203 : loss : 0.021856, loss_ce: 0.007937\n",
            "iteration 9204 : loss : 0.026004, loss_ce: 0.007788\n",
            "iteration 9205 : loss : 0.043176, loss_ce: 0.010264\n",
            "iteration 9206 : loss : 0.026270, loss_ce: 0.007197\n",
            "iteration 9207 : loss : 0.149354, loss_ce: 0.005227\n",
            " 66%|█████████████████▊         | 99/150 [3:36:43<1:50:58, 130.55s/it]iteration 9208 : loss : 0.086149, loss_ce: 0.007045\n",
            "iteration 9209 : loss : 0.026853, loss_ce: 0.009283\n",
            "iteration 9210 : loss : 0.027230, loss_ce: 0.007608\n",
            "iteration 9211 : loss : 0.024336, loss_ce: 0.010895\n",
            "iteration 9212 : loss : 0.028229, loss_ce: 0.010873\n",
            "iteration 9213 : loss : 0.035759, loss_ce: 0.012065\n",
            "iteration 9214 : loss : 0.030928, loss_ce: 0.012807\n",
            "iteration 9215 : loss : 0.031860, loss_ce: 0.009755\n",
            "iteration 9216 : loss : 0.026278, loss_ce: 0.008375\n",
            "iteration 9217 : loss : 0.035491, loss_ce: 0.011000\n",
            "iteration 9218 : loss : 0.027367, loss_ce: 0.010631\n",
            "iteration 9219 : loss : 0.032297, loss_ce: 0.009922\n",
            "iteration 9220 : loss : 0.030405, loss_ce: 0.011884\n",
            "iteration 9221 : loss : 0.159679, loss_ce: 0.006392\n",
            "iteration 9222 : loss : 0.078764, loss_ce: 0.009443\n",
            "iteration 9223 : loss : 0.026569, loss_ce: 0.009522\n",
            "iteration 9224 : loss : 0.034672, loss_ce: 0.010665\n",
            "iteration 9225 : loss : 0.025193, loss_ce: 0.010831\n",
            "iteration 9226 : loss : 0.028444, loss_ce: 0.009137\n",
            "iteration 9227 : loss : 0.027114, loss_ce: 0.010453\n",
            "iteration 9228 : loss : 0.027902, loss_ce: 0.012795\n",
            "iteration 9229 : loss : 0.023183, loss_ce: 0.009980\n",
            "iteration 9230 : loss : 0.025681, loss_ce: 0.010677\n",
            "iteration 9231 : loss : 0.025366, loss_ce: 0.007872\n",
            "iteration 9232 : loss : 0.031430, loss_ce: 0.012309\n",
            "iteration 9233 : loss : 0.035220, loss_ce: 0.013305\n",
            "iteration 9234 : loss : 0.026686, loss_ce: 0.010758\n",
            "iteration 9235 : loss : 0.031523, loss_ce: 0.007742\n",
            "iteration 9236 : loss : 0.027124, loss_ce: 0.015373\n",
            "iteration 9237 : loss : 0.021368, loss_ce: 0.008278\n",
            "iteration 9238 : loss : 0.028410, loss_ce: 0.013328\n",
            "iteration 9239 : loss : 0.025186, loss_ce: 0.010649\n",
            "iteration 9240 : loss : 0.028781, loss_ce: 0.013069\n",
            "iteration 9241 : loss : 0.029304, loss_ce: 0.010208\n",
            "iteration 9242 : loss : 0.029130, loss_ce: 0.006563\n",
            "iteration 9243 : loss : 0.025690, loss_ce: 0.012594\n",
            "iteration 9244 : loss : 0.029978, loss_ce: 0.010507\n",
            "iteration 9245 : loss : 0.075933, loss_ce: 0.005642\n",
            "iteration 9246 : loss : 0.023923, loss_ce: 0.009325\n",
            "iteration 9247 : loss : 0.026479, loss_ce: 0.009044\n",
            "iteration 9248 : loss : 0.035421, loss_ce: 0.009316\n",
            "iteration 9249 : loss : 0.077946, loss_ce: 0.007487\n",
            "iteration 9250 : loss : 0.019044, loss_ce: 0.007626\n",
            "iteration 9251 : loss : 0.030210, loss_ce: 0.011157\n",
            "iteration 9252 : loss : 0.024409, loss_ce: 0.008125\n",
            "iteration 9253 : loss : 0.027245, loss_ce: 0.010719\n",
            "iteration 9254 : loss : 0.079432, loss_ce: 0.007304\n",
            "iteration 9255 : loss : 0.021720, loss_ce: 0.005218\n",
            "iteration 9256 : loss : 0.025357, loss_ce: 0.008407\n",
            "iteration 9257 : loss : 0.034829, loss_ce: 0.009558\n",
            "iteration 9258 : loss : 0.032790, loss_ce: 0.012305\n",
            "iteration 9259 : loss : 0.040941, loss_ce: 0.008435\n",
            "iteration 9260 : loss : 0.024483, loss_ce: 0.010247\n",
            "iteration 9261 : loss : 0.024971, loss_ce: 0.005605\n",
            "iteration 9262 : loss : 0.021539, loss_ce: 0.004351\n",
            "iteration 9263 : loss : 0.029177, loss_ce: 0.007613\n",
            "iteration 9264 : loss : 0.025695, loss_ce: 0.005120\n",
            "iteration 9265 : loss : 0.034222, loss_ce: 0.011150\n",
            "iteration 9266 : loss : 0.022003, loss_ce: 0.005975\n",
            "iteration 9267 : loss : 0.027200, loss_ce: 0.007714\n",
            "iteration 9268 : loss : 0.024210, loss_ce: 0.006116\n",
            "iteration 9269 : loss : 0.024992, loss_ce: 0.009602\n",
            "iteration 9270 : loss : 0.024322, loss_ce: 0.009158\n",
            "iteration 9271 : loss : 0.032865, loss_ce: 0.011552\n",
            "iteration 9272 : loss : 0.027504, loss_ce: 0.004538\n",
            "iteration 9273 : loss : 0.044010, loss_ce: 0.006926\n",
            "iteration 9274 : loss : 0.025054, loss_ce: 0.007000\n",
            "iteration 9275 : loss : 0.023771, loss_ce: 0.008124\n",
            "iteration 9276 : loss : 0.025482, loss_ce: 0.005944\n",
            "iteration 9277 : loss : 0.025507, loss_ce: 0.012011\n",
            "iteration 9278 : loss : 0.028704, loss_ce: 0.012307\n",
            "iteration 9279 : loss : 0.087097, loss_ce: 0.004305\n",
            "iteration 9280 : loss : 0.025756, loss_ce: 0.008179\n",
            "iteration 9281 : loss : 0.019344, loss_ce: 0.005492\n",
            "iteration 9282 : loss : 0.025575, loss_ce: 0.007870\n",
            "iteration 9283 : loss : 0.023200, loss_ce: 0.006172\n",
            "iteration 9284 : loss : 0.026778, loss_ce: 0.006185\n",
            "iteration 9285 : loss : 0.026624, loss_ce: 0.009589\n",
            "iteration 9286 : loss : 0.026191, loss_ce: 0.010268\n",
            "iteration 9287 : loss : 0.028551, loss_ce: 0.006923\n",
            "iteration 9288 : loss : 0.029212, loss_ce: 0.008621\n",
            "iteration 9289 : loss : 0.024164, loss_ce: 0.010262\n",
            "iteration 9290 : loss : 0.033646, loss_ce: 0.009960\n",
            "iteration 9291 : loss : 0.031741, loss_ce: 0.008102\n",
            "iteration 9292 : loss : 0.026654, loss_ce: 0.006152\n",
            "iteration 9293 : loss : 0.032164, loss_ce: 0.014278\n",
            "iteration 9294 : loss : 0.028559, loss_ce: 0.013140\n",
            "iteration 9295 : loss : 0.023151, loss_ce: 0.006962\n",
            "iteration 9296 : loss : 0.080098, loss_ce: 0.007854\n",
            "iteration 9297 : loss : 0.033268, loss_ce: 0.008345\n",
            "iteration 9298 : loss : 0.022267, loss_ce: 0.009493\n",
            "iteration 9299 : loss : 0.021226, loss_ce: 0.006590\n",
            "iteration 9300 : loss : 0.392101, loss_ce: 0.001323\n",
            "save model to ../model/TU_Synapse224/TU_pretrain_R50-ViT-B_16_skip3_epo150_bs24_224/epoch_99.pth\n",
            " 67%|█████████████████▎        | 100/150 [3:38:57<1:49:30, 131.40s/it]iteration 9301 : loss : 0.027736, loss_ce: 0.011155\n",
            "iteration 9302 : loss : 0.028122, loss_ce: 0.011214\n",
            "iteration 9303 : loss : 0.029452, loss_ce: 0.012015\n",
            "iteration 9304 : loss : 0.022482, loss_ce: 0.006858\n",
            "iteration 9305 : loss : 0.029364, loss_ce: 0.008124\n",
            "iteration 9306 : loss : 0.026106, loss_ce: 0.009163\n",
            "iteration 9307 : loss : 0.026911, loss_ce: 0.012417\n",
            "iteration 9308 : loss : 0.026469, loss_ce: 0.008529\n",
            "iteration 9309 : loss : 0.024665, loss_ce: 0.008534\n",
            "iteration 9310 : loss : 0.028913, loss_ce: 0.013762\n",
            "iteration 9311 : loss : 0.029590, loss_ce: 0.011435\n",
            "iteration 9312 : loss : 0.025548, loss_ce: 0.011922\n",
            "iteration 9313 : loss : 0.035653, loss_ce: 0.004436\n",
            "iteration 9314 : loss : 0.074729, loss_ce: 0.006626\n",
            "iteration 9315 : loss : 0.024565, loss_ce: 0.005461\n",
            "iteration 9316 : loss : 0.017826, loss_ce: 0.006003\n",
            "iteration 9317 : loss : 0.024906, loss_ce: 0.009916\n",
            "iteration 9318 : loss : 0.025183, loss_ce: 0.006846\n",
            "iteration 9319 : loss : 0.027489, loss_ce: 0.009168\n",
            "iteration 9320 : loss : 0.031770, loss_ce: 0.007110\n",
            "iteration 9321 : loss : 0.078756, loss_ce: 0.009111\n",
            "iteration 9322 : loss : 0.029403, loss_ce: 0.012456\n",
            "iteration 9323 : loss : 0.027540, loss_ce: 0.007618\n",
            "iteration 9324 : loss : 0.024592, loss_ce: 0.007944\n",
            "iteration 9325 : loss : 0.028297, loss_ce: 0.012156\n",
            "iteration 9326 : loss : 0.023514, loss_ce: 0.007287\n",
            "iteration 9327 : loss : 0.023447, loss_ce: 0.007622\n",
            "iteration 9328 : loss : 0.027255, loss_ce: 0.011428\n",
            "iteration 9329 : loss : 0.033034, loss_ce: 0.010335\n",
            "iteration 9330 : loss : 0.021101, loss_ce: 0.007639\n",
            "iteration 9331 : loss : 0.078421, loss_ce: 0.009120\n",
            "iteration 9332 : loss : 0.020363, loss_ce: 0.006403\n",
            "iteration 9333 : loss : 0.025578, loss_ce: 0.007000\n",
            "iteration 9334 : loss : 0.021414, loss_ce: 0.006721\n",
            "iteration 9335 : loss : 0.026769, loss_ce: 0.008142\n",
            "iteration 9336 : loss : 0.023345, loss_ce: 0.008226\n",
            "iteration 9337 : loss : 0.024886, loss_ce: 0.010418\n",
            "iteration 9338 : loss : 0.023808, loss_ce: 0.008111\n",
            "iteration 9339 : loss : 0.025247, loss_ce: 0.008748\n",
            "iteration 9340 : loss : 0.075326, loss_ce: 0.007009\n",
            "iteration 9341 : loss : 0.040523, loss_ce: 0.007094\n",
            "iteration 9342 : loss : 0.026183, loss_ce: 0.009269\n",
            "iteration 9343 : loss : 0.026397, loss_ce: 0.012959\n",
            "iteration 9344 : loss : 0.024629, loss_ce: 0.003948\n",
            "iteration 9345 : loss : 0.025047, loss_ce: 0.009439\n",
            "iteration 9346 : loss : 0.024196, loss_ce: 0.007416\n",
            "iteration 9347 : loss : 0.020585, loss_ce: 0.004407\n",
            "iteration 9348 : loss : 0.026203, loss_ce: 0.009203\n",
            "iteration 9349 : loss : 0.031874, loss_ce: 0.008496\n",
            "iteration 9350 : loss : 0.033284, loss_ce: 0.006603\n",
            "iteration 9351 : loss : 0.033385, loss_ce: 0.008200\n",
            "iteration 9352 : loss : 0.022336, loss_ce: 0.008348\n",
            "iteration 9353 : loss : 0.028399, loss_ce: 0.009064\n",
            "iteration 9354 : loss : 0.030940, loss_ce: 0.011858\n",
            "iteration 9355 : loss : 0.026021, loss_ce: 0.008711\n",
            "iteration 9356 : loss : 0.029015, loss_ce: 0.014405\n",
            "iteration 9357 : loss : 0.027544, loss_ce: 0.008700\n",
            "iteration 9358 : loss : 0.023957, loss_ce: 0.009083\n",
            "iteration 9359 : loss : 0.025547, loss_ce: 0.011289\n",
            "iteration 9360 : loss : 0.026412, loss_ce: 0.012136\n",
            "iteration 9361 : loss : 0.024225, loss_ce: 0.009012\n",
            "iteration 9362 : loss : 0.024831, loss_ce: 0.008887\n",
            "iteration 9363 : loss : 0.026886, loss_ce: 0.008483\n",
            "iteration 9364 : loss : 0.028852, loss_ce: 0.010811\n",
            "iteration 9365 : loss : 0.048966, loss_ce: 0.011295\n",
            "iteration 9366 : loss : 0.036838, loss_ce: 0.007811\n",
            "iteration 9367 : loss : 0.026436, loss_ce: 0.004856\n",
            "iteration 9368 : loss : 0.030565, loss_ce: 0.011317\n",
            "iteration 9369 : loss : 0.028830, loss_ce: 0.009939\n",
            "iteration 9370 : loss : 0.021400, loss_ce: 0.006581\n",
            "iteration 9371 : loss : 0.018273, loss_ce: 0.004288\n",
            "iteration 9372 : loss : 0.078725, loss_ce: 0.004555\n",
            "iteration 9373 : loss : 0.079995, loss_ce: 0.005111\n",
            "iteration 9374 : loss : 0.030178, loss_ce: 0.007174\n",
            "iteration 9375 : loss : 0.053676, loss_ce: 0.010438\n",
            "iteration 9376 : loss : 0.029655, loss_ce: 0.007167\n",
            "iteration 9377 : loss : 0.031235, loss_ce: 0.016637\n",
            "iteration 9378 : loss : 0.031896, loss_ce: 0.003516\n",
            "iteration 9379 : loss : 0.026881, loss_ce: 0.012986\n",
            "iteration 9380 : loss : 0.033430, loss_ce: 0.008918\n",
            "iteration 9381 : loss : 0.027479, loss_ce: 0.005965\n",
            "iteration 9382 : loss : 0.027205, loss_ce: 0.008967\n",
            "iteration 9383 : loss : 0.031075, loss_ce: 0.013028\n",
            "iteration 9384 : loss : 0.030840, loss_ce: 0.008899\n",
            "iteration 9385 : loss : 0.026907, loss_ce: 0.010533\n",
            "iteration 9386 : loss : 0.025887, loss_ce: 0.011898\n",
            "iteration 9387 : loss : 0.023560, loss_ce: 0.006930\n",
            "iteration 9388 : loss : 0.029940, loss_ce: 0.012051\n",
            "iteration 9389 : loss : 0.039516, loss_ce: 0.011535\n",
            "iteration 9390 : loss : 0.029786, loss_ce: 0.010762\n",
            "iteration 9391 : loss : 0.032709, loss_ce: 0.011184\n",
            "iteration 9392 : loss : 0.031992, loss_ce: 0.009843\n",
            "iteration 9393 : loss : 0.131545, loss_ce: 0.008452\n",
            " 67%|█████████████████▌        | 101/150 [3:41:07<1:47:03, 131.09s/it]iteration 9394 : loss : 0.025398, loss_ce: 0.010987\n",
            "iteration 9395 : loss : 0.027232, loss_ce: 0.006523\n",
            "iteration 9396 : loss : 0.029638, loss_ce: 0.004948\n",
            "iteration 9397 : loss : 0.025031, loss_ce: 0.008455\n",
            "iteration 9398 : loss : 0.030457, loss_ce: 0.011139\n",
            "iteration 9399 : loss : 0.033732, loss_ce: 0.016310\n",
            "iteration 9400 : loss : 0.025376, loss_ce: 0.011287\n",
            "iteration 9401 : loss : 0.072107, loss_ce: 0.006505\n",
            "iteration 9402 : loss : 0.024870, loss_ce: 0.006585\n",
            "iteration 9403 : loss : 0.024731, loss_ce: 0.011152\n",
            "iteration 9404 : loss : 0.036695, loss_ce: 0.010720\n",
            "iteration 9405 : loss : 0.025148, loss_ce: 0.008960\n",
            "iteration 9406 : loss : 0.027263, loss_ce: 0.006274\n",
            "iteration 9407 : loss : 0.023937, loss_ce: 0.012345\n",
            "iteration 9408 : loss : 0.021780, loss_ce: 0.006743\n",
            "iteration 9409 : loss : 0.029699, loss_ce: 0.008163\n",
            "iteration 9410 : loss : 0.027134, loss_ce: 0.012553\n",
            "iteration 9411 : loss : 0.027971, loss_ce: 0.010129\n",
            "iteration 9412 : loss : 0.029210, loss_ce: 0.011539\n",
            "iteration 9413 : loss : 0.029124, loss_ce: 0.010549\n",
            "iteration 9414 : loss : 0.029116, loss_ce: 0.006541\n",
            "iteration 9415 : loss : 0.082675, loss_ce: 0.004953\n",
            "iteration 9416 : loss : 0.025386, loss_ce: 0.009618\n",
            "iteration 9417 : loss : 0.026097, loss_ce: 0.008321\n",
            "iteration 9418 : loss : 0.028022, loss_ce: 0.006925\n",
            "iteration 9419 : loss : 0.027696, loss_ce: 0.010747\n",
            "iteration 9420 : loss : 0.025290, loss_ce: 0.004729\n",
            "iteration 9421 : loss : 0.032479, loss_ce: 0.006335\n",
            "iteration 9422 : loss : 0.025209, loss_ce: 0.007609\n",
            "iteration 9423 : loss : 0.075702, loss_ce: 0.009309\n",
            "iteration 9424 : loss : 0.025236, loss_ce: 0.010240\n",
            "iteration 9425 : loss : 0.029017, loss_ce: 0.010159\n",
            "iteration 9426 : loss : 0.022235, loss_ce: 0.005133\n",
            "iteration 9427 : loss : 0.026438, loss_ce: 0.005717\n",
            "iteration 9428 : loss : 0.024790, loss_ce: 0.009847\n",
            "iteration 9429 : loss : 0.021845, loss_ce: 0.006231\n",
            "iteration 9430 : loss : 0.026311, loss_ce: 0.011849\n",
            "iteration 9431 : loss : 0.029078, loss_ce: 0.011197\n",
            "iteration 9432 : loss : 0.026730, loss_ce: 0.006924\n",
            "iteration 9433 : loss : 0.032098, loss_ce: 0.011650\n",
            "iteration 9434 : loss : 0.025790, loss_ce: 0.007189\n",
            "iteration 9435 : loss : 0.028626, loss_ce: 0.011242\n",
            "iteration 9436 : loss : 0.032006, loss_ce: 0.008651\n",
            "iteration 9437 : loss : 0.024315, loss_ce: 0.006358\n",
            "iteration 9438 : loss : 0.025629, loss_ce: 0.008934\n",
            "iteration 9439 : loss : 0.026227, loss_ce: 0.006914\n",
            "iteration 9440 : loss : 0.023205, loss_ce: 0.007695\n",
            "iteration 9441 : loss : 0.020345, loss_ce: 0.007699\n",
            "iteration 9442 : loss : 0.034234, loss_ce: 0.008681\n",
            "iteration 9443 : loss : 0.027685, loss_ce: 0.012108\n",
            "iteration 9444 : loss : 0.032330, loss_ce: 0.011082\n",
            "iteration 9445 : loss : 0.075332, loss_ce: 0.004437\n",
            "iteration 9446 : loss : 0.031740, loss_ce: 0.008978\n",
            "iteration 9447 : loss : 0.022580, loss_ce: 0.005450\n",
            "iteration 9448 : loss : 0.077686, loss_ce: 0.007685\n",
            "iteration 9449 : loss : 0.023370, loss_ce: 0.008280\n",
            "iteration 9450 : loss : 0.026828, loss_ce: 0.010113\n",
            "iteration 9451 : loss : 0.024508, loss_ce: 0.009439\n",
            "iteration 9452 : loss : 0.078141, loss_ce: 0.005765\n",
            "iteration 9453 : loss : 0.040634, loss_ce: 0.008726\n",
            "iteration 9454 : loss : 0.023639, loss_ce: 0.010414\n",
            "iteration 9455 : loss : 0.030332, loss_ce: 0.012596\n",
            "iteration 9456 : loss : 0.078263, loss_ce: 0.007595\n",
            "iteration 9457 : loss : 0.027699, loss_ce: 0.009509\n",
            "iteration 9458 : loss : 0.029414, loss_ce: 0.012114\n",
            "iteration 9459 : loss : 0.024406, loss_ce: 0.008975\n",
            "iteration 9460 : loss : 0.032976, loss_ce: 0.008885\n",
            "iteration 9461 : loss : 0.023291, loss_ce: 0.005707\n",
            "iteration 9462 : loss : 0.027690, loss_ce: 0.008156\n",
            "iteration 9463 : loss : 0.059505, loss_ce: 0.005442\n",
            "iteration 9464 : loss : 0.023775, loss_ce: 0.008020\n",
            "iteration 9465 : loss : 0.027877, loss_ce: 0.008313\n",
            "iteration 9466 : loss : 0.032190, loss_ce: 0.006371\n",
            "iteration 9467 : loss : 0.044515, loss_ce: 0.010302\n",
            "iteration 9468 : loss : 0.023107, loss_ce: 0.008758\n",
            "iteration 9469 : loss : 0.033618, loss_ce: 0.006347\n",
            "iteration 9470 : loss : 0.025934, loss_ce: 0.009664\n",
            "iteration 9471 : loss : 0.028413, loss_ce: 0.010976\n",
            "iteration 9472 : loss : 0.027433, loss_ce: 0.011867\n",
            "iteration 9473 : loss : 0.034833, loss_ce: 0.014836\n",
            "iteration 9474 : loss : 0.028265, loss_ce: 0.010982\n",
            "iteration 9475 : loss : 0.025901, loss_ce: 0.012236\n",
            "iteration 9476 : loss : 0.023269, loss_ce: 0.008103\n",
            "iteration 9477 : loss : 0.039236, loss_ce: 0.004697\n",
            "iteration 9478 : loss : 0.023804, loss_ce: 0.009187\n",
            "iteration 9479 : loss : 0.026717, loss_ce: 0.008373\n",
            "iteration 9480 : loss : 0.038089, loss_ce: 0.006909\n",
            "iteration 9481 : loss : 0.028379, loss_ce: 0.010565\n",
            "iteration 9482 : loss : 0.023598, loss_ce: 0.008421\n",
            "iteration 9483 : loss : 0.024147, loss_ce: 0.012500\n",
            "iteration 9484 : loss : 0.026648, loss_ce: 0.014283\n",
            "iteration 9485 : loss : 0.024830, loss_ce: 0.008533\n",
            "iteration 9486 : loss : 0.237656, loss_ce: 0.010287\n",
            " 68%|█████████████████▋        | 102/150 [3:43:17<1:44:40, 130.85s/it]iteration 9487 : loss : 0.028016, loss_ce: 0.014592\n",
            "iteration 9488 : loss : 0.025870, loss_ce: 0.007313\n",
            "iteration 9489 : loss : 0.028717, loss_ce: 0.011705\n",
            "iteration 9490 : loss : 0.027352, loss_ce: 0.009926\n",
            "iteration 9491 : loss : 0.075760, loss_ce: 0.008244\n",
            "iteration 9492 : loss : 0.027147, loss_ce: 0.009153\n",
            "iteration 9493 : loss : 0.029898, loss_ce: 0.015037\n",
            "iteration 9494 : loss : 0.021813, loss_ce: 0.008201\n",
            "iteration 9495 : loss : 0.028900, loss_ce: 0.008815\n",
            "iteration 9496 : loss : 0.030990, loss_ce: 0.006197\n",
            "iteration 9497 : loss : 0.033472, loss_ce: 0.009709\n",
            "iteration 9498 : loss : 0.023031, loss_ce: 0.008125\n",
            "iteration 9499 : loss : 0.027251, loss_ce: 0.010495\n",
            "iteration 9500 : loss : 0.027406, loss_ce: 0.010317\n",
            "iteration 9501 : loss : 0.024829, loss_ce: 0.007493\n",
            "iteration 9502 : loss : 0.023520, loss_ce: 0.008777\n",
            "iteration 9503 : loss : 0.031084, loss_ce: 0.009261\n",
            "iteration 9504 : loss : 0.026211, loss_ce: 0.008460\n",
            "iteration 9505 : loss : 0.025247, loss_ce: 0.009293\n",
            "iteration 9506 : loss : 0.031998, loss_ce: 0.012537\n",
            "iteration 9507 : loss : 0.023005, loss_ce: 0.009342\n",
            "iteration 9508 : loss : 0.024767, loss_ce: 0.014837\n",
            "iteration 9509 : loss : 0.023928, loss_ce: 0.008849\n",
            "iteration 9510 : loss : 0.026550, loss_ce: 0.007499\n",
            "iteration 9511 : loss : 0.026159, loss_ce: 0.007752\n",
            "iteration 9512 : loss : 0.022774, loss_ce: 0.008503\n",
            "iteration 9513 : loss : 0.029391, loss_ce: 0.009132\n",
            "iteration 9514 : loss : 0.036779, loss_ce: 0.008709\n",
            "iteration 9515 : loss : 0.025213, loss_ce: 0.010144\n",
            "iteration 9516 : loss : 0.023136, loss_ce: 0.008779\n",
            "iteration 9517 : loss : 0.035762, loss_ce: 0.008902\n",
            "iteration 9518 : loss : 0.075934, loss_ce: 0.004348\n",
            "iteration 9519 : loss : 0.022180, loss_ce: 0.009960\n",
            "iteration 9520 : loss : 0.078666, loss_ce: 0.007993\n",
            "iteration 9521 : loss : 0.032446, loss_ce: 0.010390\n",
            "iteration 9522 : loss : 0.076817, loss_ce: 0.007976\n",
            "iteration 9523 : loss : 0.025888, loss_ce: 0.007822\n",
            "iteration 9524 : loss : 0.026167, loss_ce: 0.008523\n",
            "iteration 9525 : loss : 0.024591, loss_ce: 0.006859\n",
            "iteration 9526 : loss : 0.030214, loss_ce: 0.008521\n",
            "iteration 9527 : loss : 0.056593, loss_ce: 0.006346\n",
            "iteration 9528 : loss : 0.028890, loss_ce: 0.011172\n",
            "iteration 9529 : loss : 0.024492, loss_ce: 0.006770\n",
            "iteration 9530 : loss : 0.020630, loss_ce: 0.007938\n",
            "iteration 9531 : loss : 0.024447, loss_ce: 0.007853\n",
            "iteration 9532 : loss : 0.019881, loss_ce: 0.008676\n",
            "iteration 9533 : loss : 0.023826, loss_ce: 0.007665\n",
            "iteration 9534 : loss : 0.023456, loss_ce: 0.006654\n",
            "iteration 9535 : loss : 0.077525, loss_ce: 0.006381\n",
            "iteration 9536 : loss : 0.022525, loss_ce: 0.004439\n",
            "iteration 9537 : loss : 0.076995, loss_ce: 0.008514\n",
            "iteration 9538 : loss : 0.024017, loss_ce: 0.007445\n",
            "iteration 9539 : loss : 0.032653, loss_ce: 0.006721\n",
            "iteration 9540 : loss : 0.021178, loss_ce: 0.006684\n",
            "iteration 9541 : loss : 0.025920, loss_ce: 0.008758\n",
            "iteration 9542 : loss : 0.026605, loss_ce: 0.008151\n",
            "iteration 9543 : loss : 0.029543, loss_ce: 0.012161\n",
            "iteration 9544 : loss : 0.024278, loss_ce: 0.006904\n",
            "iteration 9545 : loss : 0.024876, loss_ce: 0.010726\n",
            "iteration 9546 : loss : 0.025746, loss_ce: 0.006088\n",
            "iteration 9547 : loss : 0.037796, loss_ce: 0.011387\n",
            "iteration 9548 : loss : 0.030288, loss_ce: 0.008196\n",
            "iteration 9549 : loss : 0.028542, loss_ce: 0.009075\n",
            "iteration 9550 : loss : 0.027627, loss_ce: 0.009248\n",
            "iteration 9551 : loss : 0.029285, loss_ce: 0.009324\n",
            "iteration 9552 : loss : 0.026777, loss_ce: 0.010239\n",
            "iteration 9553 : loss : 0.025654, loss_ce: 0.005639\n",
            "iteration 9554 : loss : 0.076235, loss_ce: 0.007129\n",
            "iteration 9555 : loss : 0.024151, loss_ce: 0.009322\n",
            "iteration 9556 : loss : 0.073418, loss_ce: 0.003215\n",
            "iteration 9557 : loss : 0.026584, loss_ce: 0.010976\n",
            "iteration 9558 : loss : 0.026628, loss_ce: 0.012090\n",
            "iteration 9559 : loss : 0.024239, loss_ce: 0.009126\n",
            "iteration 9560 : loss : 0.023480, loss_ce: 0.008844\n",
            "iteration 9561 : loss : 0.028950, loss_ce: 0.014232\n",
            "iteration 9562 : loss : 0.027184, loss_ce: 0.006260\n",
            "iteration 9563 : loss : 0.020967, loss_ce: 0.006692\n",
            "iteration 9564 : loss : 0.028875, loss_ce: 0.012796\n",
            "iteration 9565 : loss : 0.076095, loss_ce: 0.006338\n",
            "iteration 9566 : loss : 0.027410, loss_ce: 0.013384\n",
            "iteration 9567 : loss : 0.026697, loss_ce: 0.010831\n",
            "iteration 9568 : loss : 0.028133, loss_ce: 0.010784\n",
            "iteration 9569 : loss : 0.024061, loss_ce: 0.009153\n",
            "iteration 9570 : loss : 0.026638, loss_ce: 0.013710\n",
            "iteration 9571 : loss : 0.024469, loss_ce: 0.007508\n",
            "iteration 9572 : loss : 0.027303, loss_ce: 0.004272\n",
            "iteration 9573 : loss : 0.028456, loss_ce: 0.006815\n",
            "iteration 9574 : loss : 0.027781, loss_ce: 0.010981\n",
            "iteration 9575 : loss : 0.028705, loss_ce: 0.011753\n",
            "iteration 9576 : loss : 0.177346, loss_ce: 0.004076\n",
            "iteration 9577 : loss : 0.027015, loss_ce: 0.008624\n",
            "iteration 9578 : loss : 0.025427, loss_ce: 0.004599\n",
            "iteration 9579 : loss : 0.443352, loss_ce: 0.000551\n",
            " 69%|█████████████████▊        | 103/150 [3:45:29<1:42:47, 131.23s/it]iteration 9580 : loss : 0.021849, loss_ce: 0.004683\n",
            "iteration 9581 : loss : 0.025415, loss_ce: 0.008298\n",
            "iteration 9582 : loss : 0.029041, loss_ce: 0.010766\n",
            "iteration 9583 : loss : 0.020335, loss_ce: 0.007050\n",
            "iteration 9584 : loss : 0.075580, loss_ce: 0.007019\n",
            "iteration 9585 : loss : 0.025604, loss_ce: 0.011214\n",
            "iteration 9586 : loss : 0.025070, loss_ce: 0.011534\n",
            "iteration 9587 : loss : 0.028094, loss_ce: 0.010246\n",
            "iteration 9588 : loss : 0.026843, loss_ce: 0.007682\n",
            "iteration 9589 : loss : 0.027143, loss_ce: 0.006992\n",
            "iteration 9590 : loss : 0.028787, loss_ce: 0.012818\n",
            "iteration 9591 : loss : 0.020849, loss_ce: 0.005631\n",
            "iteration 9592 : loss : 0.024314, loss_ce: 0.005627\n",
            "iteration 9593 : loss : 0.023049, loss_ce: 0.009663\n",
            "iteration 9594 : loss : 0.081937, loss_ce: 0.008380\n",
            "iteration 9595 : loss : 0.042703, loss_ce: 0.006813\n",
            "iteration 9596 : loss : 0.025055, loss_ce: 0.008955\n",
            "iteration 9597 : loss : 0.025653, loss_ce: 0.006556\n",
            "iteration 9598 : loss : 0.027314, loss_ce: 0.008023\n",
            "iteration 9599 : loss : 0.021325, loss_ce: 0.006626\n",
            "iteration 9600 : loss : 0.027917, loss_ce: 0.010528\n",
            "iteration 9601 : loss : 0.021543, loss_ce: 0.007842\n",
            "iteration 9602 : loss : 0.030034, loss_ce: 0.007891\n",
            "iteration 9603 : loss : 0.023258, loss_ce: 0.004984\n",
            "iteration 9604 : loss : 0.027290, loss_ce: 0.012543\n",
            "iteration 9605 : loss : 0.076994, loss_ce: 0.008170\n",
            "iteration 9606 : loss : 0.022946, loss_ce: 0.006048\n",
            "iteration 9607 : loss : 0.030489, loss_ce: 0.007251\n",
            "iteration 9608 : loss : 0.025893, loss_ce: 0.010204\n",
            "iteration 9609 : loss : 0.028312, loss_ce: 0.010950\n",
            "iteration 9610 : loss : 0.020791, loss_ce: 0.009785\n",
            "iteration 9611 : loss : 0.023218, loss_ce: 0.008312\n",
            "iteration 9612 : loss : 0.028967, loss_ce: 0.007197\n",
            "iteration 9613 : loss : 0.021036, loss_ce: 0.006422\n",
            "iteration 9614 : loss : 0.026995, loss_ce: 0.008452\n",
            "iteration 9615 : loss : 0.078809, loss_ce: 0.004971\n",
            "iteration 9616 : loss : 0.038334, loss_ce: 0.012816\n",
            "iteration 9617 : loss : 0.025344, loss_ce: 0.009363\n",
            "iteration 9618 : loss : 0.022768, loss_ce: 0.008452\n",
            "iteration 9619 : loss : 0.030090, loss_ce: 0.007635\n",
            "iteration 9620 : loss : 0.021596, loss_ce: 0.007823\n",
            "iteration 9621 : loss : 0.028231, loss_ce: 0.010544\n",
            "iteration 9622 : loss : 0.028427, loss_ce: 0.010165\n",
            "iteration 9623 : loss : 0.026510, loss_ce: 0.008817\n",
            "iteration 9624 : loss : 0.086779, loss_ce: 0.005600\n",
            "iteration 9625 : loss : 0.026312, loss_ce: 0.008593\n",
            "iteration 9626 : loss : 0.076438, loss_ce: 0.006418\n",
            "iteration 9627 : loss : 0.025962, loss_ce: 0.009330\n",
            "iteration 9628 : loss : 0.022900, loss_ce: 0.009072\n",
            "iteration 9629 : loss : 0.028670, loss_ce: 0.011796\n",
            "iteration 9630 : loss : 0.030726, loss_ce: 0.007374\n",
            "iteration 9631 : loss : 0.037955, loss_ce: 0.013593\n",
            "iteration 9632 : loss : 0.033054, loss_ce: 0.014044\n",
            "iteration 9633 : loss : 0.020753, loss_ce: 0.006765\n",
            "iteration 9634 : loss : 0.026825, loss_ce: 0.008020\n",
            "iteration 9635 : loss : 0.077258, loss_ce: 0.007022\n",
            "iteration 9636 : loss : 0.027291, loss_ce: 0.009686\n",
            "iteration 9637 : loss : 0.033612, loss_ce: 0.015251\n",
            "iteration 9638 : loss : 0.023869, loss_ce: 0.006519\n",
            "iteration 9639 : loss : 0.023364, loss_ce: 0.008004\n",
            "iteration 9640 : loss : 0.036542, loss_ce: 0.008165\n",
            "iteration 9641 : loss : 0.024803, loss_ce: 0.010684\n",
            "iteration 9642 : loss : 0.024296, loss_ce: 0.008015\n",
            "iteration 9643 : loss : 0.022961, loss_ce: 0.007891\n",
            "iteration 9644 : loss : 0.023564, loss_ce: 0.009250\n",
            "iteration 9645 : loss : 0.027391, loss_ce: 0.006039\n",
            "iteration 9646 : loss : 0.028375, loss_ce: 0.011549\n",
            "iteration 9647 : loss : 0.028099, loss_ce: 0.008263\n",
            "iteration 9648 : loss : 0.028971, loss_ce: 0.013103\n",
            "iteration 9649 : loss : 0.021558, loss_ce: 0.008937\n",
            "iteration 9650 : loss : 0.025688, loss_ce: 0.007286\n",
            "iteration 9651 : loss : 0.021096, loss_ce: 0.003629\n",
            "iteration 9652 : loss : 0.026273, loss_ce: 0.010706\n",
            "iteration 9653 : loss : 0.031290, loss_ce: 0.009354\n",
            "iteration 9654 : loss : 0.076350, loss_ce: 0.008796\n",
            "iteration 9655 : loss : 0.026860, loss_ce: 0.012741\n",
            "iteration 9656 : loss : 0.024652, loss_ce: 0.009956\n",
            "iteration 9657 : loss : 0.029396, loss_ce: 0.012953\n",
            "iteration 9658 : loss : 0.028706, loss_ce: 0.009961\n",
            "iteration 9659 : loss : 0.024920, loss_ce: 0.007819\n",
            "iteration 9660 : loss : 0.026826, loss_ce: 0.013504\n",
            "iteration 9661 : loss : 0.023019, loss_ce: 0.010185\n",
            "iteration 9662 : loss : 0.028829, loss_ce: 0.010219\n",
            "iteration 9663 : loss : 0.025287, loss_ce: 0.005327\n",
            "iteration 9664 : loss : 0.024149, loss_ce: 0.005574\n",
            "iteration 9665 : loss : 0.036952, loss_ce: 0.011971\n",
            "iteration 9666 : loss : 0.024621, loss_ce: 0.006329\n",
            "iteration 9667 : loss : 0.031576, loss_ce: 0.007715\n",
            "iteration 9668 : loss : 0.023694, loss_ce: 0.006282\n",
            "iteration 9669 : loss : 0.074359, loss_ce: 0.005279\n",
            "iteration 9670 : loss : 0.034428, loss_ce: 0.009475\n",
            "iteration 9671 : loss : 0.023445, loss_ce: 0.003944\n",
            "iteration 9672 : loss : 0.148413, loss_ce: 0.009947\n",
            " 69%|██████████████████        | 104/150 [3:47:39<1:40:16, 130.79s/it]iteration 9673 : loss : 0.076735, loss_ce: 0.009116\n",
            "iteration 9674 : loss : 0.026307, loss_ce: 0.010205\n",
            "iteration 9675 : loss : 0.031240, loss_ce: 0.009794\n",
            "iteration 9676 : loss : 0.021575, loss_ce: 0.008863\n",
            "iteration 9677 : loss : 0.031586, loss_ce: 0.010234\n",
            "iteration 9678 : loss : 0.026714, loss_ce: 0.006842\n",
            "iteration 9679 : loss : 0.029176, loss_ce: 0.006645\n",
            "iteration 9680 : loss : 0.026222, loss_ce: 0.007779\n",
            "iteration 9681 : loss : 0.025199, loss_ce: 0.006663\n",
            "iteration 9682 : loss : 0.027734, loss_ce: 0.008953\n",
            "iteration 9683 : loss : 0.026392, loss_ce: 0.005350\n",
            "iteration 9684 : loss : 0.023538, loss_ce: 0.007848\n",
            "iteration 9685 : loss : 0.029523, loss_ce: 0.008054\n",
            "iteration 9686 : loss : 0.023062, loss_ce: 0.006440\n",
            "iteration 9687 : loss : 0.020432, loss_ce: 0.006164\n",
            "iteration 9688 : loss : 0.022691, loss_ce: 0.008993\n",
            "iteration 9689 : loss : 0.027108, loss_ce: 0.007653\n",
            "iteration 9690 : loss : 0.039170, loss_ce: 0.010107\n",
            "iteration 9691 : loss : 0.029944, loss_ce: 0.008497\n",
            "iteration 9692 : loss : 0.026028, loss_ce: 0.009491\n",
            "iteration 9693 : loss : 0.024823, loss_ce: 0.008995\n",
            "iteration 9694 : loss : 0.025715, loss_ce: 0.006912\n",
            "iteration 9695 : loss : 0.033339, loss_ce: 0.012540\n",
            "iteration 9696 : loss : 0.026513, loss_ce: 0.009828\n",
            "iteration 9697 : loss : 0.023487, loss_ce: 0.011170\n",
            "iteration 9698 : loss : 0.023517, loss_ce: 0.006056\n",
            "iteration 9699 : loss : 0.026195, loss_ce: 0.013524\n",
            "iteration 9700 : loss : 0.028234, loss_ce: 0.006795\n",
            "iteration 9701 : loss : 0.024276, loss_ce: 0.009293\n",
            "iteration 9702 : loss : 0.040945, loss_ce: 0.004840\n",
            "iteration 9703 : loss : 0.073837, loss_ce: 0.006120\n",
            "iteration 9704 : loss : 0.027831, loss_ce: 0.006665\n",
            "iteration 9705 : loss : 0.024484, loss_ce: 0.009967\n",
            "iteration 9706 : loss : 0.028011, loss_ce: 0.006700\n",
            "iteration 9707 : loss : 0.026599, loss_ce: 0.011523\n",
            "iteration 9708 : loss : 0.033726, loss_ce: 0.009282\n",
            "iteration 9709 : loss : 0.027481, loss_ce: 0.006578\n",
            "iteration 9710 : loss : 0.026814, loss_ce: 0.007269\n",
            "iteration 9711 : loss : 0.026925, loss_ce: 0.006739\n",
            "iteration 9712 : loss : 0.025694, loss_ce: 0.010336\n",
            "iteration 9713 : loss : 0.025500, loss_ce: 0.011540\n",
            "iteration 9714 : loss : 0.023885, loss_ce: 0.007004\n",
            "iteration 9715 : loss : 0.024389, loss_ce: 0.012289\n",
            "iteration 9716 : loss : 0.078380, loss_ce: 0.008597\n",
            "iteration 9717 : loss : 0.039966, loss_ce: 0.007673\n",
            "iteration 9718 : loss : 0.026750, loss_ce: 0.012175\n",
            "iteration 9719 : loss : 0.024102, loss_ce: 0.010480\n",
            "iteration 9720 : loss : 0.078966, loss_ce: 0.007738\n",
            "iteration 9721 : loss : 0.027433, loss_ce: 0.007293\n",
            "iteration 9722 : loss : 0.024933, loss_ce: 0.009781\n",
            "iteration 9723 : loss : 0.025247, loss_ce: 0.008212\n",
            "iteration 9724 : loss : 0.023185, loss_ce: 0.008268\n",
            "iteration 9725 : loss : 0.024958, loss_ce: 0.011545\n",
            "iteration 9726 : loss : 0.028674, loss_ce: 0.008735\n",
            "iteration 9727 : loss : 0.026483, loss_ce: 0.012255\n",
            "iteration 9728 : loss : 0.025261, loss_ce: 0.008123\n",
            "iteration 9729 : loss : 0.073697, loss_ce: 0.006040\n",
            "iteration 9730 : loss : 0.031764, loss_ce: 0.006447\n",
            "iteration 9731 : loss : 0.032461, loss_ce: 0.013010\n",
            "iteration 9732 : loss : 0.026541, loss_ce: 0.012114\n",
            "iteration 9733 : loss : 0.025713, loss_ce: 0.009051\n",
            "iteration 9734 : loss : 0.026339, loss_ce: 0.009125\n",
            "iteration 9735 : loss : 0.020405, loss_ce: 0.006136\n",
            "iteration 9736 : loss : 0.022779, loss_ce: 0.008738\n",
            "iteration 9737 : loss : 0.024609, loss_ce: 0.013119\n",
            "iteration 9738 : loss : 0.025828, loss_ce: 0.013214\n",
            "iteration 9739 : loss : 0.036715, loss_ce: 0.006929\n",
            "iteration 9740 : loss : 0.040527, loss_ce: 0.007164\n",
            "iteration 9741 : loss : 0.076699, loss_ce: 0.006135\n",
            "iteration 9742 : loss : 0.026034, loss_ce: 0.008331\n",
            "iteration 9743 : loss : 0.025691, loss_ce: 0.011428\n",
            "iteration 9744 : loss : 0.023868, loss_ce: 0.006589\n",
            "iteration 9745 : loss : 0.029442, loss_ce: 0.010407\n",
            "iteration 9746 : loss : 0.040084, loss_ce: 0.006176\n",
            "iteration 9747 : loss : 0.026099, loss_ce: 0.008573\n",
            "iteration 9748 : loss : 0.022199, loss_ce: 0.005215\n",
            "iteration 9749 : loss : 0.029072, loss_ce: 0.013116\n",
            "iteration 9750 : loss : 0.030677, loss_ce: 0.010950\n",
            "iteration 9751 : loss : 0.027209, loss_ce: 0.010160\n",
            "iteration 9752 : loss : 0.026842, loss_ce: 0.009320\n",
            "iteration 9753 : loss : 0.028476, loss_ce: 0.009284\n",
            "iteration 9754 : loss : 0.021018, loss_ce: 0.007366\n",
            "iteration 9755 : loss : 0.076918, loss_ce: 0.008177\n",
            "iteration 9756 : loss : 0.023118, loss_ce: 0.006904\n",
            "iteration 9757 : loss : 0.028323, loss_ce: 0.008643\n",
            "iteration 9758 : loss : 0.024589, loss_ce: 0.009004\n",
            "iteration 9759 : loss : 0.029344, loss_ce: 0.005015\n",
            "iteration 9760 : loss : 0.025132, loss_ce: 0.007734\n",
            "iteration 9761 : loss : 0.027106, loss_ce: 0.007313\n",
            "iteration 9762 : loss : 0.024132, loss_ce: 0.010561\n",
            "iteration 9763 : loss : 0.028785, loss_ce: 0.014249\n",
            "iteration 9764 : loss : 0.026126, loss_ce: 0.011299\n",
            "iteration 9765 : loss : 0.089701, loss_ce: 0.014161\n",
            " 70%|██████████████████▏       | 105/150 [3:49:50<1:38:01, 130.70s/it]iteration 9766 : loss : 0.025718, loss_ce: 0.008014\n",
            "iteration 9767 : loss : 0.027854, loss_ce: 0.007710\n",
            "iteration 9768 : loss : 0.027162, loss_ce: 0.007118\n",
            "iteration 9769 : loss : 0.022138, loss_ce: 0.006027\n",
            "iteration 9770 : loss : 0.025871, loss_ce: 0.011442\n",
            "iteration 9771 : loss : 0.076045, loss_ce: 0.009196\n",
            "iteration 9772 : loss : 0.077621, loss_ce: 0.005712\n",
            "iteration 9773 : loss : 0.027014, loss_ce: 0.008543\n",
            "iteration 9774 : loss : 0.024908, loss_ce: 0.006317\n",
            "iteration 9775 : loss : 0.025336, loss_ce: 0.009647\n",
            "iteration 9776 : loss : 0.037733, loss_ce: 0.012326\n",
            "iteration 9777 : loss : 0.024832, loss_ce: 0.013560\n",
            "iteration 9778 : loss : 0.029568, loss_ce: 0.007928\n",
            "iteration 9779 : loss : 0.026309, loss_ce: 0.010983\n",
            "iteration 9780 : loss : 0.025695, loss_ce: 0.009686\n",
            "iteration 9781 : loss : 0.029952, loss_ce: 0.011322\n",
            "iteration 9782 : loss : 0.036043, loss_ce: 0.010434\n",
            "iteration 9783 : loss : 0.025244, loss_ce: 0.010046\n",
            "iteration 9784 : loss : 0.023348, loss_ce: 0.004228\n",
            "iteration 9785 : loss : 0.081222, loss_ce: 0.008125\n",
            "iteration 9786 : loss : 0.024327, loss_ce: 0.006429\n",
            "iteration 9787 : loss : 0.022812, loss_ce: 0.006601\n",
            "iteration 9788 : loss : 0.024893, loss_ce: 0.012320\n",
            "iteration 9789 : loss : 0.025522, loss_ce: 0.010214\n",
            "iteration 9790 : loss : 0.025695, loss_ce: 0.006740\n",
            "iteration 9791 : loss : 0.020323, loss_ce: 0.004869\n",
            "iteration 9792 : loss : 0.025008, loss_ce: 0.008664\n",
            "iteration 9793 : loss : 0.073708, loss_ce: 0.005324\n",
            "iteration 9794 : loss : 0.031806, loss_ce: 0.010403\n",
            "iteration 9795 : loss : 0.025024, loss_ce: 0.010917\n",
            "iteration 9796 : loss : 0.032725, loss_ce: 0.011921\n",
            "iteration 9797 : loss : 0.025781, loss_ce: 0.008314\n",
            "iteration 9798 : loss : 0.029172, loss_ce: 0.011648\n",
            "iteration 9799 : loss : 0.028243, loss_ce: 0.015240\n",
            "iteration 9800 : loss : 0.024243, loss_ce: 0.008296\n",
            "iteration 9801 : loss : 0.033459, loss_ce: 0.018776\n",
            "iteration 9802 : loss : 0.025906, loss_ce: 0.012814\n",
            "iteration 9803 : loss : 0.026024, loss_ce: 0.009380\n",
            "iteration 9804 : loss : 0.023660, loss_ce: 0.008736\n",
            "iteration 9805 : loss : 0.026818, loss_ce: 0.009465\n",
            "iteration 9806 : loss : 0.024752, loss_ce: 0.005617\n",
            "iteration 9807 : loss : 0.025262, loss_ce: 0.010613\n",
            "iteration 9808 : loss : 0.028764, loss_ce: 0.010514\n",
            "iteration 9809 : loss : 0.025086, loss_ce: 0.009300\n",
            "iteration 9810 : loss : 0.023338, loss_ce: 0.006973\n",
            "iteration 9811 : loss : 0.033127, loss_ce: 0.012976\n",
            "iteration 9812 : loss : 0.028242, loss_ce: 0.004419\n",
            "iteration 9813 : loss : 0.024504, loss_ce: 0.011239\n",
            "iteration 9814 : loss : 0.024938, loss_ce: 0.006077\n",
            "iteration 9815 : loss : 0.024165, loss_ce: 0.008124\n",
            "iteration 9816 : loss : 0.027687, loss_ce: 0.009122\n",
            "iteration 9817 : loss : 0.074453, loss_ce: 0.003431\n",
            "iteration 9818 : loss : 0.091555, loss_ce: 0.006391\n",
            "iteration 9819 : loss : 0.028098, loss_ce: 0.009285\n",
            "iteration 9820 : loss : 0.022988, loss_ce: 0.006185\n",
            "iteration 9821 : loss : 0.031799, loss_ce: 0.008442\n",
            "iteration 9822 : loss : 0.026474, loss_ce: 0.006269\n",
            "iteration 9823 : loss : 0.026370, loss_ce: 0.009510\n",
            "iteration 9824 : loss : 0.025820, loss_ce: 0.006228\n",
            "iteration 9825 : loss : 0.025602, loss_ce: 0.009872\n",
            "iteration 9826 : loss : 0.024481, loss_ce: 0.005935\n",
            "iteration 9827 : loss : 0.022666, loss_ce: 0.007697\n",
            "iteration 9828 : loss : 0.023280, loss_ce: 0.005750\n",
            "iteration 9829 : loss : 0.081165, loss_ce: 0.007538\n",
            "iteration 9830 : loss : 0.025064, loss_ce: 0.010013\n",
            "iteration 9831 : loss : 0.024417, loss_ce: 0.006908\n",
            "iteration 9832 : loss : 0.028111, loss_ce: 0.012233\n",
            "iteration 9833 : loss : 0.055540, loss_ce: 0.005582\n",
            "iteration 9834 : loss : 0.027547, loss_ce: 0.007465\n",
            "iteration 9835 : loss : 0.025480, loss_ce: 0.010798\n",
            "iteration 9836 : loss : 0.023949, loss_ce: 0.009977\n",
            "iteration 9837 : loss : 0.024803, loss_ce: 0.007630\n",
            "iteration 9838 : loss : 0.078991, loss_ce: 0.008372\n",
            "iteration 9839 : loss : 0.024712, loss_ce: 0.010293\n",
            "iteration 9840 : loss : 0.023807, loss_ce: 0.006721\n",
            "iteration 9841 : loss : 0.024954, loss_ce: 0.009659\n",
            "iteration 9842 : loss : 0.026196, loss_ce: 0.008492\n",
            "iteration 9843 : loss : 0.075406, loss_ce: 0.007868\n",
            "iteration 9844 : loss : 0.030110, loss_ce: 0.005431\n",
            "iteration 9845 : loss : 0.032950, loss_ce: 0.008772\n",
            "iteration 9846 : loss : 0.026925, loss_ce: 0.009542\n",
            "iteration 9847 : loss : 0.079819, loss_ce: 0.007949\n",
            "iteration 9848 : loss : 0.031032, loss_ce: 0.011001\n",
            "iteration 9849 : loss : 0.027801, loss_ce: 0.012307\n",
            "iteration 9850 : loss : 0.024996, loss_ce: 0.009559\n",
            "iteration 9851 : loss : 0.026048, loss_ce: 0.010278\n",
            "iteration 9852 : loss : 0.081971, loss_ce: 0.007741\n",
            "iteration 9853 : loss : 0.023539, loss_ce: 0.009387\n",
            "iteration 9854 : loss : 0.030996, loss_ce: 0.010162\n",
            "iteration 9855 : loss : 0.024826, loss_ce: 0.004933\n",
            "iteration 9856 : loss : 0.020348, loss_ce: 0.006792\n",
            "iteration 9857 : loss : 0.022588, loss_ce: 0.006917\n",
            "iteration 9858 : loss : 0.233367, loss_ce: 0.008449\n",
            " 71%|██████████████████▎       | 106/150 [3:52:01<1:35:57, 130.86s/it]iteration 9859 : loss : 0.024906, loss_ce: 0.011234\n",
            "iteration 9860 : loss : 0.026677, loss_ce: 0.010747\n",
            "iteration 9861 : loss : 0.075375, loss_ce: 0.007352\n",
            "iteration 9862 : loss : 0.030480, loss_ce: 0.006260\n",
            "iteration 9863 : loss : 0.074412, loss_ce: 0.005912\n",
            "iteration 9864 : loss : 0.025639, loss_ce: 0.009057\n",
            "iteration 9865 : loss : 0.030668, loss_ce: 0.010928\n",
            "iteration 9866 : loss : 0.023886, loss_ce: 0.012204\n",
            "iteration 9867 : loss : 0.027477, loss_ce: 0.008141\n",
            "iteration 9868 : loss : 0.023816, loss_ce: 0.007624\n",
            "iteration 9869 : loss : 0.020796, loss_ce: 0.003863\n",
            "iteration 9870 : loss : 0.023210, loss_ce: 0.009499\n",
            "iteration 9871 : loss : 0.027534, loss_ce: 0.010589\n",
            "iteration 9872 : loss : 0.027561, loss_ce: 0.014101\n",
            "iteration 9873 : loss : 0.044370, loss_ce: 0.009437\n",
            "iteration 9874 : loss : 0.024178, loss_ce: 0.009572\n",
            "iteration 9875 : loss : 0.025303, loss_ce: 0.009354\n",
            "iteration 9876 : loss : 0.029934, loss_ce: 0.011112\n",
            "iteration 9877 : loss : 0.023993, loss_ce: 0.011336\n",
            "iteration 9878 : loss : 0.025946, loss_ce: 0.009393\n",
            "iteration 9879 : loss : 0.025813, loss_ce: 0.008529\n",
            "iteration 9880 : loss : 0.027302, loss_ce: 0.008499\n",
            "iteration 9881 : loss : 0.026119, loss_ce: 0.007440\n",
            "iteration 9882 : loss : 0.033292, loss_ce: 0.007988\n",
            "iteration 9883 : loss : 0.024680, loss_ce: 0.009940\n",
            "iteration 9884 : loss : 0.026633, loss_ce: 0.010605\n",
            "iteration 9885 : loss : 0.035446, loss_ce: 0.005390\n",
            "iteration 9886 : loss : 0.028474, loss_ce: 0.011841\n",
            "iteration 9887 : loss : 0.026877, loss_ce: 0.013720\n",
            "iteration 9888 : loss : 0.023631, loss_ce: 0.008894\n",
            "iteration 9889 : loss : 0.027992, loss_ce: 0.011313\n",
            "iteration 9890 : loss : 0.025906, loss_ce: 0.007765\n",
            "iteration 9891 : loss : 0.024997, loss_ce: 0.008737\n",
            "iteration 9892 : loss : 0.024671, loss_ce: 0.006139\n",
            "iteration 9893 : loss : 0.046670, loss_ce: 0.007876\n",
            "iteration 9894 : loss : 0.023471, loss_ce: 0.005454\n",
            "iteration 9895 : loss : 0.030514, loss_ce: 0.009590\n",
            "iteration 9896 : loss : 0.031202, loss_ce: 0.008335\n",
            "iteration 9897 : loss : 0.038565, loss_ce: 0.011572\n",
            "iteration 9898 : loss : 0.025953, loss_ce: 0.009268\n",
            "iteration 9899 : loss : 0.032916, loss_ce: 0.010601\n",
            "iteration 9900 : loss : 0.029849, loss_ce: 0.008379\n",
            "iteration 9901 : loss : 0.032773, loss_ce: 0.007524\n",
            "iteration 9902 : loss : 0.032833, loss_ce: 0.006525\n",
            "iteration 9903 : loss : 0.029429, loss_ce: 0.007339\n",
            "iteration 9904 : loss : 0.022929, loss_ce: 0.005767\n",
            "iteration 9905 : loss : 0.026535, loss_ce: 0.007061\n",
            "iteration 9906 : loss : 0.078051, loss_ce: 0.009617\n",
            "iteration 9907 : loss : 0.031837, loss_ce: 0.013328\n",
            "iteration 9908 : loss : 0.038078, loss_ce: 0.009212\n",
            "iteration 9909 : loss : 0.020224, loss_ce: 0.009810\n",
            "iteration 9910 : loss : 0.037260, loss_ce: 0.008645\n",
            "iteration 9911 : loss : 0.027338, loss_ce: 0.009059\n",
            "iteration 9912 : loss : 0.030922, loss_ce: 0.009117\n",
            "iteration 9913 : loss : 0.024051, loss_ce: 0.008253\n",
            "iteration 9914 : loss : 0.026033, loss_ce: 0.009701\n",
            "iteration 9915 : loss : 0.029519, loss_ce: 0.012292\n",
            "iteration 9916 : loss : 0.025006, loss_ce: 0.007840\n",
            "iteration 9917 : loss : 0.029929, loss_ce: 0.008369\n",
            "iteration 9918 : loss : 0.076721, loss_ce: 0.005213\n",
            "iteration 9919 : loss : 0.024735, loss_ce: 0.009366\n",
            "iteration 9920 : loss : 0.027981, loss_ce: 0.009642\n",
            "iteration 9921 : loss : 0.030574, loss_ce: 0.012853\n",
            "iteration 9922 : loss : 0.027339, loss_ce: 0.010535\n",
            "iteration 9923 : loss : 0.029062, loss_ce: 0.010147\n",
            "iteration 9924 : loss : 0.025303, loss_ce: 0.009793\n",
            "iteration 9925 : loss : 0.026785, loss_ce: 0.006720\n",
            "iteration 9926 : loss : 0.032368, loss_ce: 0.010851\n",
            "iteration 9927 : loss : 0.027959, loss_ce: 0.007140\n",
            "iteration 9928 : loss : 0.026360, loss_ce: 0.008710\n",
            "iteration 9929 : loss : 0.074436, loss_ce: 0.006357\n",
            "iteration 9930 : loss : 0.028126, loss_ce: 0.003797\n",
            "iteration 9931 : loss : 0.019637, loss_ce: 0.005100\n",
            "iteration 9932 : loss : 0.023433, loss_ce: 0.008253\n",
            "iteration 9933 : loss : 0.029109, loss_ce: 0.008264\n",
            "iteration 9934 : loss : 0.025759, loss_ce: 0.010005\n",
            "iteration 9935 : loss : 0.029728, loss_ce: 0.005955\n",
            "iteration 9936 : loss : 0.024993, loss_ce: 0.009610\n",
            "iteration 9937 : loss : 0.023636, loss_ce: 0.007668\n",
            "iteration 9938 : loss : 0.025157, loss_ce: 0.008658\n",
            "iteration 9939 : loss : 0.030231, loss_ce: 0.010901\n",
            "iteration 9940 : loss : 0.024918, loss_ce: 0.009918\n",
            "iteration 9941 : loss : 0.032058, loss_ce: 0.012080\n",
            "iteration 9942 : loss : 0.028323, loss_ce: 0.010524\n",
            "iteration 9943 : loss : 0.031565, loss_ce: 0.009329\n",
            "iteration 9944 : loss : 0.027616, loss_ce: 0.009354\n",
            "iteration 9945 : loss : 0.025485, loss_ce: 0.009751\n",
            "iteration 9946 : loss : 0.028121, loss_ce: 0.009214\n",
            "iteration 9947 : loss : 0.029882, loss_ce: 0.006750\n",
            "iteration 9948 : loss : 0.078995, loss_ce: 0.005564\n",
            "iteration 9949 : loss : 0.028009, loss_ce: 0.008427\n",
            "iteration 9950 : loss : 0.022838, loss_ce: 0.008175\n",
            "iteration 9951 : loss : 0.185508, loss_ce: 0.006664\n",
            " 71%|██████████████████▌       | 107/150 [3:54:11<1:33:35, 130.60s/it]iteration 9952 : loss : 0.028028, loss_ce: 0.006313\n",
            "iteration 9953 : loss : 0.025633, loss_ce: 0.008084\n",
            "iteration 9954 : loss : 0.026126, loss_ce: 0.008743\n",
            "iteration 9955 : loss : 0.073129, loss_ce: 0.005566\n",
            "iteration 9956 : loss : 0.025962, loss_ce: 0.008608\n",
            "iteration 9957 : loss : 0.022433, loss_ce: 0.009088\n",
            "iteration 9958 : loss : 0.024859, loss_ce: 0.008172\n",
            "iteration 9959 : loss : 0.017040, loss_ce: 0.005317\n",
            "iteration 9960 : loss : 0.022137, loss_ce: 0.007093\n",
            "iteration 9961 : loss : 0.070860, loss_ce: 0.007213\n",
            "iteration 9962 : loss : 0.026622, loss_ce: 0.008216\n",
            "iteration 9963 : loss : 0.026855, loss_ce: 0.005955\n",
            "iteration 9964 : loss : 0.025588, loss_ce: 0.005541\n",
            "iteration 9965 : loss : 0.033402, loss_ce: 0.011148\n",
            "iteration 9966 : loss : 0.026029, loss_ce: 0.009948\n",
            "iteration 9967 : loss : 0.026544, loss_ce: 0.012047\n",
            "iteration 9968 : loss : 0.079223, loss_ce: 0.007402\n",
            "iteration 9969 : loss : 0.025274, loss_ce: 0.009073\n",
            "iteration 9970 : loss : 0.079375, loss_ce: 0.009531\n",
            "iteration 9971 : loss : 0.025819, loss_ce: 0.012143\n",
            "iteration 9972 : loss : 0.027259, loss_ce: 0.008994\n",
            "iteration 9973 : loss : 0.034011, loss_ce: 0.008325\n",
            "iteration 9974 : loss : 0.076515, loss_ce: 0.006855\n",
            "iteration 9975 : loss : 0.029487, loss_ce: 0.008562\n",
            "iteration 9976 : loss : 0.022779, loss_ce: 0.006231\n",
            "iteration 9977 : loss : 0.021266, loss_ce: 0.007813\n",
            "iteration 9978 : loss : 0.023730, loss_ce: 0.005032\n",
            "iteration 9979 : loss : 0.023671, loss_ce: 0.006309\n",
            "iteration 9980 : loss : 0.025378, loss_ce: 0.007013\n",
            "iteration 9981 : loss : 0.028368, loss_ce: 0.012394\n",
            "iteration 9982 : loss : 0.029342, loss_ce: 0.007681\n",
            "iteration 9983 : loss : 0.031473, loss_ce: 0.011078\n",
            "iteration 9984 : loss : 0.074745, loss_ce: 0.007575\n",
            "iteration 9985 : loss : 0.021906, loss_ce: 0.007436\n",
            "iteration 9986 : loss : 0.030631, loss_ce: 0.010452\n",
            "iteration 9987 : loss : 0.026024, loss_ce: 0.010102\n",
            "iteration 9988 : loss : 0.072654, loss_ce: 0.004949\n",
            "iteration 9989 : loss : 0.026887, loss_ce: 0.010712\n",
            "iteration 9990 : loss : 0.029573, loss_ce: 0.007672\n",
            "iteration 9991 : loss : 0.025473, loss_ce: 0.009651\n",
            "iteration 9992 : loss : 0.024986, loss_ce: 0.007801\n",
            "iteration 9993 : loss : 0.023485, loss_ce: 0.006183\n",
            "iteration 9994 : loss : 0.026535, loss_ce: 0.013931\n",
            "iteration 9995 : loss : 0.037191, loss_ce: 0.007433\n",
            "iteration 9996 : loss : 0.024822, loss_ce: 0.006869\n",
            "iteration 9997 : loss : 0.025414, loss_ce: 0.009489\n",
            "iteration 9998 : loss : 0.035259, loss_ce: 0.010313\n",
            "iteration 9999 : loss : 0.023105, loss_ce: 0.008539\n",
            "iteration 10000 : loss : 0.022198, loss_ce: 0.010461\n",
            "iteration 10001 : loss : 0.038224, loss_ce: 0.008851\n",
            "iteration 10002 : loss : 0.024890, loss_ce: 0.010163\n",
            "iteration 10003 : loss : 0.025925, loss_ce: 0.008747\n",
            "iteration 10004 : loss : 0.029220, loss_ce: 0.011862\n",
            "iteration 10005 : loss : 0.029287, loss_ce: 0.007103\n",
            "iteration 10006 : loss : 0.019186, loss_ce: 0.006685\n",
            "iteration 10007 : loss : 0.028938, loss_ce: 0.012765\n",
            "iteration 10008 : loss : 0.024470, loss_ce: 0.011641\n",
            "iteration 10009 : loss : 0.031715, loss_ce: 0.015453\n",
            "iteration 10010 : loss : 0.025560, loss_ce: 0.009007\n",
            "iteration 10011 : loss : 0.029971, loss_ce: 0.007928\n",
            "iteration 10012 : loss : 0.027175, loss_ce: 0.012655\n",
            "iteration 10013 : loss : 0.050874, loss_ce: 0.006718\n",
            "iteration 10014 : loss : 0.025086, loss_ce: 0.009342\n",
            "iteration 10015 : loss : 0.028564, loss_ce: 0.008360\n",
            "iteration 10016 : loss : 0.026232, loss_ce: 0.006724\n",
            "iteration 10017 : loss : 0.027062, loss_ce: 0.007226\n",
            "iteration 10018 : loss : 0.022788, loss_ce: 0.008370\n",
            "iteration 10019 : loss : 0.077183, loss_ce: 0.004465\n",
            "iteration 10020 : loss : 0.030342, loss_ce: 0.008896\n",
            "iteration 10021 : loss : 0.025935, loss_ce: 0.008749\n",
            "iteration 10022 : loss : 0.033850, loss_ce: 0.012094\n",
            "iteration 10023 : loss : 0.024116, loss_ce: 0.006151\n",
            "iteration 10024 : loss : 0.025998, loss_ce: 0.010659\n",
            "iteration 10025 : loss : 0.025798, loss_ce: 0.006841\n",
            "iteration 10026 : loss : 0.030658, loss_ce: 0.013090\n",
            "iteration 10027 : loss : 0.027477, loss_ce: 0.011257\n",
            "iteration 10028 : loss : 0.032041, loss_ce: 0.009858\n",
            "iteration 10029 : loss : 0.025414, loss_ce: 0.012276\n",
            "iteration 10030 : loss : 0.022631, loss_ce: 0.009150\n",
            "iteration 10031 : loss : 0.021400, loss_ce: 0.005904\n",
            "iteration 10032 : loss : 0.023687, loss_ce: 0.004843\n",
            "iteration 10033 : loss : 0.025913, loss_ce: 0.009433\n",
            "iteration 10034 : loss : 0.023678, loss_ce: 0.008691\n",
            "iteration 10035 : loss : 0.025151, loss_ce: 0.008022\n",
            "iteration 10036 : loss : 0.025797, loss_ce: 0.013136\n",
            "iteration 10037 : loss : 0.126535, loss_ce: 0.006510\n",
            "iteration 10038 : loss : 0.027283, loss_ce: 0.009765\n",
            "iteration 10039 : loss : 0.021023, loss_ce: 0.007556\n",
            "iteration 10040 : loss : 0.029814, loss_ce: 0.011428\n",
            "iteration 10041 : loss : 0.026036, loss_ce: 0.009156\n",
            "iteration 10042 : loss : 0.072841, loss_ce: 0.003287\n",
            "iteration 10043 : loss : 0.021307, loss_ce: 0.008581\n",
            "iteration 10044 : loss : 0.335999, loss_ce: 0.002558\n",
            " 72%|██████████████████▋       | 108/150 [3:56:22<1:31:27, 130.65s/it]iteration 10045 : loss : 0.027042, loss_ce: 0.013616\n",
            "iteration 10046 : loss : 0.025643, loss_ce: 0.011026\n",
            "iteration 10047 : loss : 0.029884, loss_ce: 0.009852\n",
            "iteration 10048 : loss : 0.023154, loss_ce: 0.010475\n",
            "iteration 10049 : loss : 0.023810, loss_ce: 0.009075\n",
            "iteration 10050 : loss : 0.036946, loss_ce: 0.014761\n",
            "iteration 10051 : loss : 0.030232, loss_ce: 0.008000\n",
            "iteration 10052 : loss : 0.043361, loss_ce: 0.010474\n",
            "iteration 10053 : loss : 0.025637, loss_ce: 0.007210\n",
            "iteration 10054 : loss : 0.024505, loss_ce: 0.005075\n",
            "iteration 10055 : loss : 0.025472, loss_ce: 0.008505\n",
            "iteration 10056 : loss : 0.022106, loss_ce: 0.003684\n",
            "iteration 10057 : loss : 0.026911, loss_ce: 0.011325\n",
            "iteration 10058 : loss : 0.025290, loss_ce: 0.011862\n",
            "iteration 10059 : loss : 0.022472, loss_ce: 0.007870\n",
            "iteration 10060 : loss : 0.024775, loss_ce: 0.011025\n",
            "iteration 10061 : loss : 0.027593, loss_ce: 0.006600\n",
            "iteration 10062 : loss : 0.026447, loss_ce: 0.010898\n",
            "iteration 10063 : loss : 0.021899, loss_ce: 0.005965\n",
            "iteration 10064 : loss : 0.019651, loss_ce: 0.005606\n",
            "iteration 10065 : loss : 0.028750, loss_ce: 0.010878\n",
            "iteration 10066 : loss : 0.034130, loss_ce: 0.005574\n",
            "iteration 10067 : loss : 0.023548, loss_ce: 0.007642\n",
            "iteration 10068 : loss : 0.030818, loss_ce: 0.005807\n",
            "iteration 10069 : loss : 0.031405, loss_ce: 0.009672\n",
            "iteration 10070 : loss : 0.026937, loss_ce: 0.007040\n",
            "iteration 10071 : loss : 0.024171, loss_ce: 0.008884\n",
            "iteration 10072 : loss : 0.028927, loss_ce: 0.009194\n",
            "iteration 10073 : loss : 0.021971, loss_ce: 0.009996\n",
            "iteration 10074 : loss : 0.023141, loss_ce: 0.006877\n",
            "iteration 10075 : loss : 0.040353, loss_ce: 0.006874\n",
            "iteration 10076 : loss : 0.025767, loss_ce: 0.007993\n",
            "iteration 10077 : loss : 0.043946, loss_ce: 0.006774\n",
            "iteration 10078 : loss : 0.032575, loss_ce: 0.011956\n",
            "iteration 10079 : loss : 0.023303, loss_ce: 0.008548\n",
            "iteration 10080 : loss : 0.025724, loss_ce: 0.009839\n",
            "iteration 10081 : loss : 0.026289, loss_ce: 0.011639\n",
            "iteration 10082 : loss : 0.025781, loss_ce: 0.006569\n",
            "iteration 10083 : loss : 0.027694, loss_ce: 0.011857\n",
            "iteration 10084 : loss : 0.028767, loss_ce: 0.007445\n",
            "iteration 10085 : loss : 0.024947, loss_ce: 0.006590\n",
            "iteration 10086 : loss : 0.024906, loss_ce: 0.010980\n",
            "iteration 10087 : loss : 0.026865, loss_ce: 0.009304\n",
            "iteration 10088 : loss : 0.134199, loss_ce: 0.005027\n",
            "iteration 10089 : loss : 0.024037, loss_ce: 0.011876\n",
            "iteration 10090 : loss : 0.023563, loss_ce: 0.004340\n",
            "iteration 10091 : loss : 0.031298, loss_ce: 0.006899\n",
            "iteration 10092 : loss : 0.038383, loss_ce: 0.010799\n",
            "iteration 10093 : loss : 0.033243, loss_ce: 0.008679\n",
            "iteration 10094 : loss : 0.026039, loss_ce: 0.004742\n",
            "iteration 10095 : loss : 0.027102, loss_ce: 0.010369\n",
            "iteration 10096 : loss : 0.023126, loss_ce: 0.008518\n",
            "iteration 10097 : loss : 0.028194, loss_ce: 0.011829\n",
            "iteration 10098 : loss : 0.037551, loss_ce: 0.008180\n",
            "iteration 10099 : loss : 0.027684, loss_ce: 0.010453\n",
            "iteration 10100 : loss : 0.025426, loss_ce: 0.006430\n",
            "iteration 10101 : loss : 0.025538, loss_ce: 0.007830\n",
            "iteration 10102 : loss : 0.027355, loss_ce: 0.007355\n",
            "iteration 10103 : loss : 0.036370, loss_ce: 0.005313\n",
            "iteration 10104 : loss : 0.027091, loss_ce: 0.010131\n",
            "iteration 10105 : loss : 0.027690, loss_ce: 0.006640\n",
            "iteration 10106 : loss : 0.030452, loss_ce: 0.017349\n",
            "iteration 10107 : loss : 0.026891, loss_ce: 0.008826\n",
            "iteration 10108 : loss : 0.025506, loss_ce: 0.008095\n",
            "iteration 10109 : loss : 0.026516, loss_ce: 0.008572\n",
            "iteration 10110 : loss : 0.024859, loss_ce: 0.011345\n",
            "iteration 10111 : loss : 0.026050, loss_ce: 0.008235\n",
            "iteration 10112 : loss : 0.027046, loss_ce: 0.010301\n",
            "iteration 10113 : loss : 0.022231, loss_ce: 0.009050\n",
            "iteration 10114 : loss : 0.021476, loss_ce: 0.007595\n",
            "iteration 10115 : loss : 0.028623, loss_ce: 0.011785\n",
            "iteration 10116 : loss : 0.026548, loss_ce: 0.011564\n",
            "iteration 10117 : loss : 0.023639, loss_ce: 0.004488\n",
            "iteration 10118 : loss : 0.021968, loss_ce: 0.006952\n",
            "iteration 10119 : loss : 0.023089, loss_ce: 0.005862\n",
            "iteration 10120 : loss : 0.019627, loss_ce: 0.007048\n",
            "iteration 10121 : loss : 0.024030, loss_ce: 0.005995\n",
            "iteration 10122 : loss : 0.024990, loss_ce: 0.006021\n",
            "iteration 10123 : loss : 0.024454, loss_ce: 0.009247\n",
            "iteration 10124 : loss : 0.025003, loss_ce: 0.006777\n",
            "iteration 10125 : loss : 0.022472, loss_ce: 0.006867\n",
            "iteration 10126 : loss : 0.024149, loss_ce: 0.007580\n",
            "iteration 10127 : loss : 0.026593, loss_ce: 0.007324\n",
            "iteration 10128 : loss : 0.021346, loss_ce: 0.009757\n",
            "iteration 10129 : loss : 0.025948, loss_ce: 0.007568\n",
            "iteration 10130 : loss : 0.024723, loss_ce: 0.008074\n",
            "iteration 10131 : loss : 0.026089, loss_ce: 0.009811\n",
            "iteration 10132 : loss : 0.024371, loss_ce: 0.006372\n",
            "iteration 10133 : loss : 0.075976, loss_ce: 0.007359\n",
            "iteration 10134 : loss : 0.028977, loss_ce: 0.012667\n",
            "iteration 10135 : loss : 0.027801, loss_ce: 0.011624\n",
            "iteration 10136 : loss : 0.024082, loss_ce: 0.009874\n",
            "iteration 10137 : loss : 0.285224, loss_ce: 0.006007\n",
            " 73%|██████████████████▉       | 109/150 [3:58:33<1:29:32, 131.05s/it]iteration 10138 : loss : 0.024581, loss_ce: 0.009826\n",
            "iteration 10139 : loss : 0.024600, loss_ce: 0.007672\n",
            "iteration 10140 : loss : 0.029909, loss_ce: 0.014357\n",
            "iteration 10141 : loss : 0.024724, loss_ce: 0.009101\n",
            "iteration 10142 : loss : 0.022370, loss_ce: 0.008771\n",
            "iteration 10143 : loss : 0.079595, loss_ce: 0.009737\n",
            "iteration 10144 : loss : 0.032729, loss_ce: 0.012346\n",
            "iteration 10145 : loss : 0.024673, loss_ce: 0.008737\n",
            "iteration 10146 : loss : 0.023872, loss_ce: 0.003577\n",
            "iteration 10147 : loss : 0.020486, loss_ce: 0.007217\n",
            "iteration 10148 : loss : 0.024369, loss_ce: 0.009359\n",
            "iteration 10149 : loss : 0.021127, loss_ce: 0.005735\n",
            "iteration 10150 : loss : 0.023226, loss_ce: 0.004712\n",
            "iteration 10151 : loss : 0.029790, loss_ce: 0.009673\n",
            "iteration 10152 : loss : 0.078067, loss_ce: 0.009080\n",
            "iteration 10153 : loss : 0.032000, loss_ce: 0.004821\n",
            "iteration 10154 : loss : 0.029370, loss_ce: 0.010765\n",
            "iteration 10155 : loss : 0.024131, loss_ce: 0.004915\n",
            "iteration 10156 : loss : 0.072987, loss_ce: 0.005921\n",
            "iteration 10157 : loss : 0.029107, loss_ce: 0.010241\n",
            "iteration 10158 : loss : 0.023880, loss_ce: 0.006986\n",
            "iteration 10159 : loss : 0.021956, loss_ce: 0.006718\n",
            "iteration 10160 : loss : 0.030987, loss_ce: 0.007997\n",
            "iteration 10161 : loss : 0.023249, loss_ce: 0.009365\n",
            "iteration 10162 : loss : 0.031032, loss_ce: 0.010135\n",
            "iteration 10163 : loss : 0.031589, loss_ce: 0.013988\n",
            "iteration 10164 : loss : 0.026420, loss_ce: 0.008759\n",
            "iteration 10165 : loss : 0.026900, loss_ce: 0.007264\n",
            "iteration 10166 : loss : 0.025686, loss_ce: 0.007209\n",
            "iteration 10167 : loss : 0.027806, loss_ce: 0.012968\n",
            "iteration 10168 : loss : 0.027854, loss_ce: 0.008177\n",
            "iteration 10169 : loss : 0.023205, loss_ce: 0.009579\n",
            "iteration 10170 : loss : 0.073625, loss_ce: 0.008482\n",
            "iteration 10171 : loss : 0.022085, loss_ce: 0.010954\n",
            "iteration 10172 : loss : 0.022800, loss_ce: 0.008543\n",
            "iteration 10173 : loss : 0.079707, loss_ce: 0.003922\n",
            "iteration 10174 : loss : 0.028571, loss_ce: 0.008845\n",
            "iteration 10175 : loss : 0.067916, loss_ce: 0.009285\n",
            "iteration 10176 : loss : 0.078669, loss_ce: 0.011931\n",
            "iteration 10177 : loss : 0.023564, loss_ce: 0.006298\n",
            "iteration 10178 : loss : 0.026034, loss_ce: 0.009940\n",
            "iteration 10179 : loss : 0.021537, loss_ce: 0.006762\n",
            "iteration 10180 : loss : 0.029903, loss_ce: 0.009468\n",
            "iteration 10181 : loss : 0.035640, loss_ce: 0.006774\n",
            "iteration 10182 : loss : 0.026792, loss_ce: 0.007370\n",
            "iteration 10183 : loss : 0.027716, loss_ce: 0.009634\n",
            "iteration 10184 : loss : 0.030190, loss_ce: 0.005197\n",
            "iteration 10185 : loss : 0.024954, loss_ce: 0.008153\n",
            "iteration 10186 : loss : 0.026108, loss_ce: 0.009415\n",
            "iteration 10187 : loss : 0.075647, loss_ce: 0.007356\n",
            "iteration 10188 : loss : 0.023359, loss_ce: 0.008788\n",
            "iteration 10189 : loss : 0.023142, loss_ce: 0.008765\n",
            "iteration 10190 : loss : 0.024288, loss_ce: 0.010160\n",
            "iteration 10191 : loss : 0.024422, loss_ce: 0.007231\n",
            "iteration 10192 : loss : 0.131157, loss_ce: 0.001658\n",
            "iteration 10193 : loss : 0.026130, loss_ce: 0.009030\n",
            "iteration 10194 : loss : 0.025586, loss_ce: 0.008131\n",
            "iteration 10195 : loss : 0.028557, loss_ce: 0.014904\n",
            "iteration 10196 : loss : 0.025883, loss_ce: 0.007894\n",
            "iteration 10197 : loss : 0.076203, loss_ce: 0.004659\n",
            "iteration 10198 : loss : 0.021836, loss_ce: 0.006711\n",
            "iteration 10199 : loss : 0.026444, loss_ce: 0.011701\n",
            "iteration 10200 : loss : 0.026730, loss_ce: 0.010489\n",
            "iteration 10201 : loss : 0.022618, loss_ce: 0.010139\n",
            "iteration 10202 : loss : 0.029422, loss_ce: 0.009372\n",
            "iteration 10203 : loss : 0.077777, loss_ce: 0.008520\n",
            "iteration 10204 : loss : 0.030316, loss_ce: 0.010162\n",
            "iteration 10205 : loss : 0.026065, loss_ce: 0.005734\n",
            "iteration 10206 : loss : 0.078430, loss_ce: 0.009144\n",
            "iteration 10207 : loss : 0.022653, loss_ce: 0.007672\n",
            "iteration 10208 : loss : 0.024161, loss_ce: 0.008740\n",
            "iteration 10209 : loss : 0.066423, loss_ce: 0.002055\n",
            "iteration 10210 : loss : 0.027015, loss_ce: 0.009689\n",
            "iteration 10211 : loss : 0.029490, loss_ce: 0.012489\n",
            "iteration 10212 : loss : 0.026067, loss_ce: 0.010884\n",
            "iteration 10213 : loss : 0.017916, loss_ce: 0.005917\n",
            "iteration 10214 : loss : 0.029730, loss_ce: 0.009198\n",
            "iteration 10215 : loss : 0.027080, loss_ce: 0.009649\n",
            "iteration 10216 : loss : 0.028529, loss_ce: 0.013344\n",
            "iteration 10217 : loss : 0.021991, loss_ce: 0.010622\n",
            "iteration 10218 : loss : 0.031904, loss_ce: 0.005801\n",
            "iteration 10219 : loss : 0.028269, loss_ce: 0.012297\n",
            "iteration 10220 : loss : 0.031979, loss_ce: 0.007838\n",
            "iteration 10221 : loss : 0.025061, loss_ce: 0.006704\n",
            "iteration 10222 : loss : 0.034238, loss_ce: 0.008492\n",
            "iteration 10223 : loss : 0.028305, loss_ce: 0.017392\n",
            "iteration 10224 : loss : 0.026513, loss_ce: 0.008969\n",
            "iteration 10225 : loss : 0.026309, loss_ce: 0.009448\n",
            "iteration 10226 : loss : 0.023967, loss_ce: 0.007205\n",
            "iteration 10227 : loss : 0.021495, loss_ce: 0.007886\n",
            "iteration 10228 : loss : 0.032671, loss_ce: 0.008099\n",
            "iteration 10229 : loss : 0.076840, loss_ce: 0.006481\n",
            "iteration 10230 : loss : 0.091458, loss_ce: 0.010793\n",
            " 73%|███████████████████       | 110/150 [4:00:43<1:27:08, 130.72s/it]iteration 10231 : loss : 0.023222, loss_ce: 0.009231\n",
            "iteration 10232 : loss : 0.027647, loss_ce: 0.008994\n",
            "iteration 10233 : loss : 0.025893, loss_ce: 0.009773\n",
            "iteration 10234 : loss : 0.024649, loss_ce: 0.008231\n",
            "iteration 10235 : loss : 0.029116, loss_ce: 0.014782\n",
            "iteration 10236 : loss : 0.027449, loss_ce: 0.014057\n",
            "iteration 10237 : loss : 0.029610, loss_ce: 0.010245\n",
            "iteration 10238 : loss : 0.028582, loss_ce: 0.011737\n",
            "iteration 10239 : loss : 0.023550, loss_ce: 0.010024\n",
            "iteration 10240 : loss : 0.022907, loss_ce: 0.008272\n",
            "iteration 10241 : loss : 0.027789, loss_ce: 0.010525\n",
            "iteration 10242 : loss : 0.027540, loss_ce: 0.011852\n",
            "iteration 10243 : loss : 0.024038, loss_ce: 0.010732\n",
            "iteration 10244 : loss : 0.030024, loss_ce: 0.013018\n",
            "iteration 10245 : loss : 0.029471, loss_ce: 0.009310\n",
            "iteration 10246 : loss : 0.019766, loss_ce: 0.004355\n",
            "iteration 10247 : loss : 0.023176, loss_ce: 0.007433\n",
            "iteration 10248 : loss : 0.026194, loss_ce: 0.009729\n",
            "iteration 10249 : loss : 0.026205, loss_ce: 0.005546\n",
            "iteration 10250 : loss : 0.024742, loss_ce: 0.006610\n",
            "iteration 10251 : loss : 0.024793, loss_ce: 0.008202\n",
            "iteration 10252 : loss : 0.022444, loss_ce: 0.009046\n",
            "iteration 10253 : loss : 0.026620, loss_ce: 0.012505\n",
            "iteration 10254 : loss : 0.023964, loss_ce: 0.009606\n",
            "iteration 10255 : loss : 0.022893, loss_ce: 0.008583\n",
            "iteration 10256 : loss : 0.022606, loss_ce: 0.005956\n",
            "iteration 10257 : loss : 0.025158, loss_ce: 0.006253\n",
            "iteration 10258 : loss : 0.026606, loss_ce: 0.011196\n",
            "iteration 10259 : loss : 0.023674, loss_ce: 0.010397\n",
            "iteration 10260 : loss : 0.020742, loss_ce: 0.006322\n",
            "iteration 10261 : loss : 0.075715, loss_ce: 0.009686\n",
            "iteration 10262 : loss : 0.023803, loss_ce: 0.010677\n",
            "iteration 10263 : loss : 0.020630, loss_ce: 0.006714\n",
            "iteration 10264 : loss : 0.022948, loss_ce: 0.004877\n",
            "iteration 10265 : loss : 0.025867, loss_ce: 0.011231\n",
            "iteration 10266 : loss : 0.026977, loss_ce: 0.009190\n",
            "iteration 10267 : loss : 0.022142, loss_ce: 0.010485\n",
            "iteration 10268 : loss : 0.022896, loss_ce: 0.007048\n",
            "iteration 10269 : loss : 0.023606, loss_ce: 0.007177\n",
            "iteration 10270 : loss : 0.024233, loss_ce: 0.007517\n",
            "iteration 10271 : loss : 0.027796, loss_ce: 0.010911\n",
            "iteration 10272 : loss : 0.046034, loss_ce: 0.009602\n",
            "iteration 10273 : loss : 0.023545, loss_ce: 0.006713\n",
            "iteration 10274 : loss : 0.021588, loss_ce: 0.006002\n",
            "iteration 10275 : loss : 0.020561, loss_ce: 0.007110\n",
            "iteration 10276 : loss : 0.023181, loss_ce: 0.006932\n",
            "iteration 10277 : loss : 0.022111, loss_ce: 0.005243\n",
            "iteration 10278 : loss : 0.017736, loss_ce: 0.007011\n",
            "iteration 10279 : loss : 0.022461, loss_ce: 0.007845\n",
            "iteration 10280 : loss : 0.079748, loss_ce: 0.006003\n",
            "iteration 10281 : loss : 0.023238, loss_ce: 0.007910\n",
            "iteration 10282 : loss : 0.024869, loss_ce: 0.011362\n",
            "iteration 10283 : loss : 0.026836, loss_ce: 0.011024\n",
            "iteration 10284 : loss : 0.076011, loss_ce: 0.005664\n",
            "iteration 10285 : loss : 0.024207, loss_ce: 0.005116\n",
            "iteration 10286 : loss : 0.022996, loss_ce: 0.007142\n",
            "iteration 10287 : loss : 0.026243, loss_ce: 0.009328\n",
            "iteration 10288 : loss : 0.026657, loss_ce: 0.010188\n",
            "iteration 10289 : loss : 0.028723, loss_ce: 0.009931\n",
            "iteration 10290 : loss : 0.021836, loss_ce: 0.005893\n",
            "iteration 10291 : loss : 0.029602, loss_ce: 0.008906\n",
            "iteration 10292 : loss : 0.028381, loss_ce: 0.010608\n",
            "iteration 10293 : loss : 0.029067, loss_ce: 0.008160\n",
            "iteration 10294 : loss : 0.025147, loss_ce: 0.006388\n",
            "iteration 10295 : loss : 0.024966, loss_ce: 0.010432\n",
            "iteration 10296 : loss : 0.026284, loss_ce: 0.007127\n",
            "iteration 10297 : loss : 0.027377, loss_ce: 0.004296\n",
            "iteration 10298 : loss : 0.025817, loss_ce: 0.010360\n",
            "iteration 10299 : loss : 0.029709, loss_ce: 0.009790\n",
            "iteration 10300 : loss : 0.021953, loss_ce: 0.006397\n",
            "iteration 10301 : loss : 0.027167, loss_ce: 0.008580\n",
            "iteration 10302 : loss : 0.025604, loss_ce: 0.009359\n",
            "iteration 10303 : loss : 0.027625, loss_ce: 0.008564\n",
            "iteration 10304 : loss : 0.026600, loss_ce: 0.008563\n",
            "iteration 10305 : loss : 0.078139, loss_ce: 0.009967\n",
            "iteration 10306 : loss : 0.030348, loss_ce: 0.006974\n",
            "iteration 10307 : loss : 0.028465, loss_ce: 0.006928\n",
            "iteration 10308 : loss : 0.023654, loss_ce: 0.007714\n",
            "iteration 10309 : loss : 0.035410, loss_ce: 0.006761\n",
            "iteration 10310 : loss : 0.029401, loss_ce: 0.004863\n",
            "iteration 10311 : loss : 0.026509, loss_ce: 0.009381\n",
            "iteration 10312 : loss : 0.023475, loss_ce: 0.004935\n",
            "iteration 10313 : loss : 0.028266, loss_ce: 0.009032\n",
            "iteration 10314 : loss : 0.024789, loss_ce: 0.011778\n",
            "iteration 10315 : loss : 0.025674, loss_ce: 0.008107\n",
            "iteration 10316 : loss : 0.033259, loss_ce: 0.009686\n",
            "iteration 10317 : loss : 0.031210, loss_ce: 0.007688\n",
            "iteration 10318 : loss : 0.030983, loss_ce: 0.006721\n",
            "iteration 10319 : loss : 0.025787, loss_ce: 0.008158\n",
            "iteration 10320 : loss : 0.023751, loss_ce: 0.009676\n",
            "iteration 10321 : loss : 0.025533, loss_ce: 0.010610\n",
            "iteration 10322 : loss : 0.020440, loss_ce: 0.005813\n",
            "iteration 10323 : loss : 0.091126, loss_ce: 0.020510\n",
            " 74%|███████████████████▏      | 111/150 [4:02:54<1:24:53, 130.60s/it]iteration 10324 : loss : 0.074566, loss_ce: 0.008923\n",
            "iteration 10325 : loss : 0.026798, loss_ce: 0.007660\n",
            "iteration 10326 : loss : 0.029269, loss_ce: 0.012238\n",
            "iteration 10327 : loss : 0.025687, loss_ce: 0.011699\n",
            "iteration 10328 : loss : 0.023273, loss_ce: 0.008470\n",
            "iteration 10329 : loss : 0.025375, loss_ce: 0.010651\n",
            "iteration 10330 : loss : 0.024322, loss_ce: 0.010330\n",
            "iteration 10331 : loss : 0.026950, loss_ce: 0.008361\n",
            "iteration 10332 : loss : 0.026060, loss_ce: 0.007333\n",
            "iteration 10333 : loss : 0.021265, loss_ce: 0.008091\n",
            "iteration 10334 : loss : 0.027534, loss_ce: 0.007933\n",
            "iteration 10335 : loss : 0.027293, loss_ce: 0.008359\n",
            "iteration 10336 : loss : 0.078692, loss_ce: 0.011401\n",
            "iteration 10337 : loss : 0.030427, loss_ce: 0.008366\n",
            "iteration 10338 : loss : 0.036692, loss_ce: 0.008589\n",
            "iteration 10339 : loss : 0.028336, loss_ce: 0.010871\n",
            "iteration 10340 : loss : 0.022386, loss_ce: 0.007295\n",
            "iteration 10341 : loss : 0.026439, loss_ce: 0.004672\n",
            "iteration 10342 : loss : 0.026148, loss_ce: 0.003141\n",
            "iteration 10343 : loss : 0.024404, loss_ce: 0.009996\n",
            "iteration 10344 : loss : 0.022205, loss_ce: 0.006960\n",
            "iteration 10345 : loss : 0.023779, loss_ce: 0.009834\n",
            "iteration 10346 : loss : 0.023742, loss_ce: 0.009617\n",
            "iteration 10347 : loss : 0.022123, loss_ce: 0.005500\n",
            "iteration 10348 : loss : 0.028805, loss_ce: 0.008985\n",
            "iteration 10349 : loss : 0.024693, loss_ce: 0.003489\n",
            "iteration 10350 : loss : 0.025266, loss_ce: 0.010663\n",
            "iteration 10351 : loss : 0.126182, loss_ce: 0.005334\n",
            "iteration 10352 : loss : 0.025694, loss_ce: 0.013957\n",
            "iteration 10353 : loss : 0.026657, loss_ce: 0.008611\n",
            "iteration 10354 : loss : 0.027997, loss_ce: 0.011401\n",
            "iteration 10355 : loss : 0.022964, loss_ce: 0.009253\n",
            "iteration 10356 : loss : 0.019715, loss_ce: 0.007447\n",
            "iteration 10357 : loss : 0.023007, loss_ce: 0.009271\n",
            "iteration 10358 : loss : 0.027698, loss_ce: 0.012542\n",
            "iteration 10359 : loss : 0.076121, loss_ce: 0.005503\n",
            "iteration 10360 : loss : 0.021673, loss_ce: 0.008202\n",
            "iteration 10361 : loss : 0.028196, loss_ce: 0.007923\n",
            "iteration 10362 : loss : 0.032094, loss_ce: 0.007451\n",
            "iteration 10363 : loss : 0.022660, loss_ce: 0.008003\n",
            "iteration 10364 : loss : 0.018368, loss_ce: 0.004814\n",
            "iteration 10365 : loss : 0.021670, loss_ce: 0.008633\n",
            "iteration 10366 : loss : 0.024466, loss_ce: 0.008380\n",
            "iteration 10367 : loss : 0.024306, loss_ce: 0.009925\n",
            "iteration 10368 : loss : 0.075229, loss_ce: 0.008453\n",
            "iteration 10369 : loss : 0.030234, loss_ce: 0.006654\n",
            "iteration 10370 : loss : 0.026519, loss_ce: 0.008424\n",
            "iteration 10371 : loss : 0.074660, loss_ce: 0.003129\n",
            "iteration 10372 : loss : 0.024950, loss_ce: 0.006321\n",
            "iteration 10373 : loss : 0.024280, loss_ce: 0.011153\n",
            "iteration 10374 : loss : 0.021371, loss_ce: 0.005309\n",
            "iteration 10375 : loss : 0.025663, loss_ce: 0.009605\n",
            "iteration 10376 : loss : 0.025762, loss_ce: 0.009237\n",
            "iteration 10377 : loss : 0.021032, loss_ce: 0.008090\n",
            "iteration 10378 : loss : 0.027922, loss_ce: 0.009243\n",
            "iteration 10379 : loss : 0.027208, loss_ce: 0.009334\n",
            "iteration 10380 : loss : 0.020862, loss_ce: 0.010433\n",
            "iteration 10381 : loss : 0.074365, loss_ce: 0.008017\n",
            "iteration 10382 : loss : 0.029750, loss_ce: 0.007345\n",
            "iteration 10383 : loss : 0.027177, loss_ce: 0.011395\n",
            "iteration 10384 : loss : 0.026319, loss_ce: 0.006747\n",
            "iteration 10385 : loss : 0.030071, loss_ce: 0.007999\n",
            "iteration 10386 : loss : 0.026530, loss_ce: 0.010610\n",
            "iteration 10387 : loss : 0.029873, loss_ce: 0.012356\n",
            "iteration 10388 : loss : 0.027034, loss_ce: 0.008346\n",
            "iteration 10389 : loss : 0.020488, loss_ce: 0.007519\n",
            "iteration 10390 : loss : 0.021972, loss_ce: 0.009724\n",
            "iteration 10391 : loss : 0.030061, loss_ce: 0.009941\n",
            "iteration 10392 : loss : 0.028321, loss_ce: 0.009734\n",
            "iteration 10393 : loss : 0.027127, loss_ce: 0.007686\n",
            "iteration 10394 : loss : 0.022457, loss_ce: 0.006569\n",
            "iteration 10395 : loss : 0.032188, loss_ce: 0.005934\n",
            "iteration 10396 : loss : 0.076044, loss_ce: 0.008183\n",
            "iteration 10397 : loss : 0.031031, loss_ce: 0.007966\n",
            "iteration 10398 : loss : 0.025071, loss_ce: 0.009842\n",
            "iteration 10399 : loss : 0.021507, loss_ce: 0.009650\n",
            "iteration 10400 : loss : 0.022709, loss_ce: 0.004572\n",
            "iteration 10401 : loss : 0.020084, loss_ce: 0.006198\n",
            "iteration 10402 : loss : 0.026338, loss_ce: 0.007753\n",
            "iteration 10403 : loss : 0.025956, loss_ce: 0.010332\n",
            "iteration 10404 : loss : 0.025867, loss_ce: 0.008994\n",
            "iteration 10405 : loss : 0.028529, loss_ce: 0.011896\n",
            "iteration 10406 : loss : 0.023659, loss_ce: 0.007399\n",
            "iteration 10407 : loss : 0.023252, loss_ce: 0.007912\n",
            "iteration 10408 : loss : 0.032930, loss_ce: 0.005473\n",
            "iteration 10409 : loss : 0.025025, loss_ce: 0.009008\n",
            "iteration 10410 : loss : 0.028006, loss_ce: 0.006972\n",
            "iteration 10411 : loss : 0.025693, loss_ce: 0.008155\n",
            "iteration 10412 : loss : 0.028482, loss_ce: 0.010660\n",
            "iteration 10413 : loss : 0.027978, loss_ce: 0.009032\n",
            "iteration 10414 : loss : 0.062211, loss_ce: 0.008043\n",
            "iteration 10415 : loss : 0.023933, loss_ce: 0.008035\n",
            "iteration 10416 : loss : 0.135474, loss_ce: 0.014902\n",
            " 75%|███████████████████▍      | 112/150 [4:05:05<1:22:55, 130.94s/it]iteration 10417 : loss : 0.076650, loss_ce: 0.007076\n",
            "iteration 10418 : loss : 0.020653, loss_ce: 0.008153\n",
            "iteration 10419 : loss : 0.074883, loss_ce: 0.004937\n",
            "iteration 10420 : loss : 0.022926, loss_ce: 0.009033\n",
            "iteration 10421 : loss : 0.028634, loss_ce: 0.010078\n",
            "iteration 10422 : loss : 0.025890, loss_ce: 0.007771\n",
            "iteration 10423 : loss : 0.023613, loss_ce: 0.007851\n",
            "iteration 10424 : loss : 0.128528, loss_ce: 0.003834\n",
            "iteration 10425 : loss : 0.024490, loss_ce: 0.010531\n",
            "iteration 10426 : loss : 0.025001, loss_ce: 0.008179\n",
            "iteration 10427 : loss : 0.124997, loss_ce: 0.004397\n",
            "iteration 10428 : loss : 0.081523, loss_ce: 0.007206\n",
            "iteration 10429 : loss : 0.028795, loss_ce: 0.007911\n",
            "iteration 10430 : loss : 0.023983, loss_ce: 0.008442\n",
            "iteration 10431 : loss : 0.025969, loss_ce: 0.006443\n",
            "iteration 10432 : loss : 0.026754, loss_ce: 0.004749\n",
            "iteration 10433 : loss : 0.020418, loss_ce: 0.004667\n",
            "iteration 10434 : loss : 0.022866, loss_ce: 0.009754\n",
            "iteration 10435 : loss : 0.025241, loss_ce: 0.010492\n",
            "iteration 10436 : loss : 0.027513, loss_ce: 0.006727\n",
            "iteration 10437 : loss : 0.026332, loss_ce: 0.009443\n",
            "iteration 10438 : loss : 0.028304, loss_ce: 0.009805\n",
            "iteration 10439 : loss : 0.022486, loss_ce: 0.008472\n",
            "iteration 10440 : loss : 0.025867, loss_ce: 0.009700\n",
            "iteration 10441 : loss : 0.024929, loss_ce: 0.009095\n",
            "iteration 10442 : loss : 0.021922, loss_ce: 0.009916\n",
            "iteration 10443 : loss : 0.026464, loss_ce: 0.007946\n",
            "iteration 10444 : loss : 0.030036, loss_ce: 0.010037\n",
            "iteration 10445 : loss : 0.123391, loss_ce: 0.005419\n",
            "iteration 10446 : loss : 0.029033, loss_ce: 0.009355\n",
            "iteration 10447 : loss : 0.020401, loss_ce: 0.007588\n",
            "iteration 10448 : loss : 0.025011, loss_ce: 0.009036\n",
            "iteration 10449 : loss : 0.026295, loss_ce: 0.005874\n",
            "iteration 10450 : loss : 0.019173, loss_ce: 0.003282\n",
            "iteration 10451 : loss : 0.026558, loss_ce: 0.009540\n",
            "iteration 10452 : loss : 0.026808, loss_ce: 0.007390\n",
            "iteration 10453 : loss : 0.021869, loss_ce: 0.009273\n",
            "iteration 10454 : loss : 0.028932, loss_ce: 0.009300\n",
            "iteration 10455 : loss : 0.029662, loss_ce: 0.010934\n",
            "iteration 10456 : loss : 0.026493, loss_ce: 0.009574\n",
            "iteration 10457 : loss : 0.029698, loss_ce: 0.006940\n",
            "iteration 10458 : loss : 0.023654, loss_ce: 0.007446\n",
            "iteration 10459 : loss : 0.037839, loss_ce: 0.006915\n",
            "iteration 10460 : loss : 0.020879, loss_ce: 0.005695\n",
            "iteration 10461 : loss : 0.023448, loss_ce: 0.008608\n",
            "iteration 10462 : loss : 0.027735, loss_ce: 0.006003\n",
            "iteration 10463 : loss : 0.028697, loss_ce: 0.009512\n",
            "iteration 10464 : loss : 0.021329, loss_ce: 0.009703\n",
            "iteration 10465 : loss : 0.023341, loss_ce: 0.006797\n",
            "iteration 10466 : loss : 0.026029, loss_ce: 0.007739\n",
            "iteration 10467 : loss : 0.026658, loss_ce: 0.015107\n",
            "iteration 10468 : loss : 0.026525, loss_ce: 0.012000\n",
            "iteration 10469 : loss : 0.023632, loss_ce: 0.011301\n",
            "iteration 10470 : loss : 0.027407, loss_ce: 0.008386\n",
            "iteration 10471 : loss : 0.022114, loss_ce: 0.007580\n",
            "iteration 10472 : loss : 0.020645, loss_ce: 0.006683\n",
            "iteration 10473 : loss : 0.074000, loss_ce: 0.006797\n",
            "iteration 10474 : loss : 0.024547, loss_ce: 0.009525\n",
            "iteration 10475 : loss : 0.030184, loss_ce: 0.009929\n",
            "iteration 10476 : loss : 0.074561, loss_ce: 0.004198\n",
            "iteration 10477 : loss : 0.025265, loss_ce: 0.013576\n",
            "iteration 10478 : loss : 0.024676, loss_ce: 0.008288\n",
            "iteration 10479 : loss : 0.029263, loss_ce: 0.010267\n",
            "iteration 10480 : loss : 0.030471, loss_ce: 0.014117\n",
            "iteration 10481 : loss : 0.026639, loss_ce: 0.010160\n",
            "iteration 10482 : loss : 0.027417, loss_ce: 0.011156\n",
            "iteration 10483 : loss : 0.028923, loss_ce: 0.012157\n",
            "iteration 10484 : loss : 0.031408, loss_ce: 0.006364\n",
            "iteration 10485 : loss : 0.027277, loss_ce: 0.007994\n",
            "iteration 10486 : loss : 0.022719, loss_ce: 0.009556\n",
            "iteration 10487 : loss : 0.027378, loss_ce: 0.008996\n",
            "iteration 10488 : loss : 0.021252, loss_ce: 0.006594\n",
            "iteration 10489 : loss : 0.027320, loss_ce: 0.010171\n",
            "iteration 10490 : loss : 0.023568, loss_ce: 0.009799\n",
            "iteration 10491 : loss : 0.028066, loss_ce: 0.010049\n",
            "iteration 10492 : loss : 0.023747, loss_ce: 0.011408\n",
            "iteration 10493 : loss : 0.022425, loss_ce: 0.007760\n",
            "iteration 10494 : loss : 0.028463, loss_ce: 0.009935\n",
            "iteration 10495 : loss : 0.021767, loss_ce: 0.006950\n",
            "iteration 10496 : loss : 0.023640, loss_ce: 0.006190\n",
            "iteration 10497 : loss : 0.023543, loss_ce: 0.007495\n",
            "iteration 10498 : loss : 0.031843, loss_ce: 0.014042\n",
            "iteration 10499 : loss : 0.027954, loss_ce: 0.009868\n",
            "iteration 10500 : loss : 0.025954, loss_ce: 0.011335\n",
            "iteration 10501 : loss : 0.024249, loss_ce: 0.008433\n",
            "iteration 10502 : loss : 0.075242, loss_ce: 0.005234\n",
            "iteration 10503 : loss : 0.024550, loss_ce: 0.011419\n",
            "iteration 10504 : loss : 0.020336, loss_ce: 0.005845\n",
            "iteration 10505 : loss : 0.027190, loss_ce: 0.011333\n",
            "iteration 10506 : loss : 0.024877, loss_ce: 0.006748\n",
            "iteration 10507 : loss : 0.029150, loss_ce: 0.006347\n",
            "iteration 10508 : loss : 0.023737, loss_ce: 0.008472\n",
            "iteration 10509 : loss : 0.391971, loss_ce: 0.001285\n",
            " 75%|███████████████████▌      | 113/150 [4:07:16<1:20:38, 130.77s/it]iteration 10510 : loss : 0.030333, loss_ce: 0.005575\n",
            "iteration 10511 : loss : 0.026183, loss_ce: 0.008869\n",
            "iteration 10512 : loss : 0.077928, loss_ce: 0.007142\n",
            "iteration 10513 : loss : 0.025363, loss_ce: 0.007896\n",
            "iteration 10514 : loss : 0.027439, loss_ce: 0.010503\n",
            "iteration 10515 : loss : 0.028098, loss_ce: 0.011750\n",
            "iteration 10516 : loss : 0.028767, loss_ce: 0.010260\n",
            "iteration 10517 : loss : 0.037566, loss_ce: 0.012080\n",
            "iteration 10518 : loss : 0.024365, loss_ce: 0.011055\n",
            "iteration 10519 : loss : 0.035124, loss_ce: 0.008745\n",
            "iteration 10520 : loss : 0.027203, loss_ce: 0.008104\n",
            "iteration 10521 : loss : 0.024771, loss_ce: 0.008409\n",
            "iteration 10522 : loss : 0.025634, loss_ce: 0.011905\n",
            "iteration 10523 : loss : 0.020185, loss_ce: 0.005983\n",
            "iteration 10524 : loss : 0.052777, loss_ce: 0.005827\n",
            "iteration 10525 : loss : 0.020184, loss_ce: 0.006969\n",
            "iteration 10526 : loss : 0.072194, loss_ce: 0.003439\n",
            "iteration 10527 : loss : 0.024839, loss_ce: 0.009422\n",
            "iteration 10528 : loss : 0.022518, loss_ce: 0.008814\n",
            "iteration 10529 : loss : 0.030245, loss_ce: 0.010498\n",
            "iteration 10530 : loss : 0.025576, loss_ce: 0.008091\n",
            "iteration 10531 : loss : 0.026654, loss_ce: 0.008321\n",
            "iteration 10532 : loss : 0.020258, loss_ce: 0.005349\n",
            "iteration 10533 : loss : 0.022685, loss_ce: 0.005163\n",
            "iteration 10534 : loss : 0.029389, loss_ce: 0.007332\n",
            "iteration 10535 : loss : 0.075520, loss_ce: 0.004701\n",
            "iteration 10536 : loss : 0.025991, loss_ce: 0.006292\n",
            "iteration 10537 : loss : 0.028570, loss_ce: 0.011431\n",
            "iteration 10538 : loss : 0.049866, loss_ce: 0.005397\n",
            "iteration 10539 : loss : 0.026944, loss_ce: 0.008043\n",
            "iteration 10540 : loss : 0.021604, loss_ce: 0.007423\n",
            "iteration 10541 : loss : 0.029696, loss_ce: 0.008234\n",
            "iteration 10542 : loss : 0.025799, loss_ce: 0.005913\n",
            "iteration 10543 : loss : 0.026182, loss_ce: 0.011042\n",
            "iteration 10544 : loss : 0.023024, loss_ce: 0.007513\n",
            "iteration 10545 : loss : 0.022393, loss_ce: 0.008907\n",
            "iteration 10546 : loss : 0.021811, loss_ce: 0.006718\n",
            "iteration 10547 : loss : 0.028299, loss_ce: 0.011612\n",
            "iteration 10548 : loss : 0.025276, loss_ce: 0.009206\n",
            "iteration 10549 : loss : 0.029953, loss_ce: 0.009729\n",
            "iteration 10550 : loss : 0.078221, loss_ce: 0.006167\n",
            "iteration 10551 : loss : 0.026842, loss_ce: 0.008280\n",
            "iteration 10552 : loss : 0.076883, loss_ce: 0.007958\n",
            "iteration 10553 : loss : 0.020677, loss_ce: 0.009059\n",
            "iteration 10554 : loss : 0.021959, loss_ce: 0.006394\n",
            "iteration 10555 : loss : 0.023733, loss_ce: 0.011580\n",
            "iteration 10556 : loss : 0.026100, loss_ce: 0.007251\n",
            "iteration 10557 : loss : 0.029376, loss_ce: 0.010988\n",
            "iteration 10558 : loss : 0.026437, loss_ce: 0.009148\n",
            "iteration 10559 : loss : 0.022347, loss_ce: 0.006240\n",
            "iteration 10560 : loss : 0.025662, loss_ce: 0.009589\n",
            "iteration 10561 : loss : 0.028302, loss_ce: 0.007242\n",
            "iteration 10562 : loss : 0.025411, loss_ce: 0.012591\n",
            "iteration 10563 : loss : 0.021814, loss_ce: 0.006078\n",
            "iteration 10564 : loss : 0.045450, loss_ce: 0.006265\n",
            "iteration 10565 : loss : 0.022775, loss_ce: 0.005356\n",
            "iteration 10566 : loss : 0.028812, loss_ce: 0.009603\n",
            "iteration 10567 : loss : 0.073795, loss_ce: 0.007402\n",
            "iteration 10568 : loss : 0.026983, loss_ce: 0.008932\n",
            "iteration 10569 : loss : 0.023749, loss_ce: 0.006501\n",
            "iteration 10570 : loss : 0.024965, loss_ce: 0.010379\n",
            "iteration 10571 : loss : 0.027207, loss_ce: 0.010598\n",
            "iteration 10572 : loss : 0.020051, loss_ce: 0.005547\n",
            "iteration 10573 : loss : 0.026753, loss_ce: 0.009503\n",
            "iteration 10574 : loss : 0.021129, loss_ce: 0.007384\n",
            "iteration 10575 : loss : 0.026477, loss_ce: 0.009721\n",
            "iteration 10576 : loss : 0.047357, loss_ce: 0.012087\n",
            "iteration 10577 : loss : 0.027356, loss_ce: 0.007427\n",
            "iteration 10578 : loss : 0.024291, loss_ce: 0.008954\n",
            "iteration 10579 : loss : 0.036902, loss_ce: 0.004747\n",
            "iteration 10580 : loss : 0.021110, loss_ce: 0.006845\n",
            "iteration 10581 : loss : 0.078719, loss_ce: 0.007271\n",
            "iteration 10582 : loss : 0.023221, loss_ce: 0.007147\n",
            "iteration 10583 : loss : 0.027957, loss_ce: 0.010740\n",
            "iteration 10584 : loss : 0.024378, loss_ce: 0.008551\n",
            "iteration 10585 : loss : 0.025068, loss_ce: 0.010629\n",
            "iteration 10586 : loss : 0.022572, loss_ce: 0.010157\n",
            "iteration 10587 : loss : 0.033057, loss_ce: 0.009039\n",
            "iteration 10588 : loss : 0.077203, loss_ce: 0.005987\n",
            "iteration 10589 : loss : 0.027591, loss_ce: 0.011861\n",
            "iteration 10590 : loss : 0.022608, loss_ce: 0.009412\n",
            "iteration 10591 : loss : 0.027681, loss_ce: 0.010805\n",
            "iteration 10592 : loss : 0.026563, loss_ce: 0.008491\n",
            "iteration 10593 : loss : 0.039228, loss_ce: 0.009117\n",
            "iteration 10594 : loss : 0.029792, loss_ce: 0.011788\n",
            "iteration 10595 : loss : 0.026831, loss_ce: 0.011496\n",
            "iteration 10596 : loss : 0.027959, loss_ce: 0.009550\n",
            "iteration 10597 : loss : 0.025776, loss_ce: 0.011169\n",
            "iteration 10598 : loss : 0.024944, loss_ce: 0.011304\n",
            "iteration 10599 : loss : 0.026935, loss_ce: 0.007760\n",
            "iteration 10600 : loss : 0.024294, loss_ce: 0.009515\n",
            "iteration 10601 : loss : 0.019655, loss_ce: 0.004180\n",
            "iteration 10602 : loss : 0.193548, loss_ce: 0.032663\n",
            " 76%|███████████████████▊      | 114/150 [4:09:26<1:18:21, 130.59s/it]iteration 10603 : loss : 0.027972, loss_ce: 0.008027\n",
            "iteration 10604 : loss : 0.029675, loss_ce: 0.013030\n",
            "iteration 10605 : loss : 0.026433, loss_ce: 0.010931\n",
            "iteration 10606 : loss : 0.022755, loss_ce: 0.006013\n",
            "iteration 10607 : loss : 0.127829, loss_ce: 0.003706\n",
            "iteration 10608 : loss : 0.027041, loss_ce: 0.011477\n",
            "iteration 10609 : loss : 0.026702, loss_ce: 0.009550\n",
            "iteration 10610 : loss : 0.028603, loss_ce: 0.014620\n",
            "iteration 10611 : loss : 0.029264, loss_ce: 0.005333\n",
            "iteration 10612 : loss : 0.026992, loss_ce: 0.008712\n",
            "iteration 10613 : loss : 0.019657, loss_ce: 0.008625\n",
            "iteration 10614 : loss : 0.022774, loss_ce: 0.008846\n",
            "iteration 10615 : loss : 0.075699, loss_ce: 0.006022\n",
            "iteration 10616 : loss : 0.022642, loss_ce: 0.011107\n",
            "iteration 10617 : loss : 0.023596, loss_ce: 0.009612\n",
            "iteration 10618 : loss : 0.023404, loss_ce: 0.010092\n",
            "iteration 10619 : loss : 0.023864, loss_ce: 0.008746\n",
            "iteration 10620 : loss : 0.025642, loss_ce: 0.010976\n",
            "iteration 10621 : loss : 0.026854, loss_ce: 0.007279\n",
            "iteration 10622 : loss : 0.021707, loss_ce: 0.006452\n",
            "iteration 10623 : loss : 0.022587, loss_ce: 0.007068\n",
            "iteration 10624 : loss : 0.028485, loss_ce: 0.006263\n",
            "iteration 10625 : loss : 0.024595, loss_ce: 0.010142\n",
            "iteration 10626 : loss : 0.021567, loss_ce: 0.010206\n",
            "iteration 10627 : loss : 0.025314, loss_ce: 0.011427\n",
            "iteration 10628 : loss : 0.065334, loss_ce: 0.007922\n",
            "iteration 10629 : loss : 0.021152, loss_ce: 0.009067\n",
            "iteration 10630 : loss : 0.024267, loss_ce: 0.009147\n",
            "iteration 10631 : loss : 0.024199, loss_ce: 0.008087\n",
            "iteration 10632 : loss : 0.024667, loss_ce: 0.012032\n",
            "iteration 10633 : loss : 0.026695, loss_ce: 0.009572\n",
            "iteration 10634 : loss : 0.026665, loss_ce: 0.004686\n",
            "iteration 10635 : loss : 0.027628, loss_ce: 0.009741\n",
            "iteration 10636 : loss : 0.027502, loss_ce: 0.014620\n",
            "iteration 10637 : loss : 0.027068, loss_ce: 0.005330\n",
            "iteration 10638 : loss : 0.027035, loss_ce: 0.011557\n",
            "iteration 10639 : loss : 0.023575, loss_ce: 0.008953\n",
            "iteration 10640 : loss : 0.026430, loss_ce: 0.010599\n",
            "iteration 10641 : loss : 0.026049, loss_ce: 0.010005\n",
            "iteration 10642 : loss : 0.029220, loss_ce: 0.010556\n",
            "iteration 10643 : loss : 0.020958, loss_ce: 0.006235\n",
            "iteration 10644 : loss : 0.023184, loss_ce: 0.006421\n",
            "iteration 10645 : loss : 0.025336, loss_ce: 0.007222\n",
            "iteration 10646 : loss : 0.024733, loss_ce: 0.007766\n",
            "iteration 10647 : loss : 0.022638, loss_ce: 0.009180\n",
            "iteration 10648 : loss : 0.022056, loss_ce: 0.009262\n",
            "iteration 10649 : loss : 0.028713, loss_ce: 0.013648\n",
            "iteration 10650 : loss : 0.024806, loss_ce: 0.008749\n",
            "iteration 10651 : loss : 0.027583, loss_ce: 0.008163\n",
            "iteration 10652 : loss : 0.022013, loss_ce: 0.006857\n",
            "iteration 10653 : loss : 0.125005, loss_ce: 0.006119\n",
            "iteration 10654 : loss : 0.026220, loss_ce: 0.006105\n",
            "iteration 10655 : loss : 0.025440, loss_ce: 0.011194\n",
            "iteration 10656 : loss : 0.069429, loss_ce: 0.003342\n",
            "iteration 10657 : loss : 0.125698, loss_ce: 0.007629\n",
            "iteration 10658 : loss : 0.077849, loss_ce: 0.006770\n",
            "iteration 10659 : loss : 0.026236, loss_ce: 0.008449\n",
            "iteration 10660 : loss : 0.023192, loss_ce: 0.008564\n",
            "iteration 10661 : loss : 0.025459, loss_ce: 0.008450\n",
            "iteration 10662 : loss : 0.023504, loss_ce: 0.008368\n",
            "iteration 10663 : loss : 0.026314, loss_ce: 0.006096\n",
            "iteration 10664 : loss : 0.040114, loss_ce: 0.004885\n",
            "iteration 10665 : loss : 0.027978, loss_ce: 0.007340\n",
            "iteration 10666 : loss : 0.076549, loss_ce: 0.006315\n",
            "iteration 10667 : loss : 0.038649, loss_ce: 0.008863\n",
            "iteration 10668 : loss : 0.024638, loss_ce: 0.010070\n",
            "iteration 10669 : loss : 0.031090, loss_ce: 0.005385\n",
            "iteration 10670 : loss : 0.030815, loss_ce: 0.013196\n",
            "iteration 10671 : loss : 0.029907, loss_ce: 0.009675\n",
            "iteration 10672 : loss : 0.025375, loss_ce: 0.007375\n",
            "iteration 10673 : loss : 0.022408, loss_ce: 0.008819\n",
            "iteration 10674 : loss : 0.035102, loss_ce: 0.005125\n",
            "iteration 10675 : loss : 0.023624, loss_ce: 0.011485\n",
            "iteration 10676 : loss : 0.027099, loss_ce: 0.006952\n",
            "iteration 10677 : loss : 0.028575, loss_ce: 0.008392\n",
            "iteration 10678 : loss : 0.022163, loss_ce: 0.004135\n",
            "iteration 10679 : loss : 0.024855, loss_ce: 0.010101\n",
            "iteration 10680 : loss : 0.028503, loss_ce: 0.007483\n",
            "iteration 10681 : loss : 0.036379, loss_ce: 0.008817\n",
            "iteration 10682 : loss : 0.079051, loss_ce: 0.011885\n",
            "iteration 10683 : loss : 0.022953, loss_ce: 0.004931\n",
            "iteration 10684 : loss : 0.023740, loss_ce: 0.007358\n",
            "iteration 10685 : loss : 0.075085, loss_ce: 0.004600\n",
            "iteration 10686 : loss : 0.021241, loss_ce: 0.010051\n",
            "iteration 10687 : loss : 0.024219, loss_ce: 0.009334\n",
            "iteration 10688 : loss : 0.026780, loss_ce: 0.006893\n",
            "iteration 10689 : loss : 0.021881, loss_ce: 0.004721\n",
            "iteration 10690 : loss : 0.023842, loss_ce: 0.008705\n",
            "iteration 10691 : loss : 0.025115, loss_ce: 0.007238\n",
            "iteration 10692 : loss : 0.075158, loss_ce: 0.008515\n",
            "iteration 10693 : loss : 0.022909, loss_ce: 0.009013\n",
            "iteration 10694 : loss : 0.021477, loss_ce: 0.007573\n",
            "iteration 10695 : loss : 0.025742, loss_ce: 0.011912\n",
            " 77%|███████████████████▉      | 115/150 [4:11:38<1:16:23, 130.95s/it]iteration 10696 : loss : 0.024164, loss_ce: 0.011678\n",
            "iteration 10697 : loss : 0.026646, loss_ce: 0.010248\n",
            "iteration 10698 : loss : 0.023446, loss_ce: 0.012287\n",
            "iteration 10699 : loss : 0.025109, loss_ce: 0.012125\n",
            "iteration 10700 : loss : 0.023283, loss_ce: 0.007120\n",
            "iteration 10701 : loss : 0.030225, loss_ce: 0.011248\n",
            "iteration 10702 : loss : 0.024080, loss_ce: 0.008128\n",
            "iteration 10703 : loss : 0.027447, loss_ce: 0.009475\n",
            "iteration 10704 : loss : 0.023879, loss_ce: 0.006048\n",
            "iteration 10705 : loss : 0.024110, loss_ce: 0.007366\n",
            "iteration 10706 : loss : 0.024589, loss_ce: 0.010453\n",
            "iteration 10707 : loss : 0.085898, loss_ce: 0.006055\n",
            "iteration 10708 : loss : 0.024561, loss_ce: 0.005984\n",
            "iteration 10709 : loss : 0.022843, loss_ce: 0.006608\n",
            "iteration 10710 : loss : 0.028934, loss_ce: 0.010056\n",
            "iteration 10711 : loss : 0.026268, loss_ce: 0.006750\n",
            "iteration 10712 : loss : 0.023235, loss_ce: 0.006899\n",
            "iteration 10713 : loss : 0.025831, loss_ce: 0.008877\n",
            "iteration 10714 : loss : 0.019052, loss_ce: 0.007248\n",
            "iteration 10715 : loss : 0.020031, loss_ce: 0.005281\n",
            "iteration 10716 : loss : 0.022677, loss_ce: 0.006586\n",
            "iteration 10717 : loss : 0.026286, loss_ce: 0.008349\n",
            "iteration 10718 : loss : 0.023409, loss_ce: 0.007172\n",
            "iteration 10719 : loss : 0.025897, loss_ce: 0.009400\n",
            "iteration 10720 : loss : 0.024617, loss_ce: 0.006180\n",
            "iteration 10721 : loss : 0.028701, loss_ce: 0.009728\n",
            "iteration 10722 : loss : 0.044640, loss_ce: 0.010849\n",
            "iteration 10723 : loss : 0.025497, loss_ce: 0.013492\n",
            "iteration 10724 : loss : 0.026104, loss_ce: 0.009659\n",
            "iteration 10725 : loss : 0.026066, loss_ce: 0.007762\n",
            "iteration 10726 : loss : 0.018949, loss_ce: 0.004939\n",
            "iteration 10727 : loss : 0.080329, loss_ce: 0.006898\n",
            "iteration 10728 : loss : 0.023360, loss_ce: 0.007827\n",
            "iteration 10729 : loss : 0.020412, loss_ce: 0.006568\n",
            "iteration 10730 : loss : 0.025463, loss_ce: 0.008014\n",
            "iteration 10731 : loss : 0.024880, loss_ce: 0.009736\n",
            "iteration 10732 : loss : 0.025055, loss_ce: 0.007026\n",
            "iteration 10733 : loss : 0.029149, loss_ce: 0.009043\n",
            "iteration 10734 : loss : 0.026030, loss_ce: 0.010252\n",
            "iteration 10735 : loss : 0.025465, loss_ce: 0.010058\n",
            "iteration 10736 : loss : 0.026962, loss_ce: 0.011596\n",
            "iteration 10737 : loss : 0.022522, loss_ce: 0.008520\n",
            "iteration 10738 : loss : 0.033631, loss_ce: 0.008664\n",
            "iteration 10739 : loss : 0.029409, loss_ce: 0.009716\n",
            "iteration 10740 : loss : 0.021396, loss_ce: 0.007085\n",
            "iteration 10741 : loss : 0.022715, loss_ce: 0.004208\n",
            "iteration 10742 : loss : 0.021110, loss_ce: 0.007577\n",
            "iteration 10743 : loss : 0.023118, loss_ce: 0.007853\n",
            "iteration 10744 : loss : 0.023008, loss_ce: 0.005780\n",
            "iteration 10745 : loss : 0.033665, loss_ce: 0.009648\n",
            "iteration 10746 : loss : 0.027396, loss_ce: 0.007820\n",
            "iteration 10747 : loss : 0.029395, loss_ce: 0.009242\n",
            "iteration 10748 : loss : 0.024696, loss_ce: 0.007642\n",
            "iteration 10749 : loss : 0.020597, loss_ce: 0.006211\n",
            "iteration 10750 : loss : 0.075882, loss_ce: 0.007103\n",
            "iteration 10751 : loss : 0.028236, loss_ce: 0.007791\n",
            "iteration 10752 : loss : 0.024619, loss_ce: 0.005707\n",
            "iteration 10753 : loss : 0.074243, loss_ce: 0.005426\n",
            "iteration 10754 : loss : 0.021589, loss_ce: 0.008386\n",
            "iteration 10755 : loss : 0.023139, loss_ce: 0.010105\n",
            "iteration 10756 : loss : 0.024576, loss_ce: 0.011918\n",
            "iteration 10757 : loss : 0.025382, loss_ce: 0.008902\n",
            "iteration 10758 : loss : 0.027371, loss_ce: 0.011118\n",
            "iteration 10759 : loss : 0.023553, loss_ce: 0.007790\n",
            "iteration 10760 : loss : 0.031339, loss_ce: 0.010494\n",
            "iteration 10761 : loss : 0.077329, loss_ce: 0.008973\n",
            "iteration 10762 : loss : 0.028123, loss_ce: 0.009663\n",
            "iteration 10763 : loss : 0.020776, loss_ce: 0.005838\n",
            "iteration 10764 : loss : 0.023641, loss_ce: 0.010987\n",
            "iteration 10765 : loss : 0.024442, loss_ce: 0.005506\n",
            "iteration 10766 : loss : 0.074360, loss_ce: 0.006672\n",
            "iteration 10767 : loss : 0.027203, loss_ce: 0.006741\n",
            "iteration 10768 : loss : 0.029964, loss_ce: 0.008530\n",
            "iteration 10769 : loss : 0.026323, loss_ce: 0.008814\n",
            "iteration 10770 : loss : 0.022446, loss_ce: 0.008129\n",
            "iteration 10771 : loss : 0.023587, loss_ce: 0.008461\n",
            "iteration 10772 : loss : 0.028101, loss_ce: 0.012844\n",
            "iteration 10773 : loss : 0.027978, loss_ce: 0.010831\n",
            "iteration 10774 : loss : 0.024150, loss_ce: 0.007952\n",
            "iteration 10775 : loss : 0.024503, loss_ce: 0.006416\n",
            "iteration 10776 : loss : 0.021713, loss_ce: 0.007937\n",
            "iteration 10777 : loss : 0.026014, loss_ce: 0.007591\n",
            "iteration 10778 : loss : 0.028058, loss_ce: 0.004938\n",
            "iteration 10779 : loss : 0.026278, loss_ce: 0.005367\n",
            "iteration 10780 : loss : 0.022074, loss_ce: 0.009161\n",
            "iteration 10781 : loss : 0.025347, loss_ce: 0.007640\n",
            "iteration 10782 : loss : 0.025351, loss_ce: 0.011259\n",
            "iteration 10783 : loss : 0.026569, loss_ce: 0.010661\n",
            "iteration 10784 : loss : 0.024392, loss_ce: 0.011108\n",
            "iteration 10785 : loss : 0.026540, loss_ce: 0.005527\n",
            "iteration 10786 : loss : 0.025136, loss_ce: 0.007785\n",
            "iteration 10787 : loss : 0.021834, loss_ce: 0.008980\n",
            "iteration 10788 : loss : 0.060456, loss_ce: 0.032003\n",
            " 77%|████████████████████      | 116/150 [4:13:48<1:14:03, 130.70s/it]iteration 10789 : loss : 0.025730, loss_ce: 0.009696\n",
            "iteration 10790 : loss : 0.077254, loss_ce: 0.007248\n",
            "iteration 10791 : loss : 0.021065, loss_ce: 0.006083\n",
            "iteration 10792 : loss : 0.023371, loss_ce: 0.008830\n",
            "iteration 10793 : loss : 0.022715, loss_ce: 0.006959\n",
            "iteration 10794 : loss : 0.023521, loss_ce: 0.008102\n",
            "iteration 10795 : loss : 0.020604, loss_ce: 0.005424\n",
            "iteration 10796 : loss : 0.026383, loss_ce: 0.008947\n",
            "iteration 10797 : loss : 0.023261, loss_ce: 0.008796\n",
            "iteration 10798 : loss : 0.022976, loss_ce: 0.008425\n",
            "iteration 10799 : loss : 0.072172, loss_ce: 0.004327\n",
            "iteration 10800 : loss : 0.028954, loss_ce: 0.007654\n",
            "iteration 10801 : loss : 0.029920, loss_ce: 0.011034\n",
            "iteration 10802 : loss : 0.029619, loss_ce: 0.005380\n",
            "iteration 10803 : loss : 0.024613, loss_ce: 0.007854\n",
            "iteration 10804 : loss : 0.026858, loss_ce: 0.008081\n",
            "iteration 10805 : loss : 0.020457, loss_ce: 0.008035\n",
            "iteration 10806 : loss : 0.027084, loss_ce: 0.009059\n",
            "iteration 10807 : loss : 0.022284, loss_ce: 0.008890\n",
            "iteration 10808 : loss : 0.023027, loss_ce: 0.005138\n",
            "iteration 10809 : loss : 0.025302, loss_ce: 0.009319\n",
            "iteration 10810 : loss : 0.022932, loss_ce: 0.008677\n",
            "iteration 10811 : loss : 0.033460, loss_ce: 0.008820\n",
            "iteration 10812 : loss : 0.025456, loss_ce: 0.006112\n",
            "iteration 10813 : loss : 0.025102, loss_ce: 0.010116\n",
            "iteration 10814 : loss : 0.023745, loss_ce: 0.006516\n",
            "iteration 10815 : loss : 0.026681, loss_ce: 0.010485\n",
            "iteration 10816 : loss : 0.030285, loss_ce: 0.004939\n",
            "iteration 10817 : loss : 0.075644, loss_ce: 0.007329\n",
            "iteration 10818 : loss : 0.027522, loss_ce: 0.006837\n",
            "iteration 10819 : loss : 0.031571, loss_ce: 0.008102\n",
            "iteration 10820 : loss : 0.027005, loss_ce: 0.009057\n",
            "iteration 10821 : loss : 0.028769, loss_ce: 0.010508\n",
            "iteration 10822 : loss : 0.020403, loss_ce: 0.003664\n",
            "iteration 10823 : loss : 0.022209, loss_ce: 0.007538\n",
            "iteration 10824 : loss : 0.023357, loss_ce: 0.008287\n",
            "iteration 10825 : loss : 0.026876, loss_ce: 0.006949\n",
            "iteration 10826 : loss : 0.024368, loss_ce: 0.008720\n",
            "iteration 10827 : loss : 0.025988, loss_ce: 0.008094\n",
            "iteration 10828 : loss : 0.025649, loss_ce: 0.011397\n",
            "iteration 10829 : loss : 0.024155, loss_ce: 0.008508\n",
            "iteration 10830 : loss : 0.025046, loss_ce: 0.007795\n",
            "iteration 10831 : loss : 0.019825, loss_ce: 0.007530\n",
            "iteration 10832 : loss : 0.022203, loss_ce: 0.010884\n",
            "iteration 10833 : loss : 0.026783, loss_ce: 0.011188\n",
            "iteration 10834 : loss : 0.023052, loss_ce: 0.010506\n",
            "iteration 10835 : loss : 0.020309, loss_ce: 0.005311\n",
            "iteration 10836 : loss : 0.024585, loss_ce: 0.005408\n",
            "iteration 10837 : loss : 0.076242, loss_ce: 0.008771\n",
            "iteration 10838 : loss : 0.028399, loss_ce: 0.010000\n",
            "iteration 10839 : loss : 0.027161, loss_ce: 0.009052\n",
            "iteration 10840 : loss : 0.023321, loss_ce: 0.009438\n",
            "iteration 10841 : loss : 0.024859, loss_ce: 0.010622\n",
            "iteration 10842 : loss : 0.023553, loss_ce: 0.009250\n",
            "iteration 10843 : loss : 0.027169, loss_ce: 0.009036\n",
            "iteration 10844 : loss : 0.028665, loss_ce: 0.013340\n",
            "iteration 10845 : loss : 0.027583, loss_ce: 0.005433\n",
            "iteration 10846 : loss : 0.022907, loss_ce: 0.009969\n",
            "iteration 10847 : loss : 0.023976, loss_ce: 0.008932\n",
            "iteration 10848 : loss : 0.023337, loss_ce: 0.009678\n",
            "iteration 10849 : loss : 0.036874, loss_ce: 0.006195\n",
            "iteration 10850 : loss : 0.023716, loss_ce: 0.007113\n",
            "iteration 10851 : loss : 0.025266, loss_ce: 0.008192\n",
            "iteration 10852 : loss : 0.043520, loss_ce: 0.008848\n",
            "iteration 10853 : loss : 0.023579, loss_ce: 0.009686\n",
            "iteration 10854 : loss : 0.020363, loss_ce: 0.004882\n",
            "iteration 10855 : loss : 0.026456, loss_ce: 0.011642\n",
            "iteration 10856 : loss : 0.024637, loss_ce: 0.011957\n",
            "iteration 10857 : loss : 0.019655, loss_ce: 0.006363\n",
            "iteration 10858 : loss : 0.026807, loss_ce: 0.004672\n",
            "iteration 10859 : loss : 0.026297, loss_ce: 0.013118\n",
            "iteration 10860 : loss : 0.023648, loss_ce: 0.009461\n",
            "iteration 10861 : loss : 0.029617, loss_ce: 0.007991\n",
            "iteration 10862 : loss : 0.022864, loss_ce: 0.009030\n",
            "iteration 10863 : loss : 0.022237, loss_ce: 0.004948\n",
            "iteration 10864 : loss : 0.027114, loss_ce: 0.009630\n",
            "iteration 10865 : loss : 0.031003, loss_ce: 0.010133\n",
            "iteration 10866 : loss : 0.027710, loss_ce: 0.010114\n",
            "iteration 10867 : loss : 0.030752, loss_ce: 0.012113\n",
            "iteration 10868 : loss : 0.021658, loss_ce: 0.009666\n",
            "iteration 10869 : loss : 0.024696, loss_ce: 0.008117\n",
            "iteration 10870 : loss : 0.023744, loss_ce: 0.008619\n",
            "iteration 10871 : loss : 0.078497, loss_ce: 0.007870\n",
            "iteration 10872 : loss : 0.029810, loss_ce: 0.008308\n",
            "iteration 10873 : loss : 0.027643, loss_ce: 0.008775\n",
            "iteration 10874 : loss : 0.029682, loss_ce: 0.010949\n",
            "iteration 10875 : loss : 0.025638, loss_ce: 0.008809\n",
            "iteration 10876 : loss : 0.025231, loss_ce: 0.006458\n",
            "iteration 10877 : loss : 0.026522, loss_ce: 0.011624\n",
            "iteration 10878 : loss : 0.073776, loss_ce: 0.007168\n",
            "iteration 10879 : loss : 0.020892, loss_ce: 0.007487\n",
            "iteration 10880 : loss : 0.020380, loss_ce: 0.006229\n",
            "iteration 10881 : loss : 0.342070, loss_ce: 0.003004\n",
            " 78%|████████████████████▎     | 117/150 [4:15:58<1:11:51, 130.64s/it]iteration 10882 : loss : 0.029406, loss_ce: 0.011756\n",
            "iteration 10883 : loss : 0.028833, loss_ce: 0.009060\n",
            "iteration 10884 : loss : 0.019728, loss_ce: 0.007044\n",
            "iteration 10885 : loss : 0.075607, loss_ce: 0.007259\n",
            "iteration 10886 : loss : 0.022097, loss_ce: 0.012215\n",
            "iteration 10887 : loss : 0.020483, loss_ce: 0.005429\n",
            "iteration 10888 : loss : 0.075750, loss_ce: 0.005894\n",
            "iteration 10889 : loss : 0.026823, loss_ce: 0.006321\n",
            "iteration 10890 : loss : 0.021450, loss_ce: 0.003606\n",
            "iteration 10891 : loss : 0.027947, loss_ce: 0.011779\n",
            "iteration 10892 : loss : 0.075551, loss_ce: 0.007825\n",
            "iteration 10893 : loss : 0.036802, loss_ce: 0.007248\n",
            "iteration 10894 : loss : 0.023357, loss_ce: 0.008041\n",
            "iteration 10895 : loss : 0.023783, loss_ce: 0.010950\n",
            "iteration 10896 : loss : 0.033954, loss_ce: 0.010586\n",
            "iteration 10897 : loss : 0.080535, loss_ce: 0.005225\n",
            "iteration 10898 : loss : 0.023397, loss_ce: 0.008324\n",
            "iteration 10899 : loss : 0.025032, loss_ce: 0.006624\n",
            "iteration 10900 : loss : 0.022770, loss_ce: 0.006383\n",
            "iteration 10901 : loss : 0.026016, loss_ce: 0.008716\n",
            "iteration 10902 : loss : 0.023311, loss_ce: 0.007608\n",
            "iteration 10903 : loss : 0.025703, loss_ce: 0.007758\n",
            "iteration 10904 : loss : 0.026804, loss_ce: 0.009276\n",
            "iteration 10905 : loss : 0.022869, loss_ce: 0.006463\n",
            "iteration 10906 : loss : 0.021292, loss_ce: 0.007873\n",
            "iteration 10907 : loss : 0.024942, loss_ce: 0.009311\n",
            "iteration 10908 : loss : 0.025469, loss_ce: 0.006477\n",
            "iteration 10909 : loss : 0.025797, loss_ce: 0.011617\n",
            "iteration 10910 : loss : 0.073406, loss_ce: 0.006607\n",
            "iteration 10911 : loss : 0.028836, loss_ce: 0.011710\n",
            "iteration 10912 : loss : 0.024465, loss_ce: 0.007372\n",
            "iteration 10913 : loss : 0.029711, loss_ce: 0.009248\n",
            "iteration 10914 : loss : 0.025089, loss_ce: 0.010413\n",
            "iteration 10915 : loss : 0.023790, loss_ce: 0.007528\n",
            "iteration 10916 : loss : 0.024651, loss_ce: 0.007952\n",
            "iteration 10917 : loss : 0.026086, loss_ce: 0.013503\n",
            "iteration 10918 : loss : 0.027404, loss_ce: 0.008558\n",
            "iteration 10919 : loss : 0.023343, loss_ce: 0.010385\n",
            "iteration 10920 : loss : 0.024995, loss_ce: 0.006330\n",
            "iteration 10921 : loss : 0.031363, loss_ce: 0.008255\n",
            "iteration 10922 : loss : 0.027970, loss_ce: 0.007081\n",
            "iteration 10923 : loss : 0.024328, loss_ce: 0.006865\n",
            "iteration 10924 : loss : 0.030881, loss_ce: 0.005657\n",
            "iteration 10925 : loss : 0.027313, loss_ce: 0.006514\n",
            "iteration 10926 : loss : 0.021213, loss_ce: 0.007835\n",
            "iteration 10927 : loss : 0.027658, loss_ce: 0.009976\n",
            "iteration 10928 : loss : 0.022035, loss_ce: 0.006935\n",
            "iteration 10929 : loss : 0.026403, loss_ce: 0.010725\n",
            "iteration 10930 : loss : 0.025027, loss_ce: 0.010180\n",
            "iteration 10931 : loss : 0.024113, loss_ce: 0.011679\n",
            "iteration 10932 : loss : 0.024915, loss_ce: 0.009235\n",
            "iteration 10933 : loss : 0.020048, loss_ce: 0.004568\n",
            "iteration 10934 : loss : 0.035076, loss_ce: 0.007694\n",
            "iteration 10935 : loss : 0.028225, loss_ce: 0.013436\n",
            "iteration 10936 : loss : 0.026586, loss_ce: 0.008332\n",
            "iteration 10937 : loss : 0.022825, loss_ce: 0.006552\n",
            "iteration 10938 : loss : 0.020422, loss_ce: 0.008710\n",
            "iteration 10939 : loss : 0.023066, loss_ce: 0.009018\n",
            "iteration 10940 : loss : 0.028530, loss_ce: 0.010462\n",
            "iteration 10941 : loss : 0.025828, loss_ce: 0.007904\n",
            "iteration 10942 : loss : 0.021617, loss_ce: 0.007642\n",
            "iteration 10943 : loss : 0.044780, loss_ce: 0.008755\n",
            "iteration 10944 : loss : 0.030746, loss_ce: 0.009809\n",
            "iteration 10945 : loss : 0.022513, loss_ce: 0.006085\n",
            "iteration 10946 : loss : 0.025864, loss_ce: 0.009375\n",
            "iteration 10947 : loss : 0.024099, loss_ce: 0.010366\n",
            "iteration 10948 : loss : 0.022670, loss_ce: 0.008777\n",
            "iteration 10949 : loss : 0.025392, loss_ce: 0.005672\n",
            "iteration 10950 : loss : 0.026202, loss_ce: 0.008812\n",
            "iteration 10951 : loss : 0.027217, loss_ce: 0.013856\n",
            "iteration 10952 : loss : 0.026924, loss_ce: 0.004042\n",
            "iteration 10953 : loss : 0.024766, loss_ce: 0.007301\n",
            "iteration 10954 : loss : 0.024641, loss_ce: 0.010042\n",
            "iteration 10955 : loss : 0.031700, loss_ce: 0.008475\n",
            "iteration 10956 : loss : 0.025871, loss_ce: 0.010595\n",
            "iteration 10957 : loss : 0.023530, loss_ce: 0.006140\n",
            "iteration 10958 : loss : 0.025839, loss_ce: 0.010726\n",
            "iteration 10959 : loss : 0.073543, loss_ce: 0.006302\n",
            "iteration 10960 : loss : 0.028211, loss_ce: 0.006188\n",
            "iteration 10961 : loss : 0.025511, loss_ce: 0.011845\n",
            "iteration 10962 : loss : 0.026030, loss_ce: 0.008503\n",
            "iteration 10963 : loss : 0.026249, loss_ce: 0.007704\n",
            "iteration 10964 : loss : 0.021926, loss_ce: 0.008031\n",
            "iteration 10965 : loss : 0.026125, loss_ce: 0.011276\n",
            "iteration 10966 : loss : 0.030253, loss_ce: 0.007673\n",
            "iteration 10967 : loss : 0.024736, loss_ce: 0.011291\n",
            "iteration 10968 : loss : 0.028596, loss_ce: 0.010201\n",
            "iteration 10969 : loss : 0.032419, loss_ce: 0.006278\n",
            "iteration 10970 : loss : 0.025811, loss_ce: 0.009686\n",
            "iteration 10971 : loss : 0.025272, loss_ce: 0.008601\n",
            "iteration 10972 : loss : 0.028056, loss_ce: 0.008646\n",
            "iteration 10973 : loss : 0.025234, loss_ce: 0.006134\n",
            "iteration 10974 : loss : 0.391483, loss_ce: 0.002586\n",
            " 79%|████████████████████▍     | 118/150 [4:18:10<1:09:53, 131.04s/it]iteration 10975 : loss : 0.023199, loss_ce: 0.005976\n",
            "iteration 10976 : loss : 0.027064, loss_ce: 0.006655\n",
            "iteration 10977 : loss : 0.021156, loss_ce: 0.004665\n",
            "iteration 10978 : loss : 0.023633, loss_ce: 0.011394\n",
            "iteration 10979 : loss : 0.073878, loss_ce: 0.007833\n",
            "iteration 10980 : loss : 0.022614, loss_ce: 0.006208\n",
            "iteration 10981 : loss : 0.074145, loss_ce: 0.004925\n",
            "iteration 10982 : loss : 0.021750, loss_ce: 0.007949\n",
            "iteration 10983 : loss : 0.030068, loss_ce: 0.005590\n",
            "iteration 10984 : loss : 0.023957, loss_ce: 0.008207\n",
            "iteration 10985 : loss : 0.024014, loss_ce: 0.009008\n",
            "iteration 10986 : loss : 0.024587, loss_ce: 0.010136\n",
            "iteration 10987 : loss : 0.025054, loss_ce: 0.007904\n",
            "iteration 10988 : loss : 0.078334, loss_ce: 0.007182\n",
            "iteration 10989 : loss : 0.043763, loss_ce: 0.011883\n",
            "iteration 10990 : loss : 0.077272, loss_ce: 0.008720\n",
            "iteration 10991 : loss : 0.022679, loss_ce: 0.007018\n",
            "iteration 10992 : loss : 0.028959, loss_ce: 0.009436\n",
            "iteration 10993 : loss : 0.024946, loss_ce: 0.009368\n",
            "iteration 10994 : loss : 0.021283, loss_ce: 0.005025\n",
            "iteration 10995 : loss : 0.025286, loss_ce: 0.009592\n",
            "iteration 10996 : loss : 0.022404, loss_ce: 0.007810\n",
            "iteration 10997 : loss : 0.026779, loss_ce: 0.007231\n",
            "iteration 10998 : loss : 0.029711, loss_ce: 0.008941\n",
            "iteration 10999 : loss : 0.025608, loss_ce: 0.009961\n",
            "iteration 11000 : loss : 0.022558, loss_ce: 0.009795\n",
            "iteration 11001 : loss : 0.028155, loss_ce: 0.007991\n",
            "iteration 11002 : loss : 0.024482, loss_ce: 0.008576\n",
            "iteration 11003 : loss : 0.029618, loss_ce: 0.012236\n",
            "iteration 11004 : loss : 0.029421, loss_ce: 0.008395\n",
            "iteration 11005 : loss : 0.026155, loss_ce: 0.006779\n",
            "iteration 11006 : loss : 0.025465, loss_ce: 0.013160\n",
            "iteration 11007 : loss : 0.023966, loss_ce: 0.009806\n",
            "iteration 11008 : loss : 0.023795, loss_ce: 0.008877\n",
            "iteration 11009 : loss : 0.024856, loss_ce: 0.012879\n",
            "iteration 11010 : loss : 0.032562, loss_ce: 0.010711\n",
            "iteration 11011 : loss : 0.025818, loss_ce: 0.008050\n",
            "iteration 11012 : loss : 0.025842, loss_ce: 0.011227\n",
            "iteration 11013 : loss : 0.041812, loss_ce: 0.009920\n",
            "iteration 11014 : loss : 0.025833, loss_ce: 0.007768\n",
            "iteration 11015 : loss : 0.025806, loss_ce: 0.010254\n",
            "iteration 11016 : loss : 0.025497, loss_ce: 0.011036\n",
            "iteration 11017 : loss : 0.027683, loss_ce: 0.010185\n",
            "iteration 11018 : loss : 0.026162, loss_ce: 0.009548\n",
            "iteration 11019 : loss : 0.021303, loss_ce: 0.007420\n",
            "iteration 11020 : loss : 0.027794, loss_ce: 0.012155\n",
            "iteration 11021 : loss : 0.033120, loss_ce: 0.008837\n",
            "iteration 11022 : loss : 0.029593, loss_ce: 0.010771\n",
            "iteration 11023 : loss : 0.080313, loss_ce: 0.004711\n",
            "iteration 11024 : loss : 0.027160, loss_ce: 0.008820\n",
            "iteration 11025 : loss : 0.023713, loss_ce: 0.008339\n",
            "iteration 11026 : loss : 0.019220, loss_ce: 0.006805\n",
            "iteration 11027 : loss : 0.022131, loss_ce: 0.007386\n",
            "iteration 11028 : loss : 0.026353, loss_ce: 0.006871\n",
            "iteration 11029 : loss : 0.022441, loss_ce: 0.003708\n",
            "iteration 11030 : loss : 0.027850, loss_ce: 0.009368\n",
            "iteration 11031 : loss : 0.021917, loss_ce: 0.005927\n",
            "iteration 11032 : loss : 0.026839, loss_ce: 0.010000\n",
            "iteration 11033 : loss : 0.025663, loss_ce: 0.011042\n",
            "iteration 11034 : loss : 0.031291, loss_ce: 0.005755\n",
            "iteration 11035 : loss : 0.023719, loss_ce: 0.006833\n",
            "iteration 11036 : loss : 0.022235, loss_ce: 0.005358\n",
            "iteration 11037 : loss : 0.074152, loss_ce: 0.006463\n",
            "iteration 11038 : loss : 0.028728, loss_ce: 0.011143\n",
            "iteration 11039 : loss : 0.022863, loss_ce: 0.008963\n",
            "iteration 11040 : loss : 0.024349, loss_ce: 0.009125\n",
            "iteration 11041 : loss : 0.023124, loss_ce: 0.007264\n",
            "iteration 11042 : loss : 0.026517, loss_ce: 0.011147\n",
            "iteration 11043 : loss : 0.022348, loss_ce: 0.007265\n",
            "iteration 11044 : loss : 0.023647, loss_ce: 0.010553\n",
            "iteration 11045 : loss : 0.028142, loss_ce: 0.007507\n",
            "iteration 11046 : loss : 0.022674, loss_ce: 0.008835\n",
            "iteration 11047 : loss : 0.034547, loss_ce: 0.008203\n",
            "iteration 11048 : loss : 0.022903, loss_ce: 0.009857\n",
            "iteration 11049 : loss : 0.024553, loss_ce: 0.006278\n",
            "iteration 11050 : loss : 0.021960, loss_ce: 0.010759\n",
            "iteration 11051 : loss : 0.023251, loss_ce: 0.005910\n",
            "iteration 11052 : loss : 0.074187, loss_ce: 0.006007\n",
            "iteration 11053 : loss : 0.022591, loss_ce: 0.006364\n",
            "iteration 11054 : loss : 0.077520, loss_ce: 0.006140\n",
            "iteration 11055 : loss : 0.025675, loss_ce: 0.007527\n",
            "iteration 11056 : loss : 0.023829, loss_ce: 0.008418\n",
            "iteration 11057 : loss : 0.024486, loss_ce: 0.013266\n",
            "iteration 11058 : loss : 0.051452, loss_ce: 0.006790\n",
            "iteration 11059 : loss : 0.025885, loss_ce: 0.007884\n",
            "iteration 11060 : loss : 0.023968, loss_ce: 0.009099\n",
            "iteration 11061 : loss : 0.023201, loss_ce: 0.007993\n",
            "iteration 11062 : loss : 0.021109, loss_ce: 0.006248\n",
            "iteration 11063 : loss : 0.026387, loss_ce: 0.007126\n",
            "iteration 11064 : loss : 0.023709, loss_ce: 0.008257\n",
            "iteration 11065 : loss : 0.024707, loss_ce: 0.008912\n",
            "iteration 11066 : loss : 0.022516, loss_ce: 0.007177\n",
            "iteration 11067 : loss : 0.079346, loss_ce: 0.008048\n",
            " 79%|████████████████████▋     | 119/150 [4:20:21<1:07:36, 130.85s/it]iteration 11068 : loss : 0.024254, loss_ce: 0.009059\n",
            "iteration 11069 : loss : 0.023776, loss_ce: 0.008315\n",
            "iteration 11070 : loss : 0.024923, loss_ce: 0.005875\n",
            "iteration 11071 : loss : 0.025329, loss_ce: 0.005739\n",
            "iteration 11072 : loss : 0.023642, loss_ce: 0.007783\n",
            "iteration 11073 : loss : 0.027568, loss_ce: 0.009527\n",
            "iteration 11074 : loss : 0.079628, loss_ce: 0.009990\n",
            "iteration 11075 : loss : 0.075437, loss_ce: 0.009785\n",
            "iteration 11076 : loss : 0.016437, loss_ce: 0.003882\n",
            "iteration 11077 : loss : 0.024497, loss_ce: 0.006728\n",
            "iteration 11078 : loss : 0.022922, loss_ce: 0.009771\n",
            "iteration 11079 : loss : 0.025634, loss_ce: 0.008576\n",
            "iteration 11080 : loss : 0.030942, loss_ce: 0.005776\n",
            "iteration 11081 : loss : 0.023473, loss_ce: 0.008324\n",
            "iteration 11082 : loss : 0.026816, loss_ce: 0.012025\n",
            "iteration 11083 : loss : 0.024119, loss_ce: 0.010578\n",
            "iteration 11084 : loss : 0.021496, loss_ce: 0.010066\n",
            "iteration 11085 : loss : 0.023449, loss_ce: 0.007927\n",
            "iteration 11086 : loss : 0.027544, loss_ce: 0.012548\n",
            "iteration 11087 : loss : 0.023342, loss_ce: 0.007642\n",
            "iteration 11088 : loss : 0.022217, loss_ce: 0.009585\n",
            "iteration 11089 : loss : 0.029916, loss_ce: 0.008163\n",
            "iteration 11090 : loss : 0.021231, loss_ce: 0.007567\n",
            "iteration 11091 : loss : 0.074376, loss_ce: 0.004650\n",
            "iteration 11092 : loss : 0.031895, loss_ce: 0.006599\n",
            "iteration 11093 : loss : 0.023732, loss_ce: 0.008038\n",
            "iteration 11094 : loss : 0.024642, loss_ce: 0.008265\n",
            "iteration 11095 : loss : 0.026871, loss_ce: 0.009018\n",
            "iteration 11096 : loss : 0.025799, loss_ce: 0.011334\n",
            "iteration 11097 : loss : 0.025256, loss_ce: 0.007525\n",
            "iteration 11098 : loss : 0.027246, loss_ce: 0.008731\n",
            "iteration 11099 : loss : 0.027151, loss_ce: 0.009534\n",
            "iteration 11100 : loss : 0.023315, loss_ce: 0.006521\n",
            "iteration 11101 : loss : 0.021394, loss_ce: 0.006364\n",
            "iteration 11102 : loss : 0.026518, loss_ce: 0.006746\n",
            "iteration 11103 : loss : 0.029049, loss_ce: 0.007739\n",
            "iteration 11104 : loss : 0.024565, loss_ce: 0.010871\n",
            "iteration 11105 : loss : 0.030249, loss_ce: 0.006561\n",
            "iteration 11106 : loss : 0.024610, loss_ce: 0.005739\n",
            "iteration 11107 : loss : 0.026598, loss_ce: 0.008450\n",
            "iteration 11108 : loss : 0.074283, loss_ce: 0.003762\n",
            "iteration 11109 : loss : 0.025202, loss_ce: 0.011499\n",
            "iteration 11110 : loss : 0.023213, loss_ce: 0.007324\n",
            "iteration 11111 : loss : 0.023987, loss_ce: 0.005560\n",
            "iteration 11112 : loss : 0.091305, loss_ce: 0.005223\n",
            "iteration 11113 : loss : 0.025301, loss_ce: 0.013689\n",
            "iteration 11114 : loss : 0.022149, loss_ce: 0.005437\n",
            "iteration 11115 : loss : 0.026341, loss_ce: 0.007991\n",
            "iteration 11116 : loss : 0.022103, loss_ce: 0.006396\n",
            "iteration 11117 : loss : 0.026741, loss_ce: 0.010492\n",
            "iteration 11118 : loss : 0.022810, loss_ce: 0.007237\n",
            "iteration 11119 : loss : 0.027095, loss_ce: 0.011369\n",
            "iteration 11120 : loss : 0.018709, loss_ce: 0.005992\n",
            "iteration 11121 : loss : 0.031194, loss_ce: 0.013382\n",
            "iteration 11122 : loss : 0.026092, loss_ce: 0.011097\n",
            "iteration 11123 : loss : 0.022659, loss_ce: 0.008761\n",
            "iteration 11124 : loss : 0.026132, loss_ce: 0.011850\n",
            "iteration 11125 : loss : 0.027139, loss_ce: 0.010149\n",
            "iteration 11126 : loss : 0.027410, loss_ce: 0.008477\n",
            "iteration 11127 : loss : 0.025862, loss_ce: 0.009400\n",
            "iteration 11128 : loss : 0.029812, loss_ce: 0.007455\n",
            "iteration 11129 : loss : 0.019979, loss_ce: 0.005230\n",
            "iteration 11130 : loss : 0.022830, loss_ce: 0.007431\n",
            "iteration 11131 : loss : 0.024283, loss_ce: 0.012167\n",
            "iteration 11132 : loss : 0.027198, loss_ce: 0.010040\n",
            "iteration 11133 : loss : 0.024932, loss_ce: 0.010714\n",
            "iteration 11134 : loss : 0.029733, loss_ce: 0.008710\n",
            "iteration 11135 : loss : 0.025389, loss_ce: 0.007064\n",
            "iteration 11136 : loss : 0.020814, loss_ce: 0.007569\n",
            "iteration 11137 : loss : 0.076680, loss_ce: 0.006606\n",
            "iteration 11138 : loss : 0.024213, loss_ce: 0.011447\n",
            "iteration 11139 : loss : 0.028969, loss_ce: 0.006440\n",
            "iteration 11140 : loss : 0.040853, loss_ce: 0.009124\n",
            "iteration 11141 : loss : 0.021825, loss_ce: 0.007160\n",
            "iteration 11142 : loss : 0.024084, loss_ce: 0.006047\n",
            "iteration 11143 : loss : 0.025997, loss_ce: 0.007312\n",
            "iteration 11144 : loss : 0.022817, loss_ce: 0.008347\n",
            "iteration 11145 : loss : 0.020358, loss_ce: 0.008226\n",
            "iteration 11146 : loss : 0.029140, loss_ce: 0.009080\n",
            "iteration 11147 : loss : 0.032885, loss_ce: 0.004296\n",
            "iteration 11148 : loss : 0.027958, loss_ce: 0.008161\n",
            "iteration 11149 : loss : 0.023132, loss_ce: 0.009376\n",
            "iteration 11150 : loss : 0.076691, loss_ce: 0.005615\n",
            "iteration 11151 : loss : 0.027257, loss_ce: 0.008985\n",
            "iteration 11152 : loss : 0.024984, loss_ce: 0.008957\n",
            "iteration 11153 : loss : 0.026604, loss_ce: 0.010534\n",
            "iteration 11154 : loss : 0.028971, loss_ce: 0.010116\n",
            "iteration 11155 : loss : 0.027202, loss_ce: 0.011938\n",
            "iteration 11156 : loss : 0.023461, loss_ce: 0.007539\n",
            "iteration 11157 : loss : 0.027863, loss_ce: 0.009339\n",
            "iteration 11158 : loss : 0.024077, loss_ce: 0.007906\n",
            "iteration 11159 : loss : 0.025960, loss_ce: 0.009407\n",
            "iteration 11160 : loss : 0.284795, loss_ce: 0.004915\n",
            " 80%|████████████████████▊     | 120/150 [4:22:31<1:05:22, 130.77s/it]iteration 11161 : loss : 0.019999, loss_ce: 0.004933\n",
            "iteration 11162 : loss : 0.023763, loss_ce: 0.009465\n",
            "iteration 11163 : loss : 0.024156, loss_ce: 0.008829\n",
            "iteration 11164 : loss : 0.027772, loss_ce: 0.010750\n",
            "iteration 11165 : loss : 0.022173, loss_ce: 0.006467\n",
            "iteration 11166 : loss : 0.027767, loss_ce: 0.007725\n",
            "iteration 11167 : loss : 0.030160, loss_ce: 0.008936\n",
            "iteration 11168 : loss : 0.020302, loss_ce: 0.007760\n",
            "iteration 11169 : loss : 0.024323, loss_ce: 0.009970\n",
            "iteration 11170 : loss : 0.023398, loss_ce: 0.009603\n",
            "iteration 11171 : loss : 0.075550, loss_ce: 0.004333\n",
            "iteration 11172 : loss : 0.022439, loss_ce: 0.007300\n",
            "iteration 11173 : loss : 0.027453, loss_ce: 0.009755\n",
            "iteration 11174 : loss : 0.026835, loss_ce: 0.012470\n",
            "iteration 11175 : loss : 0.021129, loss_ce: 0.006916\n",
            "iteration 11176 : loss : 0.022246, loss_ce: 0.009097\n",
            "iteration 11177 : loss : 0.074022, loss_ce: 0.009153\n",
            "iteration 11178 : loss : 0.024957, loss_ce: 0.005183\n",
            "iteration 11179 : loss : 0.017612, loss_ce: 0.006051\n",
            "iteration 11180 : loss : 0.023874, loss_ce: 0.006893\n",
            "iteration 11181 : loss : 0.020514, loss_ce: 0.006480\n",
            "iteration 11182 : loss : 0.027053, loss_ce: 0.007880\n",
            "iteration 11183 : loss : 0.027385, loss_ce: 0.008033\n",
            "iteration 11184 : loss : 0.022619, loss_ce: 0.008106\n",
            "iteration 11185 : loss : 0.025577, loss_ce: 0.008813\n",
            "iteration 11186 : loss : 0.031120, loss_ce: 0.008372\n",
            "iteration 11187 : loss : 0.026891, loss_ce: 0.006270\n",
            "iteration 11188 : loss : 0.024681, loss_ce: 0.008293\n",
            "iteration 11189 : loss : 0.020785, loss_ce: 0.004599\n",
            "iteration 11190 : loss : 0.073467, loss_ce: 0.003888\n",
            "iteration 11191 : loss : 0.022847, loss_ce: 0.009654\n",
            "iteration 11192 : loss : 0.037214, loss_ce: 0.003732\n",
            "iteration 11193 : loss : 0.023760, loss_ce: 0.007029\n",
            "iteration 11194 : loss : 0.026257, loss_ce: 0.010203\n",
            "iteration 11195 : loss : 0.085139, loss_ce: 0.009442\n",
            "iteration 11196 : loss : 0.023316, loss_ce: 0.006738\n",
            "iteration 11197 : loss : 0.024701, loss_ce: 0.010853\n",
            "iteration 11198 : loss : 0.025830, loss_ce: 0.008580\n",
            "iteration 11199 : loss : 0.030115, loss_ce: 0.007988\n",
            "iteration 11200 : loss : 0.024155, loss_ce: 0.007321\n",
            "iteration 11201 : loss : 0.029619, loss_ce: 0.013500\n",
            "iteration 11202 : loss : 0.027926, loss_ce: 0.011412\n",
            "iteration 11203 : loss : 0.023883, loss_ce: 0.011307\n",
            "iteration 11204 : loss : 0.023098, loss_ce: 0.010237\n",
            "iteration 11205 : loss : 0.024216, loss_ce: 0.010013\n",
            "iteration 11206 : loss : 0.027142, loss_ce: 0.009664\n",
            "iteration 11207 : loss : 0.024412, loss_ce: 0.012592\n",
            "iteration 11208 : loss : 0.021715, loss_ce: 0.007405\n",
            "iteration 11209 : loss : 0.026653, loss_ce: 0.009626\n",
            "iteration 11210 : loss : 0.081034, loss_ce: 0.008272\n",
            "iteration 11211 : loss : 0.026979, loss_ce: 0.004146\n",
            "iteration 11212 : loss : 0.025396, loss_ce: 0.006008\n",
            "iteration 11213 : loss : 0.073707, loss_ce: 0.004701\n",
            "iteration 11214 : loss : 0.082348, loss_ce: 0.008588\n",
            "iteration 11215 : loss : 0.027080, loss_ce: 0.005449\n",
            "iteration 11216 : loss : 0.029193, loss_ce: 0.007893\n",
            "iteration 11217 : loss : 0.024017, loss_ce: 0.009319\n",
            "iteration 11218 : loss : 0.074686, loss_ce: 0.007217\n",
            "iteration 11219 : loss : 0.024274, loss_ce: 0.011550\n",
            "iteration 11220 : loss : 0.030115, loss_ce: 0.007374\n",
            "iteration 11221 : loss : 0.026616, loss_ce: 0.007900\n",
            "iteration 11222 : loss : 0.077686, loss_ce: 0.005295\n",
            "iteration 11223 : loss : 0.023151, loss_ce: 0.009668\n",
            "iteration 11224 : loss : 0.026421, loss_ce: 0.008870\n",
            "iteration 11225 : loss : 0.028869, loss_ce: 0.008414\n",
            "iteration 11226 : loss : 0.022540, loss_ce: 0.009714\n",
            "iteration 11227 : loss : 0.028597, loss_ce: 0.007619\n",
            "iteration 11228 : loss : 0.025268, loss_ce: 0.006965\n",
            "iteration 11229 : loss : 0.026558, loss_ce: 0.008676\n",
            "iteration 11230 : loss : 0.029800, loss_ce: 0.008638\n",
            "iteration 11231 : loss : 0.022935, loss_ce: 0.009534\n",
            "iteration 11232 : loss : 0.026452, loss_ce: 0.009696\n",
            "iteration 11233 : loss : 0.026257, loss_ce: 0.017414\n",
            "iteration 11234 : loss : 0.028832, loss_ce: 0.007957\n",
            "iteration 11235 : loss : 0.020569, loss_ce: 0.006583\n",
            "iteration 11236 : loss : 0.023173, loss_ce: 0.006686\n",
            "iteration 11237 : loss : 0.032033, loss_ce: 0.007588\n",
            "iteration 11238 : loss : 0.028661, loss_ce: 0.006693\n",
            "iteration 11239 : loss : 0.076565, loss_ce: 0.007788\n",
            "iteration 11240 : loss : 0.029507, loss_ce: 0.004194\n",
            "iteration 11241 : loss : 0.031274, loss_ce: 0.011511\n",
            "iteration 11242 : loss : 0.020026, loss_ce: 0.010105\n",
            "iteration 11243 : loss : 0.080806, loss_ce: 0.009870\n",
            "iteration 11244 : loss : 0.024235, loss_ce: 0.009097\n",
            "iteration 11245 : loss : 0.025471, loss_ce: 0.009024\n",
            "iteration 11246 : loss : 0.021341, loss_ce: 0.006330\n",
            "iteration 11247 : loss : 0.024150, loss_ce: 0.008913\n",
            "iteration 11248 : loss : 0.028409, loss_ce: 0.016342\n",
            "iteration 11249 : loss : 0.074526, loss_ce: 0.006669\n",
            "iteration 11250 : loss : 0.075108, loss_ce: 0.007417\n",
            "iteration 11251 : loss : 0.026984, loss_ce: 0.007631\n",
            "iteration 11252 : loss : 0.022875, loss_ce: 0.007915\n",
            "iteration 11253 : loss : 0.190800, loss_ce: 0.017675\n",
            " 81%|████████████████████▉     | 121/150 [4:24:43<1:03:19, 131.03s/it]iteration 11254 : loss : 0.021006, loss_ce: 0.005507\n",
            "iteration 11255 : loss : 0.023714, loss_ce: 0.008916\n",
            "iteration 11256 : loss : 0.022945, loss_ce: 0.009251\n",
            "iteration 11257 : loss : 0.022692, loss_ce: 0.006913\n",
            "iteration 11258 : loss : 0.025894, loss_ce: 0.006603\n",
            "iteration 11259 : loss : 0.073926, loss_ce: 0.008004\n",
            "iteration 11260 : loss : 0.037439, loss_ce: 0.007729\n",
            "iteration 11261 : loss : 0.025329, loss_ce: 0.009527\n",
            "iteration 11262 : loss : 0.022635, loss_ce: 0.008699\n",
            "iteration 11263 : loss : 0.026982, loss_ce: 0.006619\n",
            "iteration 11264 : loss : 0.020155, loss_ce: 0.006202\n",
            "iteration 11265 : loss : 0.031597, loss_ce: 0.007386\n",
            "iteration 11266 : loss : 0.031896, loss_ce: 0.011184\n",
            "iteration 11267 : loss : 0.024474, loss_ce: 0.013393\n",
            "iteration 11268 : loss : 0.025561, loss_ce: 0.009993\n",
            "iteration 11269 : loss : 0.024592, loss_ce: 0.007632\n",
            "iteration 11270 : loss : 0.025791, loss_ce: 0.009651\n",
            "iteration 11271 : loss : 0.025142, loss_ce: 0.004229\n",
            "iteration 11272 : loss : 0.023230, loss_ce: 0.010058\n",
            "iteration 11273 : loss : 0.034740, loss_ce: 0.014688\n",
            "iteration 11274 : loss : 0.025187, loss_ce: 0.007778\n",
            "iteration 11275 : loss : 0.022063, loss_ce: 0.006390\n",
            "iteration 11276 : loss : 0.076602, loss_ce: 0.005898\n",
            "iteration 11277 : loss : 0.016321, loss_ce: 0.003147\n",
            "iteration 11278 : loss : 0.036465, loss_ce: 0.008025\n",
            "iteration 11279 : loss : 0.025391, loss_ce: 0.005784\n",
            "iteration 11280 : loss : 0.023545, loss_ce: 0.009689\n",
            "iteration 11281 : loss : 0.026235, loss_ce: 0.009454\n",
            "iteration 11282 : loss : 0.026110, loss_ce: 0.006490\n",
            "iteration 11283 : loss : 0.021948, loss_ce: 0.008947\n",
            "iteration 11284 : loss : 0.027284, loss_ce: 0.005030\n",
            "iteration 11285 : loss : 0.074680, loss_ce: 0.005173\n",
            "iteration 11286 : loss : 0.027143, loss_ce: 0.006196\n",
            "iteration 11287 : loss : 0.024967, loss_ce: 0.013067\n",
            "iteration 11288 : loss : 0.022852, loss_ce: 0.008337\n",
            "iteration 11289 : loss : 0.021412, loss_ce: 0.006487\n",
            "iteration 11290 : loss : 0.027867, loss_ce: 0.007231\n",
            "iteration 11291 : loss : 0.024091, loss_ce: 0.008196\n",
            "iteration 11292 : loss : 0.026277, loss_ce: 0.010748\n",
            "iteration 11293 : loss : 0.023334, loss_ce: 0.008169\n",
            "iteration 11294 : loss : 0.023180, loss_ce: 0.011201\n",
            "iteration 11295 : loss : 0.019733, loss_ce: 0.007023\n",
            "iteration 11296 : loss : 0.025657, loss_ce: 0.005951\n",
            "iteration 11297 : loss : 0.023466, loss_ce: 0.008441\n",
            "iteration 11298 : loss : 0.024994, loss_ce: 0.009640\n",
            "iteration 11299 : loss : 0.043915, loss_ce: 0.008168\n",
            "iteration 11300 : loss : 0.022650, loss_ce: 0.007678\n",
            "iteration 11301 : loss : 0.022692, loss_ce: 0.010660\n",
            "iteration 11302 : loss : 0.023410, loss_ce: 0.008095\n",
            "iteration 11303 : loss : 0.024493, loss_ce: 0.012173\n",
            "iteration 11304 : loss : 0.021872, loss_ce: 0.007814\n",
            "iteration 11305 : loss : 0.023991, loss_ce: 0.008499\n",
            "iteration 11306 : loss : 0.072127, loss_ce: 0.004177\n",
            "iteration 11307 : loss : 0.021066, loss_ce: 0.006424\n",
            "iteration 11308 : loss : 0.022877, loss_ce: 0.006783\n",
            "iteration 11309 : loss : 0.041749, loss_ce: 0.007984\n",
            "iteration 11310 : loss : 0.025875, loss_ce: 0.010236\n",
            "iteration 11311 : loss : 0.022538, loss_ce: 0.009739\n",
            "iteration 11312 : loss : 0.025529, loss_ce: 0.012967\n",
            "iteration 11313 : loss : 0.022956, loss_ce: 0.009727\n",
            "iteration 11314 : loss : 0.025096, loss_ce: 0.009431\n",
            "iteration 11315 : loss : 0.022563, loss_ce: 0.007844\n",
            "iteration 11316 : loss : 0.027750, loss_ce: 0.009531\n",
            "iteration 11317 : loss : 0.017794, loss_ce: 0.004689\n",
            "iteration 11318 : loss : 0.023084, loss_ce: 0.010291\n",
            "iteration 11319 : loss : 0.024871, loss_ce: 0.005755\n",
            "iteration 11320 : loss : 0.022292, loss_ce: 0.006230\n",
            "iteration 11321 : loss : 0.025838, loss_ce: 0.008847\n",
            "iteration 11322 : loss : 0.019914, loss_ce: 0.005130\n",
            "iteration 11323 : loss : 0.025304, loss_ce: 0.008576\n",
            "iteration 11324 : loss : 0.022555, loss_ce: 0.008264\n",
            "iteration 11325 : loss : 0.021101, loss_ce: 0.008327\n",
            "iteration 11326 : loss : 0.028312, loss_ce: 0.007494\n",
            "iteration 11327 : loss : 0.023953, loss_ce: 0.009213\n",
            "iteration 11328 : loss : 0.024742, loss_ce: 0.009451\n",
            "iteration 11329 : loss : 0.025888, loss_ce: 0.007833\n",
            "iteration 11330 : loss : 0.040799, loss_ce: 0.006590\n",
            "iteration 11331 : loss : 0.031107, loss_ce: 0.005895\n",
            "iteration 11332 : loss : 0.016996, loss_ce: 0.006346\n",
            "iteration 11333 : loss : 0.025162, loss_ce: 0.005831\n",
            "iteration 11334 : loss : 0.023566, loss_ce: 0.010883\n",
            "iteration 11335 : loss : 0.028445, loss_ce: 0.013247\n",
            "iteration 11336 : loss : 0.022327, loss_ce: 0.005619\n",
            "iteration 11337 : loss : 0.027385, loss_ce: 0.008524\n",
            "iteration 11338 : loss : 0.079844, loss_ce: 0.005286\n",
            "iteration 11339 : loss : 0.023337, loss_ce: 0.008521\n",
            "iteration 11340 : loss : 0.027735, loss_ce: 0.012898\n",
            "iteration 11341 : loss : 0.026758, loss_ce: 0.012433\n",
            "iteration 11342 : loss : 0.030248, loss_ce: 0.010117\n",
            "iteration 11343 : loss : 0.029036, loss_ce: 0.009027\n",
            "iteration 11344 : loss : 0.031447, loss_ce: 0.010764\n",
            "iteration 11345 : loss : 0.018591, loss_ce: 0.006550\n",
            "iteration 11346 : loss : 0.441668, loss_ce: 0.000411\n",
            " 81%|█████████████████████▏    | 122/150 [4:26:54<1:01:11, 131.12s/it]iteration 11347 : loss : 0.026862, loss_ce: 0.007758\n",
            "iteration 11348 : loss : 0.025802, loss_ce: 0.006176\n",
            "iteration 11349 : loss : 0.026220, loss_ce: 0.008112\n",
            "iteration 11350 : loss : 0.024961, loss_ce: 0.013803\n",
            "iteration 11351 : loss : 0.178452, loss_ce: 0.002046\n",
            "iteration 11352 : loss : 0.028771, loss_ce: 0.009232\n",
            "iteration 11353 : loss : 0.025668, loss_ce: 0.010821\n",
            "iteration 11354 : loss : 0.019971, loss_ce: 0.005431\n",
            "iteration 11355 : loss : 0.029006, loss_ce: 0.009191\n",
            "iteration 11356 : loss : 0.025230, loss_ce: 0.009480\n",
            "iteration 11357 : loss : 0.022914, loss_ce: 0.011201\n",
            "iteration 11358 : loss : 0.028540, loss_ce: 0.008758\n",
            "iteration 11359 : loss : 0.022003, loss_ce: 0.008680\n",
            "iteration 11360 : loss : 0.073086, loss_ce: 0.007293\n",
            "iteration 11361 : loss : 0.025662, loss_ce: 0.007760\n",
            "iteration 11362 : loss : 0.035530, loss_ce: 0.007969\n",
            "iteration 11363 : loss : 0.023045, loss_ce: 0.008527\n",
            "iteration 11364 : loss : 0.080158, loss_ce: 0.007074\n",
            "iteration 11365 : loss : 0.028053, loss_ce: 0.008112\n",
            "iteration 11366 : loss : 0.017444, loss_ce: 0.006990\n",
            "iteration 11367 : loss : 0.020791, loss_ce: 0.007241\n",
            "iteration 11368 : loss : 0.022128, loss_ce: 0.005972\n",
            "iteration 11369 : loss : 0.027369, loss_ce: 0.008278\n",
            "iteration 11370 : loss : 0.025615, loss_ce: 0.005926\n",
            "iteration 11371 : loss : 0.023061, loss_ce: 0.008168\n",
            "iteration 11372 : loss : 0.023497, loss_ce: 0.007625\n",
            "iteration 11373 : loss : 0.021007, loss_ce: 0.005907\n",
            "iteration 11374 : loss : 0.024587, loss_ce: 0.007667\n",
            "iteration 11375 : loss : 0.024399, loss_ce: 0.005412\n",
            "iteration 11376 : loss : 0.022751, loss_ce: 0.006575\n",
            "iteration 11377 : loss : 0.072891, loss_ce: 0.005078\n",
            "iteration 11378 : loss : 0.029114, loss_ce: 0.005151\n",
            "iteration 11379 : loss : 0.020383, loss_ce: 0.008438\n",
            "iteration 11380 : loss : 0.024814, loss_ce: 0.006491\n",
            "iteration 11381 : loss : 0.029219, loss_ce: 0.011341\n",
            "iteration 11382 : loss : 0.018558, loss_ce: 0.005845\n",
            "iteration 11383 : loss : 0.024108, loss_ce: 0.007206\n",
            "iteration 11384 : loss : 0.024319, loss_ce: 0.007498\n",
            "iteration 11385 : loss : 0.023081, loss_ce: 0.008645\n",
            "iteration 11386 : loss : 0.023910, loss_ce: 0.011164\n",
            "iteration 11387 : loss : 0.035078, loss_ce: 0.007018\n",
            "iteration 11388 : loss : 0.023661, loss_ce: 0.008560\n",
            "iteration 11389 : loss : 0.030993, loss_ce: 0.009151\n",
            "iteration 11390 : loss : 0.022163, loss_ce: 0.007971\n",
            "iteration 11391 : loss : 0.026213, loss_ce: 0.007055\n",
            "iteration 11392 : loss : 0.025216, loss_ce: 0.011735\n",
            "iteration 11393 : loss : 0.023231, loss_ce: 0.010074\n",
            "iteration 11394 : loss : 0.025762, loss_ce: 0.008023\n",
            "iteration 11395 : loss : 0.028160, loss_ce: 0.008794\n",
            "iteration 11396 : loss : 0.027191, loss_ce: 0.011311\n",
            "iteration 11397 : loss : 0.075612, loss_ce: 0.005127\n",
            "iteration 11398 : loss : 0.022297, loss_ce: 0.008679\n",
            "iteration 11399 : loss : 0.073940, loss_ce: 0.005824\n",
            "iteration 11400 : loss : 0.024166, loss_ce: 0.007361\n",
            "iteration 11401 : loss : 0.030465, loss_ce: 0.007675\n",
            "iteration 11402 : loss : 0.028054, loss_ce: 0.009039\n",
            "iteration 11403 : loss : 0.080675, loss_ce: 0.004907\n",
            "iteration 11404 : loss : 0.021548, loss_ce: 0.008854\n",
            "iteration 11405 : loss : 0.026479, loss_ce: 0.011388\n",
            "iteration 11406 : loss : 0.030009, loss_ce: 0.007295\n",
            "iteration 11407 : loss : 0.024826, loss_ce: 0.011448\n",
            "iteration 11408 : loss : 0.025112, loss_ce: 0.009476\n",
            "iteration 11409 : loss : 0.048430, loss_ce: 0.008968\n",
            "iteration 11410 : loss : 0.022505, loss_ce: 0.006520\n",
            "iteration 11411 : loss : 0.023302, loss_ce: 0.011781\n",
            "iteration 11412 : loss : 0.022911, loss_ce: 0.007257\n",
            "iteration 11413 : loss : 0.030288, loss_ce: 0.012854\n",
            "iteration 11414 : loss : 0.032828, loss_ce: 0.007724\n",
            "iteration 11415 : loss : 0.025764, loss_ce: 0.010073\n",
            "iteration 11416 : loss : 0.026770, loss_ce: 0.013009\n",
            "iteration 11417 : loss : 0.055432, loss_ce: 0.008300\n",
            "iteration 11418 : loss : 0.022482, loss_ce: 0.008060\n",
            "iteration 11419 : loss : 0.077540, loss_ce: 0.008113\n",
            "iteration 11420 : loss : 0.025673, loss_ce: 0.009605\n",
            "iteration 11421 : loss : 0.026279, loss_ce: 0.011155\n",
            "iteration 11422 : loss : 0.025355, loss_ce: 0.013323\n",
            "iteration 11423 : loss : 0.027842, loss_ce: 0.010054\n",
            "iteration 11424 : loss : 0.023987, loss_ce: 0.009709\n",
            "iteration 11425 : loss : 0.027179, loss_ce: 0.006450\n",
            "iteration 11426 : loss : 0.048954, loss_ce: 0.005760\n",
            "iteration 11427 : loss : 0.027300, loss_ce: 0.007378\n",
            "iteration 11428 : loss : 0.030226, loss_ce: 0.009759\n",
            "iteration 11429 : loss : 0.021258, loss_ce: 0.006762\n",
            "iteration 11430 : loss : 0.021734, loss_ce: 0.008079\n",
            "iteration 11431 : loss : 0.027981, loss_ce: 0.008525\n",
            "iteration 11432 : loss : 0.020747, loss_ce: 0.005464\n",
            "iteration 11433 : loss : 0.029263, loss_ce: 0.010370\n",
            "iteration 11434 : loss : 0.028452, loss_ce: 0.009921\n",
            "iteration 11435 : loss : 0.038076, loss_ce: 0.010912\n",
            "iteration 11436 : loss : 0.022941, loss_ce: 0.008675\n",
            "iteration 11437 : loss : 0.025763, loss_ce: 0.012309\n",
            "iteration 11438 : loss : 0.022809, loss_ce: 0.007383\n",
            "iteration 11439 : loss : 0.444080, loss_ce: 0.000589\n",
            " 82%|██████████████████████▉     | 123/150 [4:29:04<58:51, 130.80s/it]iteration 11440 : loss : 0.026708, loss_ce: 0.008598\n",
            "iteration 11441 : loss : 0.041939, loss_ce: 0.007593\n",
            "iteration 11442 : loss : 0.024623, loss_ce: 0.009237\n",
            "iteration 11443 : loss : 0.076387, loss_ce: 0.007776\n",
            "iteration 11444 : loss : 0.023344, loss_ce: 0.007967\n",
            "iteration 11445 : loss : 0.023108, loss_ce: 0.011081\n",
            "iteration 11446 : loss : 0.028075, loss_ce: 0.008490\n",
            "iteration 11447 : loss : 0.125866, loss_ce: 0.004394\n",
            "iteration 11448 : loss : 0.026183, loss_ce: 0.011715\n",
            "iteration 11449 : loss : 0.021957, loss_ce: 0.009303\n",
            "iteration 11450 : loss : 0.024325, loss_ce: 0.008463\n",
            "iteration 11451 : loss : 0.032321, loss_ce: 0.008656\n",
            "iteration 11452 : loss : 0.024932, loss_ce: 0.011723\n",
            "iteration 11453 : loss : 0.074093, loss_ce: 0.003868\n",
            "iteration 11454 : loss : 0.031530, loss_ce: 0.008177\n",
            "iteration 11455 : loss : 0.023787, loss_ce: 0.011671\n",
            "iteration 11456 : loss : 0.025892, loss_ce: 0.010114\n",
            "iteration 11457 : loss : 0.027025, loss_ce: 0.011695\n",
            "iteration 11458 : loss : 0.025589, loss_ce: 0.009230\n",
            "iteration 11459 : loss : 0.022347, loss_ce: 0.008521\n",
            "iteration 11460 : loss : 0.024778, loss_ce: 0.009946\n",
            "iteration 11461 : loss : 0.073912, loss_ce: 0.005490\n",
            "iteration 11462 : loss : 0.025006, loss_ce: 0.006943\n",
            "iteration 11463 : loss : 0.023251, loss_ce: 0.009204\n",
            "iteration 11464 : loss : 0.027785, loss_ce: 0.006383\n",
            "iteration 11465 : loss : 0.029832, loss_ce: 0.007466\n",
            "iteration 11466 : loss : 0.021826, loss_ce: 0.008104\n",
            "iteration 11467 : loss : 0.026778, loss_ce: 0.005706\n",
            "iteration 11468 : loss : 0.028237, loss_ce: 0.014168\n",
            "iteration 11469 : loss : 0.025687, loss_ce: 0.011485\n",
            "iteration 11470 : loss : 0.076067, loss_ce: 0.007064\n",
            "iteration 11471 : loss : 0.024531, loss_ce: 0.010907\n",
            "iteration 11472 : loss : 0.023929, loss_ce: 0.004291\n",
            "iteration 11473 : loss : 0.020159, loss_ce: 0.006917\n",
            "iteration 11474 : loss : 0.020722, loss_ce: 0.006731\n",
            "iteration 11475 : loss : 0.026256, loss_ce: 0.011104\n",
            "iteration 11476 : loss : 0.025553, loss_ce: 0.008842\n",
            "iteration 11477 : loss : 0.028380, loss_ce: 0.010093\n",
            "iteration 11478 : loss : 0.075913, loss_ce: 0.006942\n",
            "iteration 11479 : loss : 0.025908, loss_ce: 0.008394\n",
            "iteration 11480 : loss : 0.027576, loss_ce: 0.007707\n",
            "iteration 11481 : loss : 0.021681, loss_ce: 0.003943\n",
            "iteration 11482 : loss : 0.026444, loss_ce: 0.007785\n",
            "iteration 11483 : loss : 0.018409, loss_ce: 0.006562\n",
            "iteration 11484 : loss : 0.023918, loss_ce: 0.004167\n",
            "iteration 11485 : loss : 0.031094, loss_ce: 0.006330\n",
            "iteration 11486 : loss : 0.020621, loss_ce: 0.005141\n",
            "iteration 11487 : loss : 0.022486, loss_ce: 0.008200\n",
            "iteration 11488 : loss : 0.032182, loss_ce: 0.008504\n",
            "iteration 11489 : loss : 0.029201, loss_ce: 0.008179\n",
            "iteration 11490 : loss : 0.020303, loss_ce: 0.006415\n",
            "iteration 11491 : loss : 0.026888, loss_ce: 0.012499\n",
            "iteration 11492 : loss : 0.025524, loss_ce: 0.009605\n",
            "iteration 11493 : loss : 0.020633, loss_ce: 0.008058\n",
            "iteration 11494 : loss : 0.025788, loss_ce: 0.009161\n",
            "iteration 11495 : loss : 0.025179, loss_ce: 0.008721\n",
            "iteration 11496 : loss : 0.025426, loss_ce: 0.012700\n",
            "iteration 11497 : loss : 0.022103, loss_ce: 0.007629\n",
            "iteration 11498 : loss : 0.030354, loss_ce: 0.006056\n",
            "iteration 11499 : loss : 0.025310, loss_ce: 0.009398\n",
            "iteration 11500 : loss : 0.024314, loss_ce: 0.007870\n",
            "iteration 11501 : loss : 0.022390, loss_ce: 0.007500\n",
            "iteration 11502 : loss : 0.025275, loss_ce: 0.012188\n",
            "iteration 11503 : loss : 0.031534, loss_ce: 0.006115\n",
            "iteration 11504 : loss : 0.024992, loss_ce: 0.012235\n",
            "iteration 11505 : loss : 0.024917, loss_ce: 0.010846\n",
            "iteration 11506 : loss : 0.024805, loss_ce: 0.011051\n",
            "iteration 11507 : loss : 0.021002, loss_ce: 0.008628\n",
            "iteration 11508 : loss : 0.021880, loss_ce: 0.006486\n",
            "iteration 11509 : loss : 0.020480, loss_ce: 0.005109\n",
            "iteration 11510 : loss : 0.026653, loss_ce: 0.011782\n",
            "iteration 11511 : loss : 0.027137, loss_ce: 0.009557\n",
            "iteration 11512 : loss : 0.028387, loss_ce: 0.010826\n",
            "iteration 11513 : loss : 0.029185, loss_ce: 0.014693\n",
            "iteration 11514 : loss : 0.029339, loss_ce: 0.007742\n",
            "iteration 11515 : loss : 0.023584, loss_ce: 0.006801\n",
            "iteration 11516 : loss : 0.022672, loss_ce: 0.008238\n",
            "iteration 11517 : loss : 0.077440, loss_ce: 0.006774\n",
            "iteration 11518 : loss : 0.021591, loss_ce: 0.004758\n",
            "iteration 11519 : loss : 0.022856, loss_ce: 0.006743\n",
            "iteration 11520 : loss : 0.023950, loss_ce: 0.008732\n",
            "iteration 11521 : loss : 0.181794, loss_ce: 0.005181\n",
            "iteration 11522 : loss : 0.023781, loss_ce: 0.008706\n",
            "iteration 11523 : loss : 0.024301, loss_ce: 0.008697\n",
            "iteration 11524 : loss : 0.022276, loss_ce: 0.003629\n",
            "iteration 11525 : loss : 0.030102, loss_ce: 0.010849\n",
            "iteration 11526 : loss : 0.026542, loss_ce: 0.007005\n",
            "iteration 11527 : loss : 0.026270, loss_ce: 0.009431\n",
            "iteration 11528 : loss : 0.023684, loss_ce: 0.008446\n",
            "iteration 11529 : loss : 0.022728, loss_ce: 0.007170\n",
            "iteration 11530 : loss : 0.027107, loss_ce: 0.012702\n",
            "iteration 11531 : loss : 0.021190, loss_ce: 0.006852\n",
            "iteration 11532 : loss : 0.185410, loss_ce: 0.004250\n",
            " 83%|███████████████████████▏    | 124/150 [4:31:16<56:44, 130.92s/it]iteration 11533 : loss : 0.023563, loss_ce: 0.006583\n",
            "iteration 11534 : loss : 0.023553, loss_ce: 0.007364\n",
            "iteration 11535 : loss : 0.022952, loss_ce: 0.006547\n",
            "iteration 11536 : loss : 0.023803, loss_ce: 0.010606\n",
            "iteration 11537 : loss : 0.025473, loss_ce: 0.008039\n",
            "iteration 11538 : loss : 0.019703, loss_ce: 0.005209\n",
            "iteration 11539 : loss : 0.023059, loss_ce: 0.007772\n",
            "iteration 11540 : loss : 0.023796, loss_ce: 0.009640\n",
            "iteration 11541 : loss : 0.027079, loss_ce: 0.008595\n",
            "iteration 11542 : loss : 0.021750, loss_ce: 0.008584\n",
            "iteration 11543 : loss : 0.021856, loss_ce: 0.008679\n",
            "iteration 11544 : loss : 0.048327, loss_ce: 0.007117\n",
            "iteration 11545 : loss : 0.025340, loss_ce: 0.010584\n",
            "iteration 11546 : loss : 0.021773, loss_ce: 0.009644\n",
            "iteration 11547 : loss : 0.030502, loss_ce: 0.009704\n",
            "iteration 11548 : loss : 0.020605, loss_ce: 0.009076\n",
            "iteration 11549 : loss : 0.020891, loss_ce: 0.009539\n",
            "iteration 11550 : loss : 0.026540, loss_ce: 0.006040\n",
            "iteration 11551 : loss : 0.025001, loss_ce: 0.012633\n",
            "iteration 11552 : loss : 0.024535, loss_ce: 0.006745\n",
            "iteration 11553 : loss : 0.026073, loss_ce: 0.007969\n",
            "iteration 11554 : loss : 0.022776, loss_ce: 0.010847\n",
            "iteration 11555 : loss : 0.036309, loss_ce: 0.010771\n",
            "iteration 11556 : loss : 0.021016, loss_ce: 0.007627\n",
            "iteration 11557 : loss : 0.023721, loss_ce: 0.009634\n",
            "iteration 11558 : loss : 0.022251, loss_ce: 0.007381\n",
            "iteration 11559 : loss : 0.028904, loss_ce: 0.006437\n",
            "iteration 11560 : loss : 0.022712, loss_ce: 0.004624\n",
            "iteration 11561 : loss : 0.032884, loss_ce: 0.011302\n",
            "iteration 11562 : loss : 0.023008, loss_ce: 0.008315\n",
            "iteration 11563 : loss : 0.025483, loss_ce: 0.013625\n",
            "iteration 11564 : loss : 0.020443, loss_ce: 0.007123\n",
            "iteration 11565 : loss : 0.022781, loss_ce: 0.008642\n",
            "iteration 11566 : loss : 0.024421, loss_ce: 0.009289\n",
            "iteration 11567 : loss : 0.025365, loss_ce: 0.009525\n",
            "iteration 11568 : loss : 0.024266, loss_ce: 0.008817\n",
            "iteration 11569 : loss : 0.022056, loss_ce: 0.005051\n",
            "iteration 11570 : loss : 0.023860, loss_ce: 0.004902\n",
            "iteration 11571 : loss : 0.025865, loss_ce: 0.007496\n",
            "iteration 11572 : loss : 0.021589, loss_ce: 0.008547\n",
            "iteration 11573 : loss : 0.021691, loss_ce: 0.009915\n",
            "iteration 11574 : loss : 0.027849, loss_ce: 0.009021\n",
            "iteration 11575 : loss : 0.025420, loss_ce: 0.009648\n",
            "iteration 11576 : loss : 0.025883, loss_ce: 0.011027\n",
            "iteration 11577 : loss : 0.021112, loss_ce: 0.006939\n",
            "iteration 11578 : loss : 0.021776, loss_ce: 0.009605\n",
            "iteration 11579 : loss : 0.021291, loss_ce: 0.006006\n",
            "iteration 11580 : loss : 0.026024, loss_ce: 0.011695\n",
            "iteration 11581 : loss : 0.025904, loss_ce: 0.011054\n",
            "iteration 11582 : loss : 0.022828, loss_ce: 0.005959\n",
            "iteration 11583 : loss : 0.024206, loss_ce: 0.007067\n",
            "iteration 11584 : loss : 0.023856, loss_ce: 0.008610\n",
            "iteration 11585 : loss : 0.029245, loss_ce: 0.011290\n",
            "iteration 11586 : loss : 0.025327, loss_ce: 0.010638\n",
            "iteration 11587 : loss : 0.037019, loss_ce: 0.006843\n",
            "iteration 11588 : loss : 0.022185, loss_ce: 0.007761\n",
            "iteration 11589 : loss : 0.027349, loss_ce: 0.007266\n",
            "iteration 11590 : loss : 0.026208, loss_ce: 0.009469\n",
            "iteration 11591 : loss : 0.025878, loss_ce: 0.005849\n",
            "iteration 11592 : loss : 0.022544, loss_ce: 0.007864\n",
            "iteration 11593 : loss : 0.079159, loss_ce: 0.005008\n",
            "iteration 11594 : loss : 0.029122, loss_ce: 0.010995\n",
            "iteration 11595 : loss : 0.077768, loss_ce: 0.010792\n",
            "iteration 11596 : loss : 0.028416, loss_ce: 0.008782\n",
            "iteration 11597 : loss : 0.027661, loss_ce: 0.010164\n",
            "iteration 11598 : loss : 0.079024, loss_ce: 0.004827\n",
            "iteration 11599 : loss : 0.024400, loss_ce: 0.009258\n",
            "iteration 11600 : loss : 0.024040, loss_ce: 0.006436\n",
            "iteration 11601 : loss : 0.030441, loss_ce: 0.007248\n",
            "iteration 11602 : loss : 0.027415, loss_ce: 0.007492\n",
            "iteration 11603 : loss : 0.020403, loss_ce: 0.008563\n",
            "iteration 11604 : loss : 0.074674, loss_ce: 0.006072\n",
            "iteration 11605 : loss : 0.025618, loss_ce: 0.009298\n",
            "iteration 11606 : loss : 0.028915, loss_ce: 0.008053\n",
            "iteration 11607 : loss : 0.020367, loss_ce: 0.007481\n",
            "iteration 11608 : loss : 0.023984, loss_ce: 0.009227\n",
            "iteration 11609 : loss : 0.025188, loss_ce: 0.011371\n",
            "iteration 11610 : loss : 0.024911, loss_ce: 0.008263\n",
            "iteration 11611 : loss : 0.018277, loss_ce: 0.006928\n",
            "iteration 11612 : loss : 0.024858, loss_ce: 0.006525\n",
            "iteration 11613 : loss : 0.021816, loss_ce: 0.005430\n",
            "iteration 11614 : loss : 0.072803, loss_ce: 0.006436\n",
            "iteration 11615 : loss : 0.030854, loss_ce: 0.011244\n",
            "iteration 11616 : loss : 0.020359, loss_ce: 0.006191\n",
            "iteration 11617 : loss : 0.027109, loss_ce: 0.005977\n",
            "iteration 11618 : loss : 0.032694, loss_ce: 0.006373\n",
            "iteration 11619 : loss : 0.019251, loss_ce: 0.007232\n",
            "iteration 11620 : loss : 0.022115, loss_ce: 0.007636\n",
            "iteration 11621 : loss : 0.025494, loss_ce: 0.008659\n",
            "iteration 11622 : loss : 0.025958, loss_ce: 0.003790\n",
            "iteration 11623 : loss : 0.077334, loss_ce: 0.006071\n",
            "iteration 11624 : loss : 0.024185, loss_ce: 0.009724\n",
            "iteration 11625 : loss : 0.442485, loss_ce: 0.000622\n",
            " 83%|███████████████████████▎    | 125/150 [4:33:26<54:31, 130.87s/it]iteration 11626 : loss : 0.021282, loss_ce: 0.007777\n",
            "iteration 11627 : loss : 0.032579, loss_ce: 0.006213\n",
            "iteration 11628 : loss : 0.127660, loss_ce: 0.001728\n",
            "iteration 11629 : loss : 0.021895, loss_ce: 0.011761\n",
            "iteration 11630 : loss : 0.021156, loss_ce: 0.008691\n",
            "iteration 11631 : loss : 0.020389, loss_ce: 0.008089\n",
            "iteration 11632 : loss : 0.021221, loss_ce: 0.007207\n",
            "iteration 11633 : loss : 0.023231, loss_ce: 0.009829\n",
            "iteration 11634 : loss : 0.025881, loss_ce: 0.010614\n",
            "iteration 11635 : loss : 0.027916, loss_ce: 0.009259\n",
            "iteration 11636 : loss : 0.024591, loss_ce: 0.010789\n",
            "iteration 11637 : loss : 0.020938, loss_ce: 0.005515\n",
            "iteration 11638 : loss : 0.075788, loss_ce: 0.008535\n",
            "iteration 11639 : loss : 0.027540, loss_ce: 0.008757\n",
            "iteration 11640 : loss : 0.023011, loss_ce: 0.006073\n",
            "iteration 11641 : loss : 0.021865, loss_ce: 0.009067\n",
            "iteration 11642 : loss : 0.023816, loss_ce: 0.009587\n",
            "iteration 11643 : loss : 0.071485, loss_ce: 0.003800\n",
            "iteration 11644 : loss : 0.022300, loss_ce: 0.004957\n",
            "iteration 11645 : loss : 0.025321, loss_ce: 0.009741\n",
            "iteration 11646 : loss : 0.072630, loss_ce: 0.005978\n",
            "iteration 11647 : loss : 0.024987, loss_ce: 0.010784\n",
            "iteration 11648 : loss : 0.022569, loss_ce: 0.010205\n",
            "iteration 11649 : loss : 0.026679, loss_ce: 0.012898\n",
            "iteration 11650 : loss : 0.025105, loss_ce: 0.005903\n",
            "iteration 11651 : loss : 0.024982, loss_ce: 0.009256\n",
            "iteration 11652 : loss : 0.020483, loss_ce: 0.005851\n",
            "iteration 11653 : loss : 0.023801, loss_ce: 0.010005\n",
            "iteration 11654 : loss : 0.021343, loss_ce: 0.006005\n",
            "iteration 11655 : loss : 0.025987, loss_ce: 0.010169\n",
            "iteration 11656 : loss : 0.024062, loss_ce: 0.011277\n",
            "iteration 11657 : loss : 0.074333, loss_ce: 0.005261\n",
            "iteration 11658 : loss : 0.071727, loss_ce: 0.005049\n",
            "iteration 11659 : loss : 0.075550, loss_ce: 0.006451\n",
            "iteration 11660 : loss : 0.077378, loss_ce: 0.007154\n",
            "iteration 11661 : loss : 0.028369, loss_ce: 0.010086\n",
            "iteration 11662 : loss : 0.028549, loss_ce: 0.009067\n",
            "iteration 11663 : loss : 0.034957, loss_ce: 0.008014\n",
            "iteration 11664 : loss : 0.024674, loss_ce: 0.007011\n",
            "iteration 11665 : loss : 0.026187, loss_ce: 0.008138\n",
            "iteration 11666 : loss : 0.027054, loss_ce: 0.006753\n",
            "iteration 11667 : loss : 0.021242, loss_ce: 0.006519\n",
            "iteration 11668 : loss : 0.024742, loss_ce: 0.008320\n",
            "iteration 11669 : loss : 0.077372, loss_ce: 0.007580\n",
            "iteration 11670 : loss : 0.021280, loss_ce: 0.006282\n",
            "iteration 11671 : loss : 0.020537, loss_ce: 0.006007\n",
            "iteration 11672 : loss : 0.077143, loss_ce: 0.005222\n",
            "iteration 11673 : loss : 0.027856, loss_ce: 0.012118\n",
            "iteration 11674 : loss : 0.025095, loss_ce: 0.008438\n",
            "iteration 11675 : loss : 0.028736, loss_ce: 0.010205\n",
            "iteration 11676 : loss : 0.025674, loss_ce: 0.008354\n",
            "iteration 11677 : loss : 0.035256, loss_ce: 0.006814\n",
            "iteration 11678 : loss : 0.021305, loss_ce: 0.008314\n",
            "iteration 11679 : loss : 0.021513, loss_ce: 0.006666\n",
            "iteration 11680 : loss : 0.021093, loss_ce: 0.006105\n",
            "iteration 11681 : loss : 0.020899, loss_ce: 0.006029\n",
            "iteration 11682 : loss : 0.023175, loss_ce: 0.011723\n",
            "iteration 11683 : loss : 0.030774, loss_ce: 0.008432\n",
            "iteration 11684 : loss : 0.028798, loss_ce: 0.011688\n",
            "iteration 11685 : loss : 0.031868, loss_ce: 0.005487\n",
            "iteration 11686 : loss : 0.026987, loss_ce: 0.011544\n",
            "iteration 11687 : loss : 0.028332, loss_ce: 0.009337\n",
            "iteration 11688 : loss : 0.022539, loss_ce: 0.008559\n",
            "iteration 11689 : loss : 0.024464, loss_ce: 0.011106\n",
            "iteration 11690 : loss : 0.030289, loss_ce: 0.005892\n",
            "iteration 11691 : loss : 0.023072, loss_ce: 0.007439\n",
            "iteration 11692 : loss : 0.024771, loss_ce: 0.005438\n",
            "iteration 11693 : loss : 0.024687, loss_ce: 0.012349\n",
            "iteration 11694 : loss : 0.020026, loss_ce: 0.005870\n",
            "iteration 11695 : loss : 0.021677, loss_ce: 0.010264\n",
            "iteration 11696 : loss : 0.023615, loss_ce: 0.010773\n",
            "iteration 11697 : loss : 0.025042, loss_ce: 0.007599\n",
            "iteration 11698 : loss : 0.022631, loss_ce: 0.008747\n",
            "iteration 11699 : loss : 0.021445, loss_ce: 0.009274\n",
            "iteration 11700 : loss : 0.023367, loss_ce: 0.007916\n",
            "iteration 11701 : loss : 0.023189, loss_ce: 0.008671\n",
            "iteration 11702 : loss : 0.028545, loss_ce: 0.008958\n",
            "iteration 11703 : loss : 0.024965, loss_ce: 0.011991\n",
            "iteration 11704 : loss : 0.022294, loss_ce: 0.008296\n",
            "iteration 11705 : loss : 0.022859, loss_ce: 0.006708\n",
            "iteration 11706 : loss : 0.025447, loss_ce: 0.013539\n",
            "iteration 11707 : loss : 0.039944, loss_ce: 0.007430\n",
            "iteration 11708 : loss : 0.024762, loss_ce: 0.009754\n",
            "iteration 11709 : loss : 0.024921, loss_ce: 0.010180\n",
            "iteration 11710 : loss : 0.027061, loss_ce: 0.008640\n",
            "iteration 11711 : loss : 0.078514, loss_ce: 0.007357\n",
            "iteration 11712 : loss : 0.023119, loss_ce: 0.008598\n",
            "iteration 11713 : loss : 0.021164, loss_ce: 0.005523\n",
            "iteration 11714 : loss : 0.031780, loss_ce: 0.004974\n",
            "iteration 11715 : loss : 0.026429, loss_ce: 0.006539\n",
            "iteration 11716 : loss : 0.026696, loss_ce: 0.007053\n",
            "iteration 11717 : loss : 0.024816, loss_ce: 0.009985\n",
            "iteration 11718 : loss : 0.047472, loss_ce: 0.016901\n",
            " 84%|███████████████████████▌    | 126/150 [4:35:36<52:12, 130.53s/it]iteration 11719 : loss : 0.024268, loss_ce: 0.009697\n",
            "iteration 11720 : loss : 0.020585, loss_ce: 0.003715\n",
            "iteration 11721 : loss : 0.028459, loss_ce: 0.007168\n",
            "iteration 11722 : loss : 0.023840, loss_ce: 0.011203\n",
            "iteration 11723 : loss : 0.069036, loss_ce: 0.007597\n",
            "iteration 11724 : loss : 0.028456, loss_ce: 0.009503\n",
            "iteration 11725 : loss : 0.024741, loss_ce: 0.010656\n",
            "iteration 11726 : loss : 0.022325, loss_ce: 0.007996\n",
            "iteration 11727 : loss : 0.019262, loss_ce: 0.005380\n",
            "iteration 11728 : loss : 0.024416, loss_ce: 0.009733\n",
            "iteration 11729 : loss : 0.025478, loss_ce: 0.010374\n",
            "iteration 11730 : loss : 0.023966, loss_ce: 0.008440\n",
            "iteration 11731 : loss : 0.022801, loss_ce: 0.009673\n",
            "iteration 11732 : loss : 0.021912, loss_ce: 0.007949\n",
            "iteration 11733 : loss : 0.025197, loss_ce: 0.006986\n",
            "iteration 11734 : loss : 0.019942, loss_ce: 0.005455\n",
            "iteration 11735 : loss : 0.021794, loss_ce: 0.008815\n",
            "iteration 11736 : loss : 0.021540, loss_ce: 0.008464\n",
            "iteration 11737 : loss : 0.020901, loss_ce: 0.006108\n",
            "iteration 11738 : loss : 0.023801, loss_ce: 0.006435\n",
            "iteration 11739 : loss : 0.024500, loss_ce: 0.007716\n",
            "iteration 11740 : loss : 0.017525, loss_ce: 0.006806\n",
            "iteration 11741 : loss : 0.025894, loss_ce: 0.012085\n",
            "iteration 11742 : loss : 0.021104, loss_ce: 0.007660\n",
            "iteration 11743 : loss : 0.023182, loss_ce: 0.008717\n",
            "iteration 11744 : loss : 0.026875, loss_ce: 0.006545\n",
            "iteration 11745 : loss : 0.025439, loss_ce: 0.011082\n",
            "iteration 11746 : loss : 0.027861, loss_ce: 0.012896\n",
            "iteration 11747 : loss : 0.020260, loss_ce: 0.009806\n",
            "iteration 11748 : loss : 0.026374, loss_ce: 0.005582\n",
            "iteration 11749 : loss : 0.034836, loss_ce: 0.008232\n",
            "iteration 11750 : loss : 0.023369, loss_ce: 0.004791\n",
            "iteration 11751 : loss : 0.074949, loss_ce: 0.006804\n",
            "iteration 11752 : loss : 0.027929, loss_ce: 0.005325\n",
            "iteration 11753 : loss : 0.020020, loss_ce: 0.007126\n",
            "iteration 11754 : loss : 0.030276, loss_ce: 0.004933\n",
            "iteration 11755 : loss : 0.023824, loss_ce: 0.008957\n",
            "iteration 11756 : loss : 0.028419, loss_ce: 0.011572\n",
            "iteration 11757 : loss : 0.021306, loss_ce: 0.006545\n",
            "iteration 11758 : loss : 0.023128, loss_ce: 0.009983\n",
            "iteration 11759 : loss : 0.026569, loss_ce: 0.006030\n",
            "iteration 11760 : loss : 0.024914, loss_ce: 0.014584\n",
            "iteration 11761 : loss : 0.024300, loss_ce: 0.007170\n",
            "iteration 11762 : loss : 0.027030, loss_ce: 0.007770\n",
            "iteration 11763 : loss : 0.021741, loss_ce: 0.006201\n",
            "iteration 11764 : loss : 0.023410, loss_ce: 0.005477\n",
            "iteration 11765 : loss : 0.026901, loss_ce: 0.008960\n",
            "iteration 11766 : loss : 0.071974, loss_ce: 0.007276\n",
            "iteration 11767 : loss : 0.029627, loss_ce: 0.008877\n",
            "iteration 11768 : loss : 0.023081, loss_ce: 0.004734\n",
            "iteration 11769 : loss : 0.073689, loss_ce: 0.007699\n",
            "iteration 11770 : loss : 0.026836, loss_ce: 0.007815\n",
            "iteration 11771 : loss : 0.024385, loss_ce: 0.007708\n",
            "iteration 11772 : loss : 0.080072, loss_ce: 0.004055\n",
            "iteration 11773 : loss : 0.022398, loss_ce: 0.007091\n",
            "iteration 11774 : loss : 0.024523, loss_ce: 0.007721\n",
            "iteration 11775 : loss : 0.050702, loss_ce: 0.006421\n",
            "iteration 11776 : loss : 0.022799, loss_ce: 0.007624\n",
            "iteration 11777 : loss : 0.022985, loss_ce: 0.006758\n",
            "iteration 11778 : loss : 0.025129, loss_ce: 0.010907\n",
            "iteration 11779 : loss : 0.027893, loss_ce: 0.011446\n",
            "iteration 11780 : loss : 0.034252, loss_ce: 0.012052\n",
            "iteration 11781 : loss : 0.031550, loss_ce: 0.009756\n",
            "iteration 11782 : loss : 0.024281, loss_ce: 0.006999\n",
            "iteration 11783 : loss : 0.031989, loss_ce: 0.015061\n",
            "iteration 11784 : loss : 0.021991, loss_ce: 0.006370\n",
            "iteration 11785 : loss : 0.037090, loss_ce: 0.008611\n",
            "iteration 11786 : loss : 0.028267, loss_ce: 0.007492\n",
            "iteration 11787 : loss : 0.020960, loss_ce: 0.006819\n",
            "iteration 11788 : loss : 0.024744, loss_ce: 0.009474\n",
            "iteration 11789 : loss : 0.033398, loss_ce: 0.007969\n",
            "iteration 11790 : loss : 0.025439, loss_ce: 0.009114\n",
            "iteration 11791 : loss : 0.027215, loss_ce: 0.011399\n",
            "iteration 11792 : loss : 0.024531, loss_ce: 0.010924\n",
            "iteration 11793 : loss : 0.023369, loss_ce: 0.007088\n",
            "iteration 11794 : loss : 0.026939, loss_ce: 0.009427\n",
            "iteration 11795 : loss : 0.027078, loss_ce: 0.010131\n",
            "iteration 11796 : loss : 0.028245, loss_ce: 0.011324\n",
            "iteration 11797 : loss : 0.027825, loss_ce: 0.008178\n",
            "iteration 11798 : loss : 0.022619, loss_ce: 0.006614\n",
            "iteration 11799 : loss : 0.018077, loss_ce: 0.005308\n",
            "iteration 11800 : loss : 0.028724, loss_ce: 0.014572\n",
            "iteration 11801 : loss : 0.029773, loss_ce: 0.005242\n",
            "iteration 11802 : loss : 0.027007, loss_ce: 0.010235\n",
            "iteration 11803 : loss : 0.023487, loss_ce: 0.007871\n",
            "iteration 11804 : loss : 0.055485, loss_ce: 0.008562\n",
            "iteration 11805 : loss : 0.027531, loss_ce: 0.007578\n",
            "iteration 11806 : loss : 0.023733, loss_ce: 0.008046\n",
            "iteration 11807 : loss : 0.028443, loss_ce: 0.008962\n",
            "iteration 11808 : loss : 0.026117, loss_ce: 0.010051\n",
            "iteration 11809 : loss : 0.026738, loss_ce: 0.008989\n",
            "iteration 11810 : loss : 0.024142, loss_ce: 0.007464\n",
            "iteration 11811 : loss : 0.283967, loss_ce: 0.003445\n",
            " 85%|███████████████████████▋    | 127/150 [4:37:46<50:00, 130.46s/it]iteration 11812 : loss : 0.027118, loss_ce: 0.008068\n",
            "iteration 11813 : loss : 0.024875, loss_ce: 0.007815\n",
            "iteration 11814 : loss : 0.025160, loss_ce: 0.006552\n",
            "iteration 11815 : loss : 0.024955, loss_ce: 0.007977\n",
            "iteration 11816 : loss : 0.020520, loss_ce: 0.007560\n",
            "iteration 11817 : loss : 0.023876, loss_ce: 0.010133\n",
            "iteration 11818 : loss : 0.030621, loss_ce: 0.008170\n",
            "iteration 11819 : loss : 0.020617, loss_ce: 0.006972\n",
            "iteration 11820 : loss : 0.029841, loss_ce: 0.007578\n",
            "iteration 11821 : loss : 0.026881, loss_ce: 0.012562\n",
            "iteration 11822 : loss : 0.023434, loss_ce: 0.010722\n",
            "iteration 11823 : loss : 0.024656, loss_ce: 0.008262\n",
            "iteration 11824 : loss : 0.026691, loss_ce: 0.006415\n",
            "iteration 11825 : loss : 0.025482, loss_ce: 0.005826\n",
            "iteration 11826 : loss : 0.021328, loss_ce: 0.007491\n",
            "iteration 11827 : loss : 0.023262, loss_ce: 0.005948\n",
            "iteration 11828 : loss : 0.022934, loss_ce: 0.009591\n",
            "iteration 11829 : loss : 0.022493, loss_ce: 0.007494\n",
            "iteration 11830 : loss : 0.024102, loss_ce: 0.011116\n",
            "iteration 11831 : loss : 0.022682, loss_ce: 0.005898\n",
            "iteration 11832 : loss : 0.025469, loss_ce: 0.007291\n",
            "iteration 11833 : loss : 0.026402, loss_ce: 0.010730\n",
            "iteration 11834 : loss : 0.023353, loss_ce: 0.007517\n",
            "iteration 11835 : loss : 0.024822, loss_ce: 0.007580\n",
            "iteration 11836 : loss : 0.073483, loss_ce: 0.003984\n",
            "iteration 11837 : loss : 0.023993, loss_ce: 0.008180\n",
            "iteration 11838 : loss : 0.021584, loss_ce: 0.005428\n",
            "iteration 11839 : loss : 0.027934, loss_ce: 0.010807\n",
            "iteration 11840 : loss : 0.025218, loss_ce: 0.011273\n",
            "iteration 11841 : loss : 0.029494, loss_ce: 0.009195\n",
            "iteration 11842 : loss : 0.022028, loss_ce: 0.009803\n",
            "iteration 11843 : loss : 0.026761, loss_ce: 0.006994\n",
            "iteration 11844 : loss : 0.028333, loss_ce: 0.011707\n",
            "iteration 11845 : loss : 0.025638, loss_ce: 0.010406\n",
            "iteration 11846 : loss : 0.025149, loss_ce: 0.007271\n",
            "iteration 11847 : loss : 0.079186, loss_ce: 0.009667\n",
            "iteration 11848 : loss : 0.026182, loss_ce: 0.011554\n",
            "iteration 11849 : loss : 0.023618, loss_ce: 0.007723\n",
            "iteration 11850 : loss : 0.017947, loss_ce: 0.004332\n",
            "iteration 11851 : loss : 0.023349, loss_ce: 0.010044\n",
            "iteration 11852 : loss : 0.077826, loss_ce: 0.004579\n",
            "iteration 11853 : loss : 0.021884, loss_ce: 0.009324\n",
            "iteration 11854 : loss : 0.023123, loss_ce: 0.011153\n",
            "iteration 11855 : loss : 0.021925, loss_ce: 0.005070\n",
            "iteration 11856 : loss : 0.023336, loss_ce: 0.007626\n",
            "iteration 11857 : loss : 0.023370, loss_ce: 0.009875\n",
            "iteration 11858 : loss : 0.024471, loss_ce: 0.011654\n",
            "iteration 11859 : loss : 0.026610, loss_ce: 0.007804\n",
            "iteration 11860 : loss : 0.023340, loss_ce: 0.005435\n",
            "iteration 11861 : loss : 0.021467, loss_ce: 0.007174\n",
            "iteration 11862 : loss : 0.024599, loss_ce: 0.010126\n",
            "iteration 11863 : loss : 0.024041, loss_ce: 0.009477\n",
            "iteration 11864 : loss : 0.023443, loss_ce: 0.012046\n",
            "iteration 11865 : loss : 0.022616, loss_ce: 0.006802\n",
            "iteration 11866 : loss : 0.023159, loss_ce: 0.004917\n",
            "iteration 11867 : loss : 0.025187, loss_ce: 0.005209\n",
            "iteration 11868 : loss : 0.072822, loss_ce: 0.006200\n",
            "iteration 11869 : loss : 0.074713, loss_ce: 0.003707\n",
            "iteration 11870 : loss : 0.021119, loss_ce: 0.005320\n",
            "iteration 11871 : loss : 0.026823, loss_ce: 0.011702\n",
            "iteration 11872 : loss : 0.072329, loss_ce: 0.007131\n",
            "iteration 11873 : loss : 0.024640, loss_ce: 0.011062\n",
            "iteration 11874 : loss : 0.023010, loss_ce: 0.006763\n",
            "iteration 11875 : loss : 0.077512, loss_ce: 0.008840\n",
            "iteration 11876 : loss : 0.073669, loss_ce: 0.005220\n",
            "iteration 11877 : loss : 0.020880, loss_ce: 0.007444\n",
            "iteration 11878 : loss : 0.026437, loss_ce: 0.010744\n",
            "iteration 11879 : loss : 0.029360, loss_ce: 0.008861\n",
            "iteration 11880 : loss : 0.032701, loss_ce: 0.011613\n",
            "iteration 11881 : loss : 0.026755, loss_ce: 0.005125\n",
            "iteration 11882 : loss : 0.029980, loss_ce: 0.013423\n",
            "iteration 11883 : loss : 0.029737, loss_ce: 0.006745\n",
            "iteration 11884 : loss : 0.036855, loss_ce: 0.007319\n",
            "iteration 11885 : loss : 0.022373, loss_ce: 0.007702\n",
            "iteration 11886 : loss : 0.023839, loss_ce: 0.008444\n",
            "iteration 11887 : loss : 0.026344, loss_ce: 0.009017\n",
            "iteration 11888 : loss : 0.022362, loss_ce: 0.005787\n",
            "iteration 11889 : loss : 0.025844, loss_ce: 0.006272\n",
            "iteration 11890 : loss : 0.028029, loss_ce: 0.011712\n",
            "iteration 11891 : loss : 0.075254, loss_ce: 0.005957\n",
            "iteration 11892 : loss : 0.025739, loss_ce: 0.009363\n",
            "iteration 11893 : loss : 0.028054, loss_ce: 0.007072\n",
            "iteration 11894 : loss : 0.026551, loss_ce: 0.011112\n",
            "iteration 11895 : loss : 0.025415, loss_ce: 0.007195\n",
            "iteration 11896 : loss : 0.025394, loss_ce: 0.010946\n",
            "iteration 11897 : loss : 0.024629, loss_ce: 0.003466\n",
            "iteration 11898 : loss : 0.024089, loss_ce: 0.008385\n",
            "iteration 11899 : loss : 0.022477, loss_ce: 0.009292\n",
            "iteration 11900 : loss : 0.020953, loss_ce: 0.009203\n",
            "iteration 11901 : loss : 0.024084, loss_ce: 0.010722\n",
            "iteration 11902 : loss : 0.023954, loss_ce: 0.009615\n",
            "iteration 11903 : loss : 0.021765, loss_ce: 0.007674\n",
            "iteration 11904 : loss : 0.136044, loss_ce: 0.009508\n",
            " 85%|███████████████████████▉    | 128/150 [4:39:58<48:00, 130.94s/it]iteration 11905 : loss : 0.025782, loss_ce: 0.013878\n",
            "iteration 11906 : loss : 0.030559, loss_ce: 0.012576\n",
            "iteration 11907 : loss : 0.024469, loss_ce: 0.009027\n",
            "iteration 11908 : loss : 0.025958, loss_ce: 0.010676\n",
            "iteration 11909 : loss : 0.030986, loss_ce: 0.009525\n",
            "iteration 11910 : loss : 0.023127, loss_ce: 0.010754\n",
            "iteration 11911 : loss : 0.027653, loss_ce: 0.009031\n",
            "iteration 11912 : loss : 0.022823, loss_ce: 0.009773\n",
            "iteration 11913 : loss : 0.022669, loss_ce: 0.009906\n",
            "iteration 11914 : loss : 0.021611, loss_ce: 0.008582\n",
            "iteration 11915 : loss : 0.021777, loss_ce: 0.007381\n",
            "iteration 11916 : loss : 0.024324, loss_ce: 0.012427\n",
            "iteration 11917 : loss : 0.024632, loss_ce: 0.006365\n",
            "iteration 11918 : loss : 0.072723, loss_ce: 0.006650\n",
            "iteration 11919 : loss : 0.020924, loss_ce: 0.008565\n",
            "iteration 11920 : loss : 0.027752, loss_ce: 0.008365\n",
            "iteration 11921 : loss : 0.020773, loss_ce: 0.006279\n",
            "iteration 11922 : loss : 0.021920, loss_ce: 0.008319\n",
            "iteration 11923 : loss : 0.024368, loss_ce: 0.005729\n",
            "iteration 11924 : loss : 0.026494, loss_ce: 0.006867\n",
            "iteration 11925 : loss : 0.022562, loss_ce: 0.006396\n",
            "iteration 11926 : loss : 0.019278, loss_ce: 0.004695\n",
            "iteration 11927 : loss : 0.022185, loss_ce: 0.008112\n",
            "iteration 11928 : loss : 0.072552, loss_ce: 0.004650\n",
            "iteration 11929 : loss : 0.032893, loss_ce: 0.009362\n",
            "iteration 11930 : loss : 0.017895, loss_ce: 0.005508\n",
            "iteration 11931 : loss : 0.026105, loss_ce: 0.010008\n",
            "iteration 11932 : loss : 0.027457, loss_ce: 0.005706\n",
            "iteration 11933 : loss : 0.072559, loss_ce: 0.006979\n",
            "iteration 11934 : loss : 0.025006, loss_ce: 0.011387\n",
            "iteration 11935 : loss : 0.025269, loss_ce: 0.011579\n",
            "iteration 11936 : loss : 0.027128, loss_ce: 0.011307\n",
            "iteration 11937 : loss : 0.026288, loss_ce: 0.003963\n",
            "iteration 11938 : loss : 0.023287, loss_ce: 0.007604\n",
            "iteration 11939 : loss : 0.024446, loss_ce: 0.011109\n",
            "iteration 11940 : loss : 0.019889, loss_ce: 0.007631\n",
            "iteration 11941 : loss : 0.022530, loss_ce: 0.013626\n",
            "iteration 11942 : loss : 0.022707, loss_ce: 0.006532\n",
            "iteration 11943 : loss : 0.122788, loss_ce: 0.006137\n",
            "iteration 11944 : loss : 0.023526, loss_ce: 0.009294\n",
            "iteration 11945 : loss : 0.028034, loss_ce: 0.011229\n",
            "iteration 11946 : loss : 0.038663, loss_ce: 0.009605\n",
            "iteration 11947 : loss : 0.023816, loss_ce: 0.009112\n",
            "iteration 11948 : loss : 0.022143, loss_ce: 0.005432\n",
            "iteration 11949 : loss : 0.024642, loss_ce: 0.007769\n",
            "iteration 11950 : loss : 0.072880, loss_ce: 0.004461\n",
            "iteration 11951 : loss : 0.074250, loss_ce: 0.003500\n",
            "iteration 11952 : loss : 0.074086, loss_ce: 0.007441\n",
            "iteration 11953 : loss : 0.023754, loss_ce: 0.008516\n",
            "iteration 11954 : loss : 0.024284, loss_ce: 0.007438\n",
            "iteration 11955 : loss : 0.029636, loss_ce: 0.005658\n",
            "iteration 11956 : loss : 0.024586, loss_ce: 0.006628\n",
            "iteration 11957 : loss : 0.029178, loss_ce: 0.010366\n",
            "iteration 11958 : loss : 0.029304, loss_ce: 0.010572\n",
            "iteration 11959 : loss : 0.072222, loss_ce: 0.006122\n",
            "iteration 11960 : loss : 0.023999, loss_ce: 0.006220\n",
            "iteration 11961 : loss : 0.023430, loss_ce: 0.010760\n",
            "iteration 11962 : loss : 0.022685, loss_ce: 0.007401\n",
            "iteration 11963 : loss : 0.024626, loss_ce: 0.007747\n",
            "iteration 11964 : loss : 0.024452, loss_ce: 0.008883\n",
            "iteration 11965 : loss : 0.023679, loss_ce: 0.009998\n",
            "iteration 11966 : loss : 0.032315, loss_ce: 0.008267\n",
            "iteration 11967 : loss : 0.030847, loss_ce: 0.007836\n",
            "iteration 11968 : loss : 0.025480, loss_ce: 0.012587\n",
            "iteration 11969 : loss : 0.024409, loss_ce: 0.012036\n",
            "iteration 11970 : loss : 0.022242, loss_ce: 0.008454\n",
            "iteration 11971 : loss : 0.026088, loss_ce: 0.007537\n",
            "iteration 11972 : loss : 0.025373, loss_ce: 0.008603\n",
            "iteration 11973 : loss : 0.030675, loss_ce: 0.006634\n",
            "iteration 11974 : loss : 0.075948, loss_ce: 0.005965\n",
            "iteration 11975 : loss : 0.022288, loss_ce: 0.006996\n",
            "iteration 11976 : loss : 0.026944, loss_ce: 0.007538\n",
            "iteration 11977 : loss : 0.028391, loss_ce: 0.007251\n",
            "iteration 11978 : loss : 0.021548, loss_ce: 0.008619\n",
            "iteration 11979 : loss : 0.028444, loss_ce: 0.009910\n",
            "iteration 11980 : loss : 0.021792, loss_ce: 0.006151\n",
            "iteration 11981 : loss : 0.026337, loss_ce: 0.008171\n",
            "iteration 11982 : loss : 0.028350, loss_ce: 0.008284\n",
            "iteration 11983 : loss : 0.030105, loss_ce: 0.011381\n",
            "iteration 11984 : loss : 0.019483, loss_ce: 0.007265\n",
            "iteration 11985 : loss : 0.021776, loss_ce: 0.006538\n",
            "iteration 11986 : loss : 0.024015, loss_ce: 0.011800\n",
            "iteration 11987 : loss : 0.027406, loss_ce: 0.009238\n",
            "iteration 11988 : loss : 0.025702, loss_ce: 0.010926\n",
            "iteration 11989 : loss : 0.029399, loss_ce: 0.006590\n",
            "iteration 11990 : loss : 0.021713, loss_ce: 0.006183\n",
            "iteration 11991 : loss : 0.025844, loss_ce: 0.008050\n",
            "iteration 11992 : loss : 0.020010, loss_ce: 0.004910\n",
            "iteration 11993 : loss : 0.025845, loss_ce: 0.002687\n",
            "iteration 11994 : loss : 0.070272, loss_ce: 0.005838\n",
            "iteration 11995 : loss : 0.022505, loss_ce: 0.005826\n",
            "iteration 11996 : loss : 0.019549, loss_ce: 0.005809\n",
            "iteration 11997 : loss : 0.034200, loss_ce: 0.023897\n",
            " 86%|████████████████████████    | 129/150 [4:42:08<45:43, 130.65s/it]iteration 11998 : loss : 0.023536, loss_ce: 0.011163\n",
            "iteration 11999 : loss : 0.023839, loss_ce: 0.006655\n",
            "iteration 12000 : loss : 0.024799, loss_ce: 0.009232\n",
            "iteration 12001 : loss : 0.023660, loss_ce: 0.007712\n",
            "iteration 12002 : loss : 0.026502, loss_ce: 0.007290\n",
            "iteration 12003 : loss : 0.039627, loss_ce: 0.006073\n",
            "iteration 12004 : loss : 0.023365, loss_ce: 0.008992\n",
            "iteration 12005 : loss : 0.022385, loss_ce: 0.007467\n",
            "iteration 12006 : loss : 0.024299, loss_ce: 0.007102\n",
            "iteration 12007 : loss : 0.021201, loss_ce: 0.004186\n",
            "iteration 12008 : loss : 0.018963, loss_ce: 0.007944\n",
            "iteration 12009 : loss : 0.028038, loss_ce: 0.010285\n",
            "iteration 12010 : loss : 0.026009, loss_ce: 0.010663\n",
            "iteration 12011 : loss : 0.022282, loss_ce: 0.009801\n",
            "iteration 12012 : loss : 0.027369, loss_ce: 0.006303\n",
            "iteration 12013 : loss : 0.074147, loss_ce: 0.006850\n",
            "iteration 12014 : loss : 0.023349, loss_ce: 0.007516\n",
            "iteration 12015 : loss : 0.020839, loss_ce: 0.008437\n",
            "iteration 12016 : loss : 0.028625, loss_ce: 0.007089\n",
            "iteration 12017 : loss : 0.025951, loss_ce: 0.010852\n",
            "iteration 12018 : loss : 0.024222, loss_ce: 0.008614\n",
            "iteration 12019 : loss : 0.028862, loss_ce: 0.010840\n",
            "iteration 12020 : loss : 0.020078, loss_ce: 0.006842\n",
            "iteration 12021 : loss : 0.028693, loss_ce: 0.011583\n",
            "iteration 12022 : loss : 0.023409, loss_ce: 0.004989\n",
            "iteration 12023 : loss : 0.022175, loss_ce: 0.009272\n",
            "iteration 12024 : loss : 0.023021, loss_ce: 0.005608\n",
            "iteration 12025 : loss : 0.025226, loss_ce: 0.010198\n",
            "iteration 12026 : loss : 0.024100, loss_ce: 0.008663\n",
            "iteration 12027 : loss : 0.024899, loss_ce: 0.008718\n",
            "iteration 12028 : loss : 0.026950, loss_ce: 0.008379\n",
            "iteration 12029 : loss : 0.023148, loss_ce: 0.006154\n",
            "iteration 12030 : loss : 0.027198, loss_ce: 0.004799\n",
            "iteration 12031 : loss : 0.028317, loss_ce: 0.011903\n",
            "iteration 12032 : loss : 0.027683, loss_ce: 0.007040\n",
            "iteration 12033 : loss : 0.030329, loss_ce: 0.007366\n",
            "iteration 12034 : loss : 0.023050, loss_ce: 0.008553\n",
            "iteration 12035 : loss : 0.024968, loss_ce: 0.011617\n",
            "iteration 12036 : loss : 0.028030, loss_ce: 0.009667\n",
            "iteration 12037 : loss : 0.024532, loss_ce: 0.009953\n",
            "iteration 12038 : loss : 0.025560, loss_ce: 0.006770\n",
            "iteration 12039 : loss : 0.024209, loss_ce: 0.007550\n",
            "iteration 12040 : loss : 0.023069, loss_ce: 0.004495\n",
            "iteration 12041 : loss : 0.025595, loss_ce: 0.015063\n",
            "iteration 12042 : loss : 0.028276, loss_ce: 0.010239\n",
            "iteration 12043 : loss : 0.025383, loss_ce: 0.009155\n",
            "iteration 12044 : loss : 0.021345, loss_ce: 0.007344\n",
            "iteration 12045 : loss : 0.022817, loss_ce: 0.005654\n",
            "iteration 12046 : loss : 0.026271, loss_ce: 0.007011\n",
            "iteration 12047 : loss : 0.021476, loss_ce: 0.006999\n",
            "iteration 12048 : loss : 0.020466, loss_ce: 0.007437\n",
            "iteration 12049 : loss : 0.028423, loss_ce: 0.007072\n",
            "iteration 12050 : loss : 0.076619, loss_ce: 0.010181\n",
            "iteration 12051 : loss : 0.024035, loss_ce: 0.011325\n",
            "iteration 12052 : loss : 0.017809, loss_ce: 0.005850\n",
            "iteration 12053 : loss : 0.025023, loss_ce: 0.007140\n",
            "iteration 12054 : loss : 0.025230, loss_ce: 0.009473\n",
            "iteration 12055 : loss : 0.024766, loss_ce: 0.007418\n",
            "iteration 12056 : loss : 0.019789, loss_ce: 0.009167\n",
            "iteration 12057 : loss : 0.021382, loss_ce: 0.009448\n",
            "iteration 12058 : loss : 0.020975, loss_ce: 0.006287\n",
            "iteration 12059 : loss : 0.021480, loss_ce: 0.008067\n",
            "iteration 12060 : loss : 0.020128, loss_ce: 0.004247\n",
            "iteration 12061 : loss : 0.126197, loss_ce: 0.007156\n",
            "iteration 12062 : loss : 0.023487, loss_ce: 0.009759\n",
            "iteration 12063 : loss : 0.024215, loss_ce: 0.010773\n",
            "iteration 12064 : loss : 0.032159, loss_ce: 0.007073\n",
            "iteration 12065 : loss : 0.078395, loss_ce: 0.003558\n",
            "iteration 12066 : loss : 0.028381, loss_ce: 0.008957\n",
            "iteration 12067 : loss : 0.026524, loss_ce: 0.009092\n",
            "iteration 12068 : loss : 0.021197, loss_ce: 0.007451\n",
            "iteration 12069 : loss : 0.023635, loss_ce: 0.009633\n",
            "iteration 12070 : loss : 0.047408, loss_ce: 0.008740\n",
            "iteration 12071 : loss : 0.021596, loss_ce: 0.006649\n",
            "iteration 12072 : loss : 0.020623, loss_ce: 0.006318\n",
            "iteration 12073 : loss : 0.021634, loss_ce: 0.006912\n",
            "iteration 12074 : loss : 0.029160, loss_ce: 0.007918\n",
            "iteration 12075 : loss : 0.023896, loss_ce: 0.006788\n",
            "iteration 12076 : loss : 0.024334, loss_ce: 0.004493\n",
            "iteration 12077 : loss : 0.025503, loss_ce: 0.007910\n",
            "iteration 12078 : loss : 0.026611, loss_ce: 0.008159\n",
            "iteration 12079 : loss : 0.023824, loss_ce: 0.009515\n",
            "iteration 12080 : loss : 0.053319, loss_ce: 0.008475\n",
            "iteration 12081 : loss : 0.026810, loss_ce: 0.010127\n",
            "iteration 12082 : loss : 0.025222, loss_ce: 0.009230\n",
            "iteration 12083 : loss : 0.024769, loss_ce: 0.007795\n",
            "iteration 12084 : loss : 0.027351, loss_ce: 0.011546\n",
            "iteration 12085 : loss : 0.022816, loss_ce: 0.008485\n",
            "iteration 12086 : loss : 0.021726, loss_ce: 0.007432\n",
            "iteration 12087 : loss : 0.026376, loss_ce: 0.010066\n",
            "iteration 12088 : loss : 0.024905, loss_ce: 0.010058\n",
            "iteration 12089 : loss : 0.022529, loss_ce: 0.007982\n",
            "iteration 12090 : loss : 0.332481, loss_ce: 0.000697\n",
            " 87%|████████████████████████▎   | 130/150 [4:44:19<43:31, 130.58s/it]iteration 12091 : loss : 0.075042, loss_ce: 0.006329\n",
            "iteration 12092 : loss : 0.020203, loss_ce: 0.009071\n",
            "iteration 12093 : loss : 0.020191, loss_ce: 0.006771\n",
            "iteration 12094 : loss : 0.026591, loss_ce: 0.010399\n",
            "iteration 12095 : loss : 0.021973, loss_ce: 0.009618\n",
            "iteration 12096 : loss : 0.024472, loss_ce: 0.007817\n",
            "iteration 12097 : loss : 0.024567, loss_ce: 0.007459\n",
            "iteration 12098 : loss : 0.018484, loss_ce: 0.005157\n",
            "iteration 12099 : loss : 0.025483, loss_ce: 0.009879\n",
            "iteration 12100 : loss : 0.029541, loss_ce: 0.011761\n",
            "iteration 12101 : loss : 0.025471, loss_ce: 0.006533\n",
            "iteration 12102 : loss : 0.028316, loss_ce: 0.005687\n",
            "iteration 12103 : loss : 0.026225, loss_ce: 0.009592\n",
            "iteration 12104 : loss : 0.025498, loss_ce: 0.009289\n",
            "iteration 12105 : loss : 0.077922, loss_ce: 0.009739\n",
            "iteration 12106 : loss : 0.023187, loss_ce: 0.007406\n",
            "iteration 12107 : loss : 0.073878, loss_ce: 0.005374\n",
            "iteration 12108 : loss : 0.029668, loss_ce: 0.007552\n",
            "iteration 12109 : loss : 0.026844, loss_ce: 0.009524\n",
            "iteration 12110 : loss : 0.022385, loss_ce: 0.006828\n",
            "iteration 12111 : loss : 0.023549, loss_ce: 0.010527\n",
            "iteration 12112 : loss : 0.021035, loss_ce: 0.004920\n",
            "iteration 12113 : loss : 0.027758, loss_ce: 0.007781\n",
            "iteration 12114 : loss : 0.024868, loss_ce: 0.006391\n",
            "iteration 12115 : loss : 0.023629, loss_ce: 0.006913\n",
            "iteration 12116 : loss : 0.025281, loss_ce: 0.010458\n",
            "iteration 12117 : loss : 0.026611, loss_ce: 0.008260\n",
            "iteration 12118 : loss : 0.029588, loss_ce: 0.010655\n",
            "iteration 12119 : loss : 0.024572, loss_ce: 0.007005\n",
            "iteration 12120 : loss : 0.030294, loss_ce: 0.007443\n",
            "iteration 12121 : loss : 0.027311, loss_ce: 0.008585\n",
            "iteration 12122 : loss : 0.021282, loss_ce: 0.009813\n",
            "iteration 12123 : loss : 0.017754, loss_ce: 0.005804\n",
            "iteration 12124 : loss : 0.024632, loss_ce: 0.009001\n",
            "iteration 12125 : loss : 0.025489, loss_ce: 0.006952\n",
            "iteration 12126 : loss : 0.023624, loss_ce: 0.010523\n",
            "iteration 12127 : loss : 0.021623, loss_ce: 0.005512\n",
            "iteration 12128 : loss : 0.020843, loss_ce: 0.009954\n",
            "iteration 12129 : loss : 0.027774, loss_ce: 0.012187\n",
            "iteration 12130 : loss : 0.024199, loss_ce: 0.012665\n",
            "iteration 12131 : loss : 0.022281, loss_ce: 0.007230\n",
            "iteration 12132 : loss : 0.028671, loss_ce: 0.004957\n",
            "iteration 12133 : loss : 0.021760, loss_ce: 0.006611\n",
            "iteration 12134 : loss : 0.026042, loss_ce: 0.009120\n",
            "iteration 12135 : loss : 0.022528, loss_ce: 0.009991\n",
            "iteration 12136 : loss : 0.026611, loss_ce: 0.005765\n",
            "iteration 12137 : loss : 0.022807, loss_ce: 0.008455\n",
            "iteration 12138 : loss : 0.021147, loss_ce: 0.007401\n",
            "iteration 12139 : loss : 0.027208, loss_ce: 0.008482\n",
            "iteration 12140 : loss : 0.027529, loss_ce: 0.008904\n",
            "iteration 12141 : loss : 0.025444, loss_ce: 0.008238\n",
            "iteration 12142 : loss : 0.023786, loss_ce: 0.010480\n",
            "iteration 12143 : loss : 0.024623, loss_ce: 0.010684\n",
            "iteration 12144 : loss : 0.020910, loss_ce: 0.004977\n",
            "iteration 12145 : loss : 0.021560, loss_ce: 0.005512\n",
            "iteration 12146 : loss : 0.030956, loss_ce: 0.009084\n",
            "iteration 12147 : loss : 0.024655, loss_ce: 0.006905\n",
            "iteration 12148 : loss : 0.021639, loss_ce: 0.006657\n",
            "iteration 12149 : loss : 0.028635, loss_ce: 0.007835\n",
            "iteration 12150 : loss : 0.023901, loss_ce: 0.006133\n",
            "iteration 12151 : loss : 0.022317, loss_ce: 0.010413\n",
            "iteration 12152 : loss : 0.021413, loss_ce: 0.007263\n",
            "iteration 12153 : loss : 0.021914, loss_ce: 0.007708\n",
            "iteration 12154 : loss : 0.019432, loss_ce: 0.007211\n",
            "iteration 12155 : loss : 0.035636, loss_ce: 0.007980\n",
            "iteration 12156 : loss : 0.021088, loss_ce: 0.008750\n",
            "iteration 12157 : loss : 0.024019, loss_ce: 0.007155\n",
            "iteration 12158 : loss : 0.031464, loss_ce: 0.010063\n",
            "iteration 12159 : loss : 0.023884, loss_ce: 0.007388\n",
            "iteration 12160 : loss : 0.075938, loss_ce: 0.002554\n",
            "iteration 12161 : loss : 0.028948, loss_ce: 0.008700\n",
            "iteration 12162 : loss : 0.019650, loss_ce: 0.007001\n",
            "iteration 12163 : loss : 0.026660, loss_ce: 0.006783\n",
            "iteration 12164 : loss : 0.019323, loss_ce: 0.006724\n",
            "iteration 12165 : loss : 0.038440, loss_ce: 0.008855\n",
            "iteration 12166 : loss : 0.028150, loss_ce: 0.010559\n",
            "iteration 12167 : loss : 0.023005, loss_ce: 0.007763\n",
            "iteration 12168 : loss : 0.074805, loss_ce: 0.004711\n",
            "iteration 12169 : loss : 0.023655, loss_ce: 0.007637\n",
            "iteration 12170 : loss : 0.023279, loss_ce: 0.010852\n",
            "iteration 12171 : loss : 0.023424, loss_ce: 0.007368\n",
            "iteration 12172 : loss : 0.022793, loss_ce: 0.009657\n",
            "iteration 12173 : loss : 0.025516, loss_ce: 0.011746\n",
            "iteration 12174 : loss : 0.034209, loss_ce: 0.008578\n",
            "iteration 12175 : loss : 0.025810, loss_ce: 0.008945\n",
            "iteration 12176 : loss : 0.075279, loss_ce: 0.006042\n",
            "iteration 12177 : loss : 0.022600, loss_ce: 0.007174\n",
            "iteration 12178 : loss : 0.027276, loss_ce: 0.009194\n",
            "iteration 12179 : loss : 0.032398, loss_ce: 0.008315\n",
            "iteration 12180 : loss : 0.024607, loss_ce: 0.004771\n",
            "iteration 12181 : loss : 0.023232, loss_ce: 0.009670\n",
            "iteration 12182 : loss : 0.025478, loss_ce: 0.011236\n",
            "iteration 12183 : loss : 0.033476, loss_ce: 0.025421\n",
            " 87%|████████████████████████▍   | 131/150 [4:46:30<41:26, 130.85s/it]iteration 12184 : loss : 0.023926, loss_ce: 0.007132\n",
            "iteration 12185 : loss : 0.078699, loss_ce: 0.003489\n",
            "iteration 12186 : loss : 0.024328, loss_ce: 0.009485\n",
            "iteration 12187 : loss : 0.021750, loss_ce: 0.011029\n",
            "iteration 12188 : loss : 0.023878, loss_ce: 0.008864\n",
            "iteration 12189 : loss : 0.026718, loss_ce: 0.007569\n",
            "iteration 12190 : loss : 0.025973, loss_ce: 0.008318\n",
            "iteration 12191 : loss : 0.031603, loss_ce: 0.015460\n",
            "iteration 12192 : loss : 0.019822, loss_ce: 0.007578\n",
            "iteration 12193 : loss : 0.026772, loss_ce: 0.008340\n",
            "iteration 12194 : loss : 0.026852, loss_ce: 0.007701\n",
            "iteration 12195 : loss : 0.023056, loss_ce: 0.005192\n",
            "iteration 12196 : loss : 0.029915, loss_ce: 0.011306\n",
            "iteration 12197 : loss : 0.023943, loss_ce: 0.006427\n",
            "iteration 12198 : loss : 0.025312, loss_ce: 0.011239\n",
            "iteration 12199 : loss : 0.024290, loss_ce: 0.008347\n",
            "iteration 12200 : loss : 0.018552, loss_ce: 0.007754\n",
            "iteration 12201 : loss : 0.021822, loss_ce: 0.008507\n",
            "iteration 12202 : loss : 0.021081, loss_ce: 0.009852\n",
            "iteration 12203 : loss : 0.024502, loss_ce: 0.010098\n",
            "iteration 12204 : loss : 0.021390, loss_ce: 0.005436\n",
            "iteration 12205 : loss : 0.024366, loss_ce: 0.007848\n",
            "iteration 12206 : loss : 0.020467, loss_ce: 0.010045\n",
            "iteration 12207 : loss : 0.021329, loss_ce: 0.010034\n",
            "iteration 12208 : loss : 0.020908, loss_ce: 0.007786\n",
            "iteration 12209 : loss : 0.020905, loss_ce: 0.008474\n",
            "iteration 12210 : loss : 0.030610, loss_ce: 0.008948\n",
            "iteration 12211 : loss : 0.030204, loss_ce: 0.008512\n",
            "iteration 12212 : loss : 0.027060, loss_ce: 0.005167\n",
            "iteration 12213 : loss : 0.026841, loss_ce: 0.011885\n",
            "iteration 12214 : loss : 0.022317, loss_ce: 0.007533\n",
            "iteration 12215 : loss : 0.025186, loss_ce: 0.005133\n",
            "iteration 12216 : loss : 0.074977, loss_ce: 0.004615\n",
            "iteration 12217 : loss : 0.025828, loss_ce: 0.009162\n",
            "iteration 12218 : loss : 0.023683, loss_ce: 0.008536\n",
            "iteration 12219 : loss : 0.026421, loss_ce: 0.009768\n",
            "iteration 12220 : loss : 0.027518, loss_ce: 0.011202\n",
            "iteration 12221 : loss : 0.023088, loss_ce: 0.007215\n",
            "iteration 12222 : loss : 0.027486, loss_ce: 0.009416\n",
            "iteration 12223 : loss : 0.021347, loss_ce: 0.006284\n",
            "iteration 12224 : loss : 0.022209, loss_ce: 0.006569\n",
            "iteration 12225 : loss : 0.029786, loss_ce: 0.005090\n",
            "iteration 12226 : loss : 0.021360, loss_ce: 0.006066\n",
            "iteration 12227 : loss : 0.023894, loss_ce: 0.009155\n",
            "iteration 12228 : loss : 0.125693, loss_ce: 0.006064\n",
            "iteration 12229 : loss : 0.019825, loss_ce: 0.007161\n",
            "iteration 12230 : loss : 0.027725, loss_ce: 0.011430\n",
            "iteration 12231 : loss : 0.024432, loss_ce: 0.006041\n",
            "iteration 12232 : loss : 0.074494, loss_ce: 0.007657\n",
            "iteration 12233 : loss : 0.021498, loss_ce: 0.010650\n",
            "iteration 12234 : loss : 0.020425, loss_ce: 0.006145\n",
            "iteration 12235 : loss : 0.021278, loss_ce: 0.008817\n",
            "iteration 12236 : loss : 0.024557, loss_ce: 0.010705\n",
            "iteration 12237 : loss : 0.024035, loss_ce: 0.008643\n",
            "iteration 12238 : loss : 0.023051, loss_ce: 0.012560\n",
            "iteration 12239 : loss : 0.028383, loss_ce: 0.013924\n",
            "iteration 12240 : loss : 0.022774, loss_ce: 0.010702\n",
            "iteration 12241 : loss : 0.022427, loss_ce: 0.008686\n",
            "iteration 12242 : loss : 0.024234, loss_ce: 0.008792\n",
            "iteration 12243 : loss : 0.023482, loss_ce: 0.008353\n",
            "iteration 12244 : loss : 0.025106, loss_ce: 0.006176\n",
            "iteration 12245 : loss : 0.023003, loss_ce: 0.007995\n",
            "iteration 12246 : loss : 0.019591, loss_ce: 0.007858\n",
            "iteration 12247 : loss : 0.029736, loss_ce: 0.010079\n",
            "iteration 12248 : loss : 0.024603, loss_ce: 0.010707\n",
            "iteration 12249 : loss : 0.023212, loss_ce: 0.007761\n",
            "iteration 12250 : loss : 0.027233, loss_ce: 0.010470\n",
            "iteration 12251 : loss : 0.022258, loss_ce: 0.009264\n",
            "iteration 12252 : loss : 0.026372, loss_ce: 0.012093\n",
            "iteration 12253 : loss : 0.073768, loss_ce: 0.004565\n",
            "iteration 12254 : loss : 0.024399, loss_ce: 0.007291\n",
            "iteration 12255 : loss : 0.022983, loss_ce: 0.004636\n",
            "iteration 12256 : loss : 0.024150, loss_ce: 0.006655\n",
            "iteration 12257 : loss : 0.028749, loss_ce: 0.007787\n",
            "iteration 12258 : loss : 0.027675, loss_ce: 0.007327\n",
            "iteration 12259 : loss : 0.023464, loss_ce: 0.009093\n",
            "iteration 12260 : loss : 0.021439, loss_ce: 0.005290\n",
            "iteration 12261 : loss : 0.023247, loss_ce: 0.006225\n",
            "iteration 12262 : loss : 0.024549, loss_ce: 0.005991\n",
            "iteration 12263 : loss : 0.024620, loss_ce: 0.010394\n",
            "iteration 12264 : loss : 0.022259, loss_ce: 0.008427\n",
            "iteration 12265 : loss : 0.023752, loss_ce: 0.006893\n",
            "iteration 12266 : loss : 0.028516, loss_ce: 0.008087\n",
            "iteration 12267 : loss : 0.023492, loss_ce: 0.003564\n",
            "iteration 12268 : loss : 0.089398, loss_ce: 0.007751\n",
            "iteration 12269 : loss : 0.027544, loss_ce: 0.008406\n",
            "iteration 12270 : loss : 0.023549, loss_ce: 0.007113\n",
            "iteration 12271 : loss : 0.074787, loss_ce: 0.004361\n",
            "iteration 12272 : loss : 0.021657, loss_ce: 0.007909\n",
            "iteration 12273 : loss : 0.026976, loss_ce: 0.006959\n",
            "iteration 12274 : loss : 0.024004, loss_ce: 0.006377\n",
            "iteration 12275 : loss : 0.021783, loss_ce: 0.007234\n",
            "iteration 12276 : loss : 0.441898, loss_ce: 0.000413\n",
            " 88%|████████████████████████▋   | 132/150 [4:48:40<39:09, 130.53s/it]iteration 12277 : loss : 0.023272, loss_ce: 0.005564\n",
            "iteration 12278 : loss : 0.075862, loss_ce: 0.005596\n",
            "iteration 12279 : loss : 0.049791, loss_ce: 0.004564\n",
            "iteration 12280 : loss : 0.020958, loss_ce: 0.007398\n",
            "iteration 12281 : loss : 0.022922, loss_ce: 0.008843\n",
            "iteration 12282 : loss : 0.023292, loss_ce: 0.005341\n",
            "iteration 12283 : loss : 0.026473, loss_ce: 0.010576\n",
            "iteration 12284 : loss : 0.023234, loss_ce: 0.011047\n",
            "iteration 12285 : loss : 0.025543, loss_ce: 0.011677\n",
            "iteration 12286 : loss : 0.022361, loss_ce: 0.006803\n",
            "iteration 12287 : loss : 0.020865, loss_ce: 0.005469\n",
            "iteration 12288 : loss : 0.020738, loss_ce: 0.007920\n",
            "iteration 12289 : loss : 0.023490, loss_ce: 0.008246\n",
            "iteration 12290 : loss : 0.023658, loss_ce: 0.006491\n",
            "iteration 12291 : loss : 0.023022, loss_ce: 0.007268\n",
            "iteration 12292 : loss : 0.024921, loss_ce: 0.010387\n",
            "iteration 12293 : loss : 0.020843, loss_ce: 0.007087\n",
            "iteration 12294 : loss : 0.025648, loss_ce: 0.009675\n",
            "iteration 12295 : loss : 0.023304, loss_ce: 0.009812\n",
            "iteration 12296 : loss : 0.026290, loss_ce: 0.007306\n",
            "iteration 12297 : loss : 0.024127, loss_ce: 0.007453\n",
            "iteration 12298 : loss : 0.024638, loss_ce: 0.007545\n",
            "iteration 12299 : loss : 0.032444, loss_ce: 0.010470\n",
            "iteration 12300 : loss : 0.037000, loss_ce: 0.004833\n",
            "iteration 12301 : loss : 0.075395, loss_ce: 0.005681\n",
            "iteration 12302 : loss : 0.035027, loss_ce: 0.006592\n",
            "iteration 12303 : loss : 0.022858, loss_ce: 0.004890\n",
            "iteration 12304 : loss : 0.022635, loss_ce: 0.009951\n",
            "iteration 12305 : loss : 0.024355, loss_ce: 0.011136\n",
            "iteration 12306 : loss : 0.025766, loss_ce: 0.006501\n",
            "iteration 12307 : loss : 0.027484, loss_ce: 0.012223\n",
            "iteration 12308 : loss : 0.024439, loss_ce: 0.004796\n",
            "iteration 12309 : loss : 0.022458, loss_ce: 0.006903\n",
            "iteration 12310 : loss : 0.019737, loss_ce: 0.006620\n",
            "iteration 12311 : loss : 0.026066, loss_ce: 0.008376\n",
            "iteration 12312 : loss : 0.025182, loss_ce: 0.010953\n",
            "iteration 12313 : loss : 0.029792, loss_ce: 0.010271\n",
            "iteration 12314 : loss : 0.015238, loss_ce: 0.006932\n",
            "iteration 12315 : loss : 0.023442, loss_ce: 0.008494\n",
            "iteration 12316 : loss : 0.021244, loss_ce: 0.007102\n",
            "iteration 12317 : loss : 0.023828, loss_ce: 0.007520\n",
            "iteration 12318 : loss : 0.022311, loss_ce: 0.006779\n",
            "iteration 12319 : loss : 0.023417, loss_ce: 0.010938\n",
            "iteration 12320 : loss : 0.022149, loss_ce: 0.008231\n",
            "iteration 12321 : loss : 0.020475, loss_ce: 0.007860\n",
            "iteration 12322 : loss : 0.023100, loss_ce: 0.010435\n",
            "iteration 12323 : loss : 0.077680, loss_ce: 0.008006\n",
            "iteration 12324 : loss : 0.022217, loss_ce: 0.003983\n",
            "iteration 12325 : loss : 0.026196, loss_ce: 0.007832\n",
            "iteration 12326 : loss : 0.024309, loss_ce: 0.008013\n",
            "iteration 12327 : loss : 0.030899, loss_ce: 0.006714\n",
            "iteration 12328 : loss : 0.019839, loss_ce: 0.006113\n",
            "iteration 12329 : loss : 0.027672, loss_ce: 0.007126\n",
            "iteration 12330 : loss : 0.029944, loss_ce: 0.005742\n",
            "iteration 12331 : loss : 0.021044, loss_ce: 0.010379\n",
            "iteration 12332 : loss : 0.020502, loss_ce: 0.007418\n",
            "iteration 12333 : loss : 0.074067, loss_ce: 0.007100\n",
            "iteration 12334 : loss : 0.023606, loss_ce: 0.010464\n",
            "iteration 12335 : loss : 0.031282, loss_ce: 0.004708\n",
            "iteration 12336 : loss : 0.022227, loss_ce: 0.010000\n",
            "iteration 12337 : loss : 0.020462, loss_ce: 0.008768\n",
            "iteration 12338 : loss : 0.019982, loss_ce: 0.005044\n",
            "iteration 12339 : loss : 0.032493, loss_ce: 0.009138\n",
            "iteration 12340 : loss : 0.025995, loss_ce: 0.010609\n",
            "iteration 12341 : loss : 0.026483, loss_ce: 0.006645\n",
            "iteration 12342 : loss : 0.026235, loss_ce: 0.005390\n",
            "iteration 12343 : loss : 0.029302, loss_ce: 0.010616\n",
            "iteration 12344 : loss : 0.025455, loss_ce: 0.009292\n",
            "iteration 12345 : loss : 0.023865, loss_ce: 0.008094\n",
            "iteration 12346 : loss : 0.024051, loss_ce: 0.010413\n",
            "iteration 12347 : loss : 0.030204, loss_ce: 0.010485\n",
            "iteration 12348 : loss : 0.025154, loss_ce: 0.010463\n",
            "iteration 12349 : loss : 0.024609, loss_ce: 0.009962\n",
            "iteration 12350 : loss : 0.024335, loss_ce: 0.010644\n",
            "iteration 12351 : loss : 0.023682, loss_ce: 0.004438\n",
            "iteration 12352 : loss : 0.023371, loss_ce: 0.006499\n",
            "iteration 12353 : loss : 0.020842, loss_ce: 0.004868\n",
            "iteration 12354 : loss : 0.030941, loss_ce: 0.010136\n",
            "iteration 12355 : loss : 0.023396, loss_ce: 0.009326\n",
            "iteration 12356 : loss : 0.024049, loss_ce: 0.009431\n",
            "iteration 12357 : loss : 0.077552, loss_ce: 0.009829\n",
            "iteration 12358 : loss : 0.021281, loss_ce: 0.008094\n",
            "iteration 12359 : loss : 0.026311, loss_ce: 0.006786\n",
            "iteration 12360 : loss : 0.023848, loss_ce: 0.009292\n",
            "iteration 12361 : loss : 0.026157, loss_ce: 0.008861\n",
            "iteration 12362 : loss : 0.021832, loss_ce: 0.009430\n",
            "iteration 12363 : loss : 0.023304, loss_ce: 0.009751\n",
            "iteration 12364 : loss : 0.021834, loss_ce: 0.007652\n",
            "iteration 12365 : loss : 0.023911, loss_ce: 0.007456\n",
            "iteration 12366 : loss : 0.025988, loss_ce: 0.010994\n",
            "iteration 12367 : loss : 0.022489, loss_ce: 0.007925\n",
            "iteration 12368 : loss : 0.021658, loss_ce: 0.006068\n",
            "iteration 12369 : loss : 0.030191, loss_ce: 0.019806\n",
            " 89%|████████████████████████▊   | 133/150 [4:50:51<36:59, 130.55s/it]iteration 12370 : loss : 0.021185, loss_ce: 0.007426\n",
            "iteration 12371 : loss : 0.023068, loss_ce: 0.005546\n",
            "iteration 12372 : loss : 0.025179, loss_ce: 0.013417\n",
            "iteration 12373 : loss : 0.024172, loss_ce: 0.008529\n",
            "iteration 12374 : loss : 0.023254, loss_ce: 0.006320\n",
            "iteration 12375 : loss : 0.021204, loss_ce: 0.007315\n",
            "iteration 12376 : loss : 0.023402, loss_ce: 0.006722\n",
            "iteration 12377 : loss : 0.023112, loss_ce: 0.009215\n",
            "iteration 12378 : loss : 0.021335, loss_ce: 0.008529\n",
            "iteration 12379 : loss : 0.020783, loss_ce: 0.006482\n",
            "iteration 12380 : loss : 0.029442, loss_ce: 0.007878\n",
            "iteration 12381 : loss : 0.022384, loss_ce: 0.008922\n",
            "iteration 12382 : loss : 0.024317, loss_ce: 0.010150\n",
            "iteration 12383 : loss : 0.025056, loss_ce: 0.007071\n",
            "iteration 12384 : loss : 0.019638, loss_ce: 0.007760\n",
            "iteration 12385 : loss : 0.021052, loss_ce: 0.006272\n",
            "iteration 12386 : loss : 0.023233, loss_ce: 0.005044\n",
            "iteration 12387 : loss : 0.020712, loss_ce: 0.009987\n",
            "iteration 12388 : loss : 0.024530, loss_ce: 0.007944\n",
            "iteration 12389 : loss : 0.070882, loss_ce: 0.006991\n",
            "iteration 12390 : loss : 0.022555, loss_ce: 0.008840\n",
            "iteration 12391 : loss : 0.021695, loss_ce: 0.008557\n",
            "iteration 12392 : loss : 0.080182, loss_ce: 0.005986\n",
            "iteration 12393 : loss : 0.023583, loss_ce: 0.009934\n",
            "iteration 12394 : loss : 0.022180, loss_ce: 0.005449\n",
            "iteration 12395 : loss : 0.028605, loss_ce: 0.012529\n",
            "iteration 12396 : loss : 0.020830, loss_ce: 0.006594\n",
            "iteration 12397 : loss : 0.028716, loss_ce: 0.004424\n",
            "iteration 12398 : loss : 0.078276, loss_ce: 0.006045\n",
            "iteration 12399 : loss : 0.023538, loss_ce: 0.010712\n",
            "iteration 12400 : loss : 0.022533, loss_ce: 0.009206\n",
            "iteration 12401 : loss : 0.075216, loss_ce: 0.007744\n",
            "iteration 12402 : loss : 0.022681, loss_ce: 0.005386\n",
            "iteration 12403 : loss : 0.024913, loss_ce: 0.010713\n",
            "iteration 12404 : loss : 0.018673, loss_ce: 0.003201\n",
            "iteration 12405 : loss : 0.025188, loss_ce: 0.012519\n",
            "iteration 12406 : loss : 0.025468, loss_ce: 0.006776\n",
            "iteration 12407 : loss : 0.024577, loss_ce: 0.006454\n",
            "iteration 12408 : loss : 0.039451, loss_ce: 0.007912\n",
            "iteration 12409 : loss : 0.022228, loss_ce: 0.007462\n",
            "iteration 12410 : loss : 0.024085, loss_ce: 0.011288\n",
            "iteration 12411 : loss : 0.025445, loss_ce: 0.011169\n",
            "iteration 12412 : loss : 0.025525, loss_ce: 0.007198\n",
            "iteration 12413 : loss : 0.021645, loss_ce: 0.005514\n",
            "iteration 12414 : loss : 0.021642, loss_ce: 0.007941\n",
            "iteration 12415 : loss : 0.034411, loss_ce: 0.004746\n",
            "iteration 12416 : loss : 0.023455, loss_ce: 0.008591\n",
            "iteration 12417 : loss : 0.046298, loss_ce: 0.007411\n",
            "iteration 12418 : loss : 0.027751, loss_ce: 0.006208\n",
            "iteration 12419 : loss : 0.023276, loss_ce: 0.007332\n",
            "iteration 12420 : loss : 0.024366, loss_ce: 0.007946\n",
            "iteration 12421 : loss : 0.030289, loss_ce: 0.011501\n",
            "iteration 12422 : loss : 0.024978, loss_ce: 0.008428\n",
            "iteration 12423 : loss : 0.021753, loss_ce: 0.009561\n",
            "iteration 12424 : loss : 0.028530, loss_ce: 0.006602\n",
            "iteration 12425 : loss : 0.021648, loss_ce: 0.005914\n",
            "iteration 12426 : loss : 0.026161, loss_ce: 0.007471\n",
            "iteration 12427 : loss : 0.032731, loss_ce: 0.011708\n",
            "iteration 12428 : loss : 0.024754, loss_ce: 0.010092\n",
            "iteration 12429 : loss : 0.027062, loss_ce: 0.012262\n",
            "iteration 12430 : loss : 0.025758, loss_ce: 0.009087\n",
            "iteration 12431 : loss : 0.023222, loss_ce: 0.010566\n",
            "iteration 12432 : loss : 0.023405, loss_ce: 0.006694\n",
            "iteration 12433 : loss : 0.073625, loss_ce: 0.008119\n",
            "iteration 12434 : loss : 0.034412, loss_ce: 0.012262\n",
            "iteration 12435 : loss : 0.025776, loss_ce: 0.008679\n",
            "iteration 12436 : loss : 0.023695, loss_ce: 0.006043\n",
            "iteration 12437 : loss : 0.032350, loss_ce: 0.010614\n",
            "iteration 12438 : loss : 0.021643, loss_ce: 0.006690\n",
            "iteration 12439 : loss : 0.028124, loss_ce: 0.011259\n",
            "iteration 12440 : loss : 0.022661, loss_ce: 0.007844\n",
            "iteration 12441 : loss : 0.076152, loss_ce: 0.005849\n",
            "iteration 12442 : loss : 0.024850, loss_ce: 0.009074\n",
            "iteration 12443 : loss : 0.030116, loss_ce: 0.006101\n",
            "iteration 12444 : loss : 0.023117, loss_ce: 0.007318\n",
            "iteration 12445 : loss : 0.025638, loss_ce: 0.005394\n",
            "iteration 12446 : loss : 0.023500, loss_ce: 0.005784\n",
            "iteration 12447 : loss : 0.032476, loss_ce: 0.008651\n",
            "iteration 12448 : loss : 0.021523, loss_ce: 0.006979\n",
            "iteration 12449 : loss : 0.039142, loss_ce: 0.007692\n",
            "iteration 12450 : loss : 0.024245, loss_ce: 0.005005\n",
            "iteration 12451 : loss : 0.023838, loss_ce: 0.009736\n",
            "iteration 12452 : loss : 0.024113, loss_ce: 0.010368\n",
            "iteration 12453 : loss : 0.024899, loss_ce: 0.010651\n",
            "iteration 12454 : loss : 0.022214, loss_ce: 0.008634\n",
            "iteration 12455 : loss : 0.024734, loss_ce: 0.009532\n",
            "iteration 12456 : loss : 0.021616, loss_ce: 0.006420\n",
            "iteration 12457 : loss : 0.022887, loss_ce: 0.010314\n",
            "iteration 12458 : loss : 0.023990, loss_ce: 0.008574\n",
            "iteration 12459 : loss : 0.030083, loss_ce: 0.007577\n",
            "iteration 12460 : loss : 0.026754, loss_ce: 0.011015\n",
            "iteration 12461 : loss : 0.023314, loss_ce: 0.010751\n",
            "iteration 12462 : loss : 0.088310, loss_ce: 0.013271\n",
            " 89%|█████████████████████████   | 134/150 [4:53:02<34:54, 130.92s/it]iteration 12463 : loss : 0.018222, loss_ce: 0.004714\n",
            "iteration 12464 : loss : 0.024396, loss_ce: 0.009159\n",
            "iteration 12465 : loss : 0.025402, loss_ce: 0.009116\n",
            "iteration 12466 : loss : 0.021989, loss_ce: 0.007200\n",
            "iteration 12467 : loss : 0.026042, loss_ce: 0.010510\n",
            "iteration 12468 : loss : 0.029674, loss_ce: 0.008463\n",
            "iteration 12469 : loss : 0.027484, loss_ce: 0.006786\n",
            "iteration 12470 : loss : 0.022980, loss_ce: 0.008770\n",
            "iteration 12471 : loss : 0.022219, loss_ce: 0.006574\n",
            "iteration 12472 : loss : 0.022474, loss_ce: 0.008166\n",
            "iteration 12473 : loss : 0.031871, loss_ce: 0.007002\n",
            "iteration 12474 : loss : 0.023004, loss_ce: 0.009242\n",
            "iteration 12475 : loss : 0.025943, loss_ce: 0.006763\n",
            "iteration 12476 : loss : 0.023365, loss_ce: 0.008993\n",
            "iteration 12477 : loss : 0.019270, loss_ce: 0.004241\n",
            "iteration 12478 : loss : 0.023602, loss_ce: 0.009026\n",
            "iteration 12479 : loss : 0.019452, loss_ce: 0.005182\n",
            "iteration 12480 : loss : 0.025072, loss_ce: 0.010004\n",
            "iteration 12481 : loss : 0.024699, loss_ce: 0.008605\n",
            "iteration 12482 : loss : 0.023173, loss_ce: 0.007949\n",
            "iteration 12483 : loss : 0.019972, loss_ce: 0.005092\n",
            "iteration 12484 : loss : 0.021500, loss_ce: 0.006701\n",
            "iteration 12485 : loss : 0.025091, loss_ce: 0.008396\n",
            "iteration 12486 : loss : 0.025590, loss_ce: 0.016736\n",
            "iteration 12487 : loss : 0.026011, loss_ce: 0.010027\n",
            "iteration 12488 : loss : 0.024934, loss_ce: 0.012911\n",
            "iteration 12489 : loss : 0.023961, loss_ce: 0.006384\n",
            "iteration 12490 : loss : 0.022409, loss_ce: 0.006434\n",
            "iteration 12491 : loss : 0.023440, loss_ce: 0.010676\n",
            "iteration 12492 : loss : 0.028799, loss_ce: 0.008303\n",
            "iteration 12493 : loss : 0.019313, loss_ce: 0.003846\n",
            "iteration 12494 : loss : 0.019818, loss_ce: 0.005772\n",
            "iteration 12495 : loss : 0.026807, loss_ce: 0.006750\n",
            "iteration 12496 : loss : 0.025882, loss_ce: 0.012363\n",
            "iteration 12497 : loss : 0.023836, loss_ce: 0.007388\n",
            "iteration 12498 : loss : 0.077707, loss_ce: 0.002321\n",
            "iteration 12499 : loss : 0.025946, loss_ce: 0.009360\n",
            "iteration 12500 : loss : 0.021520, loss_ce: 0.006982\n",
            "iteration 12501 : loss : 0.026737, loss_ce: 0.009875\n",
            "iteration 12502 : loss : 0.029458, loss_ce: 0.008192\n",
            "iteration 12503 : loss : 0.080032, loss_ce: 0.004728\n",
            "iteration 12504 : loss : 0.027004, loss_ce: 0.005672\n",
            "iteration 12505 : loss : 0.026537, loss_ce: 0.011897\n",
            "iteration 12506 : loss : 0.026911, loss_ce: 0.007522\n",
            "iteration 12507 : loss : 0.022769, loss_ce: 0.004968\n",
            "iteration 12508 : loss : 0.023021, loss_ce: 0.008291\n",
            "iteration 12509 : loss : 0.026226, loss_ce: 0.008982\n",
            "iteration 12510 : loss : 0.077131, loss_ce: 0.004653\n",
            "iteration 12511 : loss : 0.022183, loss_ce: 0.008996\n",
            "iteration 12512 : loss : 0.025339, loss_ce: 0.012417\n",
            "iteration 12513 : loss : 0.025616, loss_ce: 0.008017\n",
            "iteration 12514 : loss : 0.025137, loss_ce: 0.007663\n",
            "iteration 12515 : loss : 0.022819, loss_ce: 0.007144\n",
            "iteration 12516 : loss : 0.026140, loss_ce: 0.009529\n",
            "iteration 12517 : loss : 0.073189, loss_ce: 0.004535\n",
            "iteration 12518 : loss : 0.022777, loss_ce: 0.007718\n",
            "iteration 12519 : loss : 0.034717, loss_ce: 0.009441\n",
            "iteration 12520 : loss : 0.024054, loss_ce: 0.011329\n",
            "iteration 12521 : loss : 0.021883, loss_ce: 0.006166\n",
            "iteration 12522 : loss : 0.025245, loss_ce: 0.004807\n",
            "iteration 12523 : loss : 0.027664, loss_ce: 0.008878\n",
            "iteration 12524 : loss : 0.022749, loss_ce: 0.007516\n",
            "iteration 12525 : loss : 0.033637, loss_ce: 0.009802\n",
            "iteration 12526 : loss : 0.024690, loss_ce: 0.010899\n",
            "iteration 12527 : loss : 0.027309, loss_ce: 0.007979\n",
            "iteration 12528 : loss : 0.025551, loss_ce: 0.010191\n",
            "iteration 12529 : loss : 0.024770, loss_ce: 0.010975\n",
            "iteration 12530 : loss : 0.074991, loss_ce: 0.009492\n",
            "iteration 12531 : loss : 0.025867, loss_ce: 0.010484\n",
            "iteration 12532 : loss : 0.028021, loss_ce: 0.009685\n",
            "iteration 12533 : loss : 0.026839, loss_ce: 0.009403\n",
            "iteration 12534 : loss : 0.024298, loss_ce: 0.009865\n",
            "iteration 12535 : loss : 0.021822, loss_ce: 0.007023\n",
            "iteration 12536 : loss : 0.020859, loss_ce: 0.008632\n",
            "iteration 12537 : loss : 0.024042, loss_ce: 0.007223\n",
            "iteration 12538 : loss : 0.024254, loss_ce: 0.012604\n",
            "iteration 12539 : loss : 0.080311, loss_ce: 0.004633\n",
            "iteration 12540 : loss : 0.028386, loss_ce: 0.011046\n",
            "iteration 12541 : loss : 0.026304, loss_ce: 0.010953\n",
            "iteration 12542 : loss : 0.023528, loss_ce: 0.011778\n",
            "iteration 12543 : loss : 0.022143, loss_ce: 0.008013\n",
            "iteration 12544 : loss : 0.022704, loss_ce: 0.007852\n",
            "iteration 12545 : loss : 0.024688, loss_ce: 0.006840\n",
            "iteration 12546 : loss : 0.022739, loss_ce: 0.006779\n",
            "iteration 12547 : loss : 0.021550, loss_ce: 0.004761\n",
            "iteration 12548 : loss : 0.024088, loss_ce: 0.007964\n",
            "iteration 12549 : loss : 0.024732, loss_ce: 0.009972\n",
            "iteration 12550 : loss : 0.027599, loss_ce: 0.006409\n",
            "iteration 12551 : loss : 0.124423, loss_ce: 0.004263\n",
            "iteration 12552 : loss : 0.026650, loss_ce: 0.012729\n",
            "iteration 12553 : loss : 0.075106, loss_ce: 0.006117\n",
            "iteration 12554 : loss : 0.035776, loss_ce: 0.004949\n",
            "iteration 12555 : loss : 0.232642, loss_ce: 0.011136\n",
            " 90%|█████████████████████████▏  | 135/150 [4:55:12<32:39, 130.65s/it]iteration 12556 : loss : 0.017606, loss_ce: 0.006370\n",
            "iteration 12557 : loss : 0.024757, loss_ce: 0.008387\n",
            "iteration 12558 : loss : 0.033993, loss_ce: 0.007358\n",
            "iteration 12559 : loss : 0.026751, loss_ce: 0.008307\n",
            "iteration 12560 : loss : 0.076252, loss_ce: 0.006368\n",
            "iteration 12561 : loss : 0.028729, loss_ce: 0.009414\n",
            "iteration 12562 : loss : 0.023854, loss_ce: 0.005810\n",
            "iteration 12563 : loss : 0.023066, loss_ce: 0.007781\n",
            "iteration 12564 : loss : 0.023354, loss_ce: 0.008407\n",
            "iteration 12565 : loss : 0.020889, loss_ce: 0.009398\n",
            "iteration 12566 : loss : 0.022422, loss_ce: 0.004732\n",
            "iteration 12567 : loss : 0.041101, loss_ce: 0.009199\n",
            "iteration 12568 : loss : 0.027238, loss_ce: 0.006731\n",
            "iteration 12569 : loss : 0.025200, loss_ce: 0.007795\n",
            "iteration 12570 : loss : 0.023275, loss_ce: 0.007653\n",
            "iteration 12571 : loss : 0.023482, loss_ce: 0.007853\n",
            "iteration 12572 : loss : 0.022720, loss_ce: 0.008193\n",
            "iteration 12573 : loss : 0.025237, loss_ce: 0.009470\n",
            "iteration 12574 : loss : 0.020763, loss_ce: 0.006938\n",
            "iteration 12575 : loss : 0.020632, loss_ce: 0.006050\n",
            "iteration 12576 : loss : 0.024878, loss_ce: 0.004345\n",
            "iteration 12577 : loss : 0.024519, loss_ce: 0.008694\n",
            "iteration 12578 : loss : 0.027977, loss_ce: 0.009572\n",
            "iteration 12579 : loss : 0.026050, loss_ce: 0.015340\n",
            "iteration 12580 : loss : 0.027466, loss_ce: 0.010855\n",
            "iteration 12581 : loss : 0.024473, loss_ce: 0.008291\n",
            "iteration 12582 : loss : 0.024138, loss_ce: 0.010935\n",
            "iteration 12583 : loss : 0.024914, loss_ce: 0.003912\n",
            "iteration 12584 : loss : 0.023056, loss_ce: 0.007923\n",
            "iteration 12585 : loss : 0.021137, loss_ce: 0.005128\n",
            "iteration 12586 : loss : 0.022363, loss_ce: 0.007336\n",
            "iteration 12587 : loss : 0.029349, loss_ce: 0.006257\n",
            "iteration 12588 : loss : 0.076631, loss_ce: 0.005029\n",
            "iteration 12589 : loss : 0.024360, loss_ce: 0.006611\n",
            "iteration 12590 : loss : 0.021515, loss_ce: 0.007888\n",
            "iteration 12591 : loss : 0.025516, loss_ce: 0.008610\n",
            "iteration 12592 : loss : 0.022636, loss_ce: 0.005365\n",
            "iteration 12593 : loss : 0.027057, loss_ce: 0.013481\n",
            "iteration 12594 : loss : 0.031963, loss_ce: 0.010173\n",
            "iteration 12595 : loss : 0.024041, loss_ce: 0.009542\n",
            "iteration 12596 : loss : 0.023613, loss_ce: 0.007871\n",
            "iteration 12597 : loss : 0.026859, loss_ce: 0.011249\n",
            "iteration 12598 : loss : 0.027828, loss_ce: 0.009498\n",
            "iteration 12599 : loss : 0.021656, loss_ce: 0.007169\n",
            "iteration 12600 : loss : 0.019024, loss_ce: 0.006696\n",
            "iteration 12601 : loss : 0.025799, loss_ce: 0.009462\n",
            "iteration 12602 : loss : 0.030281, loss_ce: 0.008231\n",
            "iteration 12603 : loss : 0.027567, loss_ce: 0.007878\n",
            "iteration 12604 : loss : 0.021417, loss_ce: 0.010177\n",
            "iteration 12605 : loss : 0.026175, loss_ce: 0.007396\n",
            "iteration 12606 : loss : 0.021060, loss_ce: 0.008423\n",
            "iteration 12607 : loss : 0.022705, loss_ce: 0.008338\n",
            "iteration 12608 : loss : 0.023304, loss_ce: 0.008409\n",
            "iteration 12609 : loss : 0.019756, loss_ce: 0.007535\n",
            "iteration 12610 : loss : 0.026613, loss_ce: 0.009161\n",
            "iteration 12611 : loss : 0.025795, loss_ce: 0.008555\n",
            "iteration 12612 : loss : 0.024714, loss_ce: 0.012037\n",
            "iteration 12613 : loss : 0.023120, loss_ce: 0.009379\n",
            "iteration 12614 : loss : 0.025907, loss_ce: 0.007347\n",
            "iteration 12615 : loss : 0.030491, loss_ce: 0.006385\n",
            "iteration 12616 : loss : 0.025796, loss_ce: 0.009922\n",
            "iteration 12617 : loss : 0.080811, loss_ce: 0.005063\n",
            "iteration 12618 : loss : 0.021101, loss_ce: 0.007212\n",
            "iteration 12619 : loss : 0.023476, loss_ce: 0.007964\n",
            "iteration 12620 : loss : 0.021902, loss_ce: 0.007514\n",
            "iteration 12621 : loss : 0.020978, loss_ce: 0.006874\n",
            "iteration 12622 : loss : 0.019088, loss_ce: 0.007805\n",
            "iteration 12623 : loss : 0.128166, loss_ce: 0.003146\n",
            "iteration 12624 : loss : 0.023888, loss_ce: 0.006887\n",
            "iteration 12625 : loss : 0.032928, loss_ce: 0.004254\n",
            "iteration 12626 : loss : 0.024426, loss_ce: 0.014755\n",
            "iteration 12627 : loss : 0.021335, loss_ce: 0.006854\n",
            "iteration 12628 : loss : 0.035746, loss_ce: 0.010613\n",
            "iteration 12629 : loss : 0.022595, loss_ce: 0.008920\n",
            "iteration 12630 : loss : 0.073868, loss_ce: 0.006750\n",
            "iteration 12631 : loss : 0.026577, loss_ce: 0.009290\n",
            "iteration 12632 : loss : 0.025201, loss_ce: 0.009608\n",
            "iteration 12633 : loss : 0.024315, loss_ce: 0.010807\n",
            "iteration 12634 : loss : 0.024160, loss_ce: 0.005277\n",
            "iteration 12635 : loss : 0.023736, loss_ce: 0.007456\n",
            "iteration 12636 : loss : 0.026355, loss_ce: 0.010792\n",
            "iteration 12637 : loss : 0.020885, loss_ce: 0.005200\n",
            "iteration 12638 : loss : 0.022829, loss_ce: 0.007039\n",
            "iteration 12639 : loss : 0.025384, loss_ce: 0.004934\n",
            "iteration 12640 : loss : 0.024155, loss_ce: 0.011868\n",
            "iteration 12641 : loss : 0.021554, loss_ce: 0.008155\n",
            "iteration 12642 : loss : 0.024106, loss_ce: 0.009621\n",
            "iteration 12643 : loss : 0.023723, loss_ce: 0.006885\n",
            "iteration 12644 : loss : 0.025129, loss_ce: 0.007742\n",
            "iteration 12645 : loss : 0.023226, loss_ce: 0.009482\n",
            "iteration 12646 : loss : 0.022002, loss_ce: 0.007413\n",
            "iteration 12647 : loss : 0.024405, loss_ce: 0.008596\n",
            "iteration 12648 : loss : 0.335001, loss_ce: 0.001582\n",
            " 91%|█████████████████████████▍  | 136/150 [4:57:23<30:29, 130.67s/it]iteration 12649 : loss : 0.020117, loss_ce: 0.006938\n",
            "iteration 12650 : loss : 0.028559, loss_ce: 0.003316\n",
            "iteration 12651 : loss : 0.030875, loss_ce: 0.007936\n",
            "iteration 12652 : loss : 0.024688, loss_ce: 0.007389\n",
            "iteration 12653 : loss : 0.024682, loss_ce: 0.007835\n",
            "iteration 12654 : loss : 0.023519, loss_ce: 0.010993\n",
            "iteration 12655 : loss : 0.020919, loss_ce: 0.007618\n",
            "iteration 12656 : loss : 0.022562, loss_ce: 0.006993\n",
            "iteration 12657 : loss : 0.025873, loss_ce: 0.010547\n",
            "iteration 12658 : loss : 0.028637, loss_ce: 0.015107\n",
            "iteration 12659 : loss : 0.024416, loss_ce: 0.008008\n",
            "iteration 12660 : loss : 0.029022, loss_ce: 0.010891\n",
            "iteration 12661 : loss : 0.030903, loss_ce: 0.004682\n",
            "iteration 12662 : loss : 0.023365, loss_ce: 0.007786\n",
            "iteration 12663 : loss : 0.023473, loss_ce: 0.009271\n",
            "iteration 12664 : loss : 0.026257, loss_ce: 0.013187\n",
            "iteration 12665 : loss : 0.030917, loss_ce: 0.007176\n",
            "iteration 12666 : loss : 0.022979, loss_ce: 0.012848\n",
            "iteration 12667 : loss : 0.026142, loss_ce: 0.007467\n",
            "iteration 12668 : loss : 0.018412, loss_ce: 0.005803\n",
            "iteration 12669 : loss : 0.023127, loss_ce: 0.011462\n",
            "iteration 12670 : loss : 0.021932, loss_ce: 0.004786\n",
            "iteration 12671 : loss : 0.024610, loss_ce: 0.009234\n",
            "iteration 12672 : loss : 0.021687, loss_ce: 0.007392\n",
            "iteration 12673 : loss : 0.022297, loss_ce: 0.007418\n",
            "iteration 12674 : loss : 0.024701, loss_ce: 0.010248\n",
            "iteration 12675 : loss : 0.024664, loss_ce: 0.008268\n",
            "iteration 12676 : loss : 0.024128, loss_ce: 0.006136\n",
            "iteration 12677 : loss : 0.019681, loss_ce: 0.004904\n",
            "iteration 12678 : loss : 0.027733, loss_ce: 0.012725\n",
            "iteration 12679 : loss : 0.024965, loss_ce: 0.005844\n",
            "iteration 12680 : loss : 0.024717, loss_ce: 0.009222\n",
            "iteration 12681 : loss : 0.020761, loss_ce: 0.006070\n",
            "iteration 12682 : loss : 0.020096, loss_ce: 0.009965\n",
            "iteration 12683 : loss : 0.034273, loss_ce: 0.012444\n",
            "iteration 12684 : loss : 0.020162, loss_ce: 0.006739\n",
            "iteration 12685 : loss : 0.024184, loss_ce: 0.014365\n",
            "iteration 12686 : loss : 0.027162, loss_ce: 0.010687\n",
            "iteration 12687 : loss : 0.027588, loss_ce: 0.007904\n",
            "iteration 12688 : loss : 0.023043, loss_ce: 0.006977\n",
            "iteration 12689 : loss : 0.020621, loss_ce: 0.006727\n",
            "iteration 12690 : loss : 0.021452, loss_ce: 0.006938\n",
            "iteration 12691 : loss : 0.027371, loss_ce: 0.002491\n",
            "iteration 12692 : loss : 0.022089, loss_ce: 0.009189\n",
            "iteration 12693 : loss : 0.025315, loss_ce: 0.009744\n",
            "iteration 12694 : loss : 0.022897, loss_ce: 0.009461\n",
            "iteration 12695 : loss : 0.022339, loss_ce: 0.010542\n",
            "iteration 12696 : loss : 0.025194, loss_ce: 0.008085\n",
            "iteration 12697 : loss : 0.021338, loss_ce: 0.010144\n",
            "iteration 12698 : loss : 0.024090, loss_ce: 0.007914\n",
            "iteration 12699 : loss : 0.029237, loss_ce: 0.012444\n",
            "iteration 12700 : loss : 0.027702, loss_ce: 0.008700\n",
            "iteration 12701 : loss : 0.024451, loss_ce: 0.007899\n",
            "iteration 12702 : loss : 0.071368, loss_ce: 0.005191\n",
            "iteration 12703 : loss : 0.022098, loss_ce: 0.005664\n",
            "iteration 12704 : loss : 0.024997, loss_ce: 0.012072\n",
            "iteration 12705 : loss : 0.018309, loss_ce: 0.005581\n",
            "iteration 12706 : loss : 0.023834, loss_ce: 0.009948\n",
            "iteration 12707 : loss : 0.025634, loss_ce: 0.009146\n",
            "iteration 12708 : loss : 0.020653, loss_ce: 0.005565\n",
            "iteration 12709 : loss : 0.022865, loss_ce: 0.008021\n",
            "iteration 12710 : loss : 0.026140, loss_ce: 0.008940\n",
            "iteration 12711 : loss : 0.023430, loss_ce: 0.004945\n",
            "iteration 12712 : loss : 0.024309, loss_ce: 0.006846\n",
            "iteration 12713 : loss : 0.022723, loss_ce: 0.007999\n",
            "iteration 12714 : loss : 0.036163, loss_ce: 0.007614\n",
            "iteration 12715 : loss : 0.021157, loss_ce: 0.007445\n",
            "iteration 12716 : loss : 0.020997, loss_ce: 0.008967\n",
            "iteration 12717 : loss : 0.033163, loss_ce: 0.007491\n",
            "iteration 12718 : loss : 0.022878, loss_ce: 0.008161\n",
            "iteration 12719 : loss : 0.021194, loss_ce: 0.006973\n",
            "iteration 12720 : loss : 0.020975, loss_ce: 0.007287\n",
            "iteration 12721 : loss : 0.020005, loss_ce: 0.006732\n",
            "iteration 12722 : loss : 0.021126, loss_ce: 0.006119\n",
            "iteration 12723 : loss : 0.021655, loss_ce: 0.006148\n",
            "iteration 12724 : loss : 0.022177, loss_ce: 0.010231\n",
            "iteration 12725 : loss : 0.019150, loss_ce: 0.004863\n",
            "iteration 12726 : loss : 0.029053, loss_ce: 0.008581\n",
            "iteration 12727 : loss : 0.070945, loss_ce: 0.004582\n",
            "iteration 12728 : loss : 0.023096, loss_ce: 0.006731\n",
            "iteration 12729 : loss : 0.018632, loss_ce: 0.005190\n",
            "iteration 12730 : loss : 0.024724, loss_ce: 0.007522\n",
            "iteration 12731 : loss : 0.076737, loss_ce: 0.006272\n",
            "iteration 12732 : loss : 0.028055, loss_ce: 0.009143\n",
            "iteration 12733 : loss : 0.023097, loss_ce: 0.010208\n",
            "iteration 12734 : loss : 0.022425, loss_ce: 0.006346\n",
            "iteration 12735 : loss : 0.072600, loss_ce: 0.006809\n",
            "iteration 12736 : loss : 0.078785, loss_ce: 0.003461\n",
            "iteration 12737 : loss : 0.020437, loss_ce: 0.005489\n",
            "iteration 12738 : loss : 0.028397, loss_ce: 0.009656\n",
            "iteration 12739 : loss : 0.026126, loss_ce: 0.009332\n",
            "iteration 12740 : loss : 0.031286, loss_ce: 0.007865\n",
            "iteration 12741 : loss : 0.182621, loss_ce: 0.009851\n",
            " 91%|█████████████████████████▌  | 137/150 [4:59:35<28:22, 130.95s/it]iteration 12742 : loss : 0.023993, loss_ce: 0.007562\n",
            "iteration 12743 : loss : 0.023361, loss_ce: 0.012602\n",
            "iteration 12744 : loss : 0.022771, loss_ce: 0.006788\n",
            "iteration 12745 : loss : 0.019285, loss_ce: 0.007694\n",
            "iteration 12746 : loss : 0.026819, loss_ce: 0.006534\n",
            "iteration 12747 : loss : 0.021238, loss_ce: 0.005443\n",
            "iteration 12748 : loss : 0.031745, loss_ce: 0.005962\n",
            "iteration 12749 : loss : 0.023272, loss_ce: 0.009926\n",
            "iteration 12750 : loss : 0.024583, loss_ce: 0.007859\n",
            "iteration 12751 : loss : 0.024985, loss_ce: 0.006763\n",
            "iteration 12752 : loss : 0.024262, loss_ce: 0.008624\n",
            "iteration 12753 : loss : 0.020627, loss_ce: 0.005592\n",
            "iteration 12754 : loss : 0.023732, loss_ce: 0.007645\n",
            "iteration 12755 : loss : 0.020594, loss_ce: 0.007252\n",
            "iteration 12756 : loss : 0.025622, loss_ce: 0.005681\n",
            "iteration 12757 : loss : 0.019987, loss_ce: 0.007278\n",
            "iteration 12758 : loss : 0.029340, loss_ce: 0.006512\n",
            "iteration 12759 : loss : 0.024346, loss_ce: 0.008566\n",
            "iteration 12760 : loss : 0.026637, loss_ce: 0.011391\n",
            "iteration 12761 : loss : 0.021015, loss_ce: 0.007834\n",
            "iteration 12762 : loss : 0.022568, loss_ce: 0.007963\n",
            "iteration 12763 : loss : 0.082024, loss_ce: 0.007223\n",
            "iteration 12764 : loss : 0.023238, loss_ce: 0.010127\n",
            "iteration 12765 : loss : 0.030875, loss_ce: 0.011705\n",
            "iteration 12766 : loss : 0.023853, loss_ce: 0.007135\n",
            "iteration 12767 : loss : 0.021135, loss_ce: 0.008820\n",
            "iteration 12768 : loss : 0.026102, loss_ce: 0.008083\n",
            "iteration 12769 : loss : 0.029739, loss_ce: 0.008652\n",
            "iteration 12770 : loss : 0.022197, loss_ce: 0.007012\n",
            "iteration 12771 : loss : 0.020588, loss_ce: 0.005340\n",
            "iteration 12772 : loss : 0.041817, loss_ce: 0.008698\n",
            "iteration 12773 : loss : 0.028574, loss_ce: 0.013424\n",
            "iteration 12774 : loss : 0.024052, loss_ce: 0.009726\n",
            "iteration 12775 : loss : 0.018375, loss_ce: 0.008130\n",
            "iteration 12776 : loss : 0.025809, loss_ce: 0.009372\n",
            "iteration 12777 : loss : 0.023479, loss_ce: 0.006662\n",
            "iteration 12778 : loss : 0.020875, loss_ce: 0.008118\n",
            "iteration 12779 : loss : 0.023272, loss_ce: 0.010508\n",
            "iteration 12780 : loss : 0.024580, loss_ce: 0.008652\n",
            "iteration 12781 : loss : 0.022190, loss_ce: 0.007276\n",
            "iteration 12782 : loss : 0.077680, loss_ce: 0.006730\n",
            "iteration 12783 : loss : 0.023710, loss_ce: 0.011287\n",
            "iteration 12784 : loss : 0.032650, loss_ce: 0.009258\n",
            "iteration 12785 : loss : 0.020646, loss_ce: 0.004799\n",
            "iteration 12786 : loss : 0.021663, loss_ce: 0.009129\n",
            "iteration 12787 : loss : 0.022256, loss_ce: 0.008695\n",
            "iteration 12788 : loss : 0.027452, loss_ce: 0.008193\n",
            "iteration 12789 : loss : 0.020593, loss_ce: 0.006214\n",
            "iteration 12790 : loss : 0.034870, loss_ce: 0.012934\n",
            "iteration 12791 : loss : 0.022453, loss_ce: 0.007958\n",
            "iteration 12792 : loss : 0.020789, loss_ce: 0.007357\n",
            "iteration 12793 : loss : 0.074342, loss_ce: 0.006692\n",
            "iteration 12794 : loss : 0.026812, loss_ce: 0.013159\n",
            "iteration 12795 : loss : 0.026255, loss_ce: 0.006495\n",
            "iteration 12796 : loss : 0.024549, loss_ce: 0.010673\n",
            "iteration 12797 : loss : 0.021554, loss_ce: 0.008691\n",
            "iteration 12798 : loss : 0.022818, loss_ce: 0.008219\n",
            "iteration 12799 : loss : 0.073850, loss_ce: 0.004547\n",
            "iteration 12800 : loss : 0.021306, loss_ce: 0.008139\n",
            "iteration 12801 : loss : 0.026835, loss_ce: 0.010639\n",
            "iteration 12802 : loss : 0.021540, loss_ce: 0.008866\n",
            "iteration 12803 : loss : 0.024165, loss_ce: 0.005242\n",
            "iteration 12804 : loss : 0.021843, loss_ce: 0.005982\n",
            "iteration 12805 : loss : 0.042247, loss_ce: 0.005003\n",
            "iteration 12806 : loss : 0.023212, loss_ce: 0.011125\n",
            "iteration 12807 : loss : 0.023509, loss_ce: 0.003711\n",
            "iteration 12808 : loss : 0.123701, loss_ce: 0.004132\n",
            "iteration 12809 : loss : 0.020955, loss_ce: 0.005144\n",
            "iteration 12810 : loss : 0.018889, loss_ce: 0.005480\n",
            "iteration 12811 : loss : 0.022053, loss_ce: 0.008775\n",
            "iteration 12812 : loss : 0.022780, loss_ce: 0.008430\n",
            "iteration 12813 : loss : 0.024610, loss_ce: 0.008538\n",
            "iteration 12814 : loss : 0.026090, loss_ce: 0.006903\n",
            "iteration 12815 : loss : 0.018623, loss_ce: 0.006007\n",
            "iteration 12816 : loss : 0.020978, loss_ce: 0.005845\n",
            "iteration 12817 : loss : 0.026178, loss_ce: 0.011645\n",
            "iteration 12818 : loss : 0.026274, loss_ce: 0.008087\n",
            "iteration 12819 : loss : 0.024239, loss_ce: 0.010427\n",
            "iteration 12820 : loss : 0.023739, loss_ce: 0.009008\n",
            "iteration 12821 : loss : 0.022609, loss_ce: 0.007817\n",
            "iteration 12822 : loss : 0.030644, loss_ce: 0.010381\n",
            "iteration 12823 : loss : 0.034660, loss_ce: 0.009984\n",
            "iteration 12824 : loss : 0.025237, loss_ce: 0.005722\n",
            "iteration 12825 : loss : 0.025026, loss_ce: 0.009710\n",
            "iteration 12826 : loss : 0.020499, loss_ce: 0.007105\n",
            "iteration 12827 : loss : 0.024656, loss_ce: 0.005820\n",
            "iteration 12828 : loss : 0.030929, loss_ce: 0.007099\n",
            "iteration 12829 : loss : 0.024673, loss_ce: 0.007069\n",
            "iteration 12830 : loss : 0.023194, loss_ce: 0.008600\n",
            "iteration 12831 : loss : 0.022604, loss_ce: 0.008258\n",
            "iteration 12832 : loss : 0.027779, loss_ce: 0.014411\n",
            "iteration 12833 : loss : 0.022489, loss_ce: 0.008313\n",
            "iteration 12834 : loss : 0.237263, loss_ce: 0.009149\n",
            " 92%|█████████████████████████▊  | 138/150 [5:01:45<26:08, 130.70s/it]iteration 12835 : loss : 0.026087, loss_ce: 0.003233\n",
            "iteration 12836 : loss : 0.022983, loss_ce: 0.006969\n",
            "iteration 12837 : loss : 0.020297, loss_ce: 0.006967\n",
            "iteration 12838 : loss : 0.025176, loss_ce: 0.011555\n",
            "iteration 12839 : loss : 0.072424, loss_ce: 0.005054\n",
            "iteration 12840 : loss : 0.025275, loss_ce: 0.009015\n",
            "iteration 12841 : loss : 0.029019, loss_ce: 0.008585\n",
            "iteration 12842 : loss : 0.019217, loss_ce: 0.007162\n",
            "iteration 12843 : loss : 0.034265, loss_ce: 0.009212\n",
            "iteration 12844 : loss : 0.021280, loss_ce: 0.005889\n",
            "iteration 12845 : loss : 0.025174, loss_ce: 0.007943\n",
            "iteration 12846 : loss : 0.020609, loss_ce: 0.007369\n",
            "iteration 12847 : loss : 0.025039, loss_ce: 0.007343\n",
            "iteration 12848 : loss : 0.028455, loss_ce: 0.008397\n",
            "iteration 12849 : loss : 0.026621, loss_ce: 0.009881\n",
            "iteration 12850 : loss : 0.019239, loss_ce: 0.006824\n",
            "iteration 12851 : loss : 0.024486, loss_ce: 0.011246\n",
            "iteration 12852 : loss : 0.021083, loss_ce: 0.007921\n",
            "iteration 12853 : loss : 0.027031, loss_ce: 0.012940\n",
            "iteration 12854 : loss : 0.031342, loss_ce: 0.010548\n",
            "iteration 12855 : loss : 0.020533, loss_ce: 0.006912\n",
            "iteration 12856 : loss : 0.023900, loss_ce: 0.009995\n",
            "iteration 12857 : loss : 0.023464, loss_ce: 0.007752\n",
            "iteration 12858 : loss : 0.022501, loss_ce: 0.007633\n",
            "iteration 12859 : loss : 0.021356, loss_ce: 0.005922\n",
            "iteration 12860 : loss : 0.021186, loss_ce: 0.006222\n",
            "iteration 12861 : loss : 0.023212, loss_ce: 0.006451\n",
            "iteration 12862 : loss : 0.028843, loss_ce: 0.010824\n",
            "iteration 12863 : loss : 0.022103, loss_ce: 0.009135\n",
            "iteration 12864 : loss : 0.022785, loss_ce: 0.007081\n",
            "iteration 12865 : loss : 0.028141, loss_ce: 0.009569\n",
            "iteration 12866 : loss : 0.023703, loss_ce: 0.009202\n",
            "iteration 12867 : loss : 0.018078, loss_ce: 0.005530\n",
            "iteration 12868 : loss : 0.024528, loss_ce: 0.009203\n",
            "iteration 12869 : loss : 0.128833, loss_ce: 0.003479\n",
            "iteration 12870 : loss : 0.027543, loss_ce: 0.010448\n",
            "iteration 12871 : loss : 0.019564, loss_ce: 0.006772\n",
            "iteration 12872 : loss : 0.020312, loss_ce: 0.008288\n",
            "iteration 12873 : loss : 0.023820, loss_ce: 0.009748\n",
            "iteration 12874 : loss : 0.022792, loss_ce: 0.010581\n",
            "iteration 12875 : loss : 0.027061, loss_ce: 0.014862\n",
            "iteration 12876 : loss : 0.022216, loss_ce: 0.009663\n",
            "iteration 12877 : loss : 0.022586, loss_ce: 0.008933\n",
            "iteration 12878 : loss : 0.025300, loss_ce: 0.010151\n",
            "iteration 12879 : loss : 0.021673, loss_ce: 0.009265\n",
            "iteration 12880 : loss : 0.020193, loss_ce: 0.007177\n",
            "iteration 12881 : loss : 0.024901, loss_ce: 0.008409\n",
            "iteration 12882 : loss : 0.027136, loss_ce: 0.009861\n",
            "iteration 12883 : loss : 0.018977, loss_ce: 0.007510\n",
            "iteration 12884 : loss : 0.023705, loss_ce: 0.005573\n",
            "iteration 12885 : loss : 0.025838, loss_ce: 0.009045\n",
            "iteration 12886 : loss : 0.022259, loss_ce: 0.010096\n",
            "iteration 12887 : loss : 0.021006, loss_ce: 0.006297\n",
            "iteration 12888 : loss : 0.031741, loss_ce: 0.015252\n",
            "iteration 12889 : loss : 0.023501, loss_ce: 0.010576\n",
            "iteration 12890 : loss : 0.020791, loss_ce: 0.005755\n",
            "iteration 12891 : loss : 0.025091, loss_ce: 0.008858\n",
            "iteration 12892 : loss : 0.019870, loss_ce: 0.006781\n",
            "iteration 12893 : loss : 0.025214, loss_ce: 0.011097\n",
            "iteration 12894 : loss : 0.020354, loss_ce: 0.005980\n",
            "iteration 12895 : loss : 0.022533, loss_ce: 0.008147\n",
            "iteration 12896 : loss : 0.073476, loss_ce: 0.006051\n",
            "iteration 12897 : loss : 0.024344, loss_ce: 0.008830\n",
            "iteration 12898 : loss : 0.024610, loss_ce: 0.012227\n",
            "iteration 12899 : loss : 0.022281, loss_ce: 0.005656\n",
            "iteration 12900 : loss : 0.024586, loss_ce: 0.006581\n",
            "iteration 12901 : loss : 0.023349, loss_ce: 0.009102\n",
            "iteration 12902 : loss : 0.019825, loss_ce: 0.005879\n",
            "iteration 12903 : loss : 0.076854, loss_ce: 0.009273\n",
            "iteration 12904 : loss : 0.074740, loss_ce: 0.006039\n",
            "iteration 12905 : loss : 0.017101, loss_ce: 0.003182\n",
            "iteration 12906 : loss : 0.018842, loss_ce: 0.006215\n",
            "iteration 12907 : loss : 0.022946, loss_ce: 0.006947\n",
            "iteration 12908 : loss : 0.021010, loss_ce: 0.006365\n",
            "iteration 12909 : loss : 0.022498, loss_ce: 0.007408\n",
            "iteration 12910 : loss : 0.023427, loss_ce: 0.005914\n",
            "iteration 12911 : loss : 0.025209, loss_ce: 0.009878\n",
            "iteration 12912 : loss : 0.077305, loss_ce: 0.005734\n",
            "iteration 12913 : loss : 0.025970, loss_ce: 0.005646\n",
            "iteration 12914 : loss : 0.023584, loss_ce: 0.007124\n",
            "iteration 12915 : loss : 0.026033, loss_ce: 0.006770\n",
            "iteration 12916 : loss : 0.024742, loss_ce: 0.007499\n",
            "iteration 12917 : loss : 0.025357, loss_ce: 0.007313\n",
            "iteration 12918 : loss : 0.025714, loss_ce: 0.011716\n",
            "iteration 12919 : loss : 0.028081, loss_ce: 0.007219\n",
            "iteration 12920 : loss : 0.022581, loss_ce: 0.006694\n",
            "iteration 12921 : loss : 0.019302, loss_ce: 0.006993\n",
            "iteration 12922 : loss : 0.021630, loss_ce: 0.004303\n",
            "iteration 12923 : loss : 0.029943, loss_ce: 0.009402\n",
            "iteration 12924 : loss : 0.024803, loss_ce: 0.006853\n",
            "iteration 12925 : loss : 0.022903, loss_ce: 0.008097\n",
            "iteration 12926 : loss : 0.022640, loss_ce: 0.008719\n",
            "iteration 12927 : loss : 0.041218, loss_ce: 0.020621\n",
            " 93%|█████████████████████████▉  | 139/150 [5:03:55<23:57, 130.64s/it]iteration 12928 : loss : 0.023324, loss_ce: 0.009533\n",
            "iteration 12929 : loss : 0.026959, loss_ce: 0.010173\n",
            "iteration 12930 : loss : 0.025358, loss_ce: 0.008581\n",
            "iteration 12931 : loss : 0.025316, loss_ce: 0.007299\n",
            "iteration 12932 : loss : 0.021917, loss_ce: 0.006939\n",
            "iteration 12933 : loss : 0.021801, loss_ce: 0.004873\n",
            "iteration 12934 : loss : 0.124608, loss_ce: 0.004535\n",
            "iteration 12935 : loss : 0.022225, loss_ce: 0.006350\n",
            "iteration 12936 : loss : 0.021917, loss_ce: 0.006998\n",
            "iteration 12937 : loss : 0.022535, loss_ce: 0.008673\n",
            "iteration 12938 : loss : 0.023744, loss_ce: 0.009406\n",
            "iteration 12939 : loss : 0.021965, loss_ce: 0.008668\n",
            "iteration 12940 : loss : 0.028264, loss_ce: 0.009678\n",
            "iteration 12941 : loss : 0.023198, loss_ce: 0.009142\n",
            "iteration 12942 : loss : 0.024327, loss_ce: 0.008040\n",
            "iteration 12943 : loss : 0.229372, loss_ce: 0.004101\n",
            "iteration 12944 : loss : 0.022436, loss_ce: 0.007978\n",
            "iteration 12945 : loss : 0.081903, loss_ce: 0.004302\n",
            "iteration 12946 : loss : 0.021351, loss_ce: 0.007657\n",
            "iteration 12947 : loss : 0.024685, loss_ce: 0.008568\n",
            "iteration 12948 : loss : 0.020328, loss_ce: 0.007365\n",
            "iteration 12949 : loss : 0.079888, loss_ce: 0.003371\n",
            "iteration 12950 : loss : 0.024338, loss_ce: 0.006261\n",
            "iteration 12951 : loss : 0.022806, loss_ce: 0.007630\n",
            "iteration 12952 : loss : 0.019998, loss_ce: 0.009981\n",
            "iteration 12953 : loss : 0.023578, loss_ce: 0.008450\n",
            "iteration 12954 : loss : 0.021830, loss_ce: 0.004987\n",
            "iteration 12955 : loss : 0.030153, loss_ce: 0.009247\n",
            "iteration 12956 : loss : 0.024158, loss_ce: 0.005897\n",
            "iteration 12957 : loss : 0.021385, loss_ce: 0.009677\n",
            "iteration 12958 : loss : 0.025885, loss_ce: 0.013111\n",
            "iteration 12959 : loss : 0.029423, loss_ce: 0.005587\n",
            "iteration 12960 : loss : 0.022299, loss_ce: 0.007982\n",
            "iteration 12961 : loss : 0.024755, loss_ce: 0.009854\n",
            "iteration 12962 : loss : 0.023330, loss_ce: 0.005421\n",
            "iteration 12963 : loss : 0.021477, loss_ce: 0.008467\n",
            "iteration 12964 : loss : 0.023208, loss_ce: 0.006207\n",
            "iteration 12965 : loss : 0.026000, loss_ce: 0.008215\n",
            "iteration 12966 : loss : 0.022796, loss_ce: 0.007632\n",
            "iteration 12967 : loss : 0.019485, loss_ce: 0.006278\n",
            "iteration 12968 : loss : 0.027369, loss_ce: 0.011785\n",
            "iteration 12969 : loss : 0.030254, loss_ce: 0.016174\n",
            "iteration 12970 : loss : 0.025453, loss_ce: 0.007304\n",
            "iteration 12971 : loss : 0.023230, loss_ce: 0.006096\n",
            "iteration 12972 : loss : 0.027396, loss_ce: 0.009201\n",
            "iteration 12973 : loss : 0.028206, loss_ce: 0.009877\n",
            "iteration 12974 : loss : 0.026008, loss_ce: 0.012098\n",
            "iteration 12975 : loss : 0.021063, loss_ce: 0.006971\n",
            "iteration 12976 : loss : 0.022361, loss_ce: 0.007174\n",
            "iteration 12977 : loss : 0.024309, loss_ce: 0.011866\n",
            "iteration 12978 : loss : 0.027328, loss_ce: 0.011227\n",
            "iteration 12979 : loss : 0.022681, loss_ce: 0.011472\n",
            "iteration 12980 : loss : 0.024201, loss_ce: 0.008456\n",
            "iteration 12981 : loss : 0.019179, loss_ce: 0.004212\n",
            "iteration 12982 : loss : 0.027012, loss_ce: 0.011699\n",
            "iteration 12983 : loss : 0.020665, loss_ce: 0.008036\n",
            "iteration 12984 : loss : 0.024822, loss_ce: 0.006413\n",
            "iteration 12985 : loss : 0.022395, loss_ce: 0.012493\n",
            "iteration 12986 : loss : 0.025324, loss_ce: 0.005220\n",
            "iteration 12987 : loss : 0.025754, loss_ce: 0.008739\n",
            "iteration 12988 : loss : 0.018043, loss_ce: 0.005748\n",
            "iteration 12989 : loss : 0.025088, loss_ce: 0.012309\n",
            "iteration 12990 : loss : 0.023849, loss_ce: 0.010244\n",
            "iteration 12991 : loss : 0.075994, loss_ce: 0.004497\n",
            "iteration 12992 : loss : 0.025295, loss_ce: 0.005427\n",
            "iteration 12993 : loss : 0.020924, loss_ce: 0.006310\n",
            "iteration 12994 : loss : 0.026290, loss_ce: 0.007611\n",
            "iteration 12995 : loss : 0.025323, loss_ce: 0.010827\n",
            "iteration 12996 : loss : 0.024946, loss_ce: 0.006402\n",
            "iteration 12997 : loss : 0.024260, loss_ce: 0.010539\n",
            "iteration 12998 : loss : 0.024070, loss_ce: 0.008397\n",
            "iteration 12999 : loss : 0.020816, loss_ce: 0.008170\n",
            "iteration 13000 : loss : 0.026584, loss_ce: 0.006892\n",
            "iteration 13001 : loss : 0.017065, loss_ce: 0.004371\n",
            "iteration 13002 : loss : 0.020353, loss_ce: 0.005125\n",
            "iteration 13003 : loss : 0.022150, loss_ce: 0.011241\n",
            "iteration 13004 : loss : 0.036537, loss_ce: 0.004515\n",
            "iteration 13005 : loss : 0.032420, loss_ce: 0.010191\n",
            "iteration 13006 : loss : 0.025521, loss_ce: 0.008221\n",
            "iteration 13007 : loss : 0.024126, loss_ce: 0.008540\n",
            "iteration 13008 : loss : 0.023644, loss_ce: 0.008980\n",
            "iteration 13009 : loss : 0.023979, loss_ce: 0.007187\n",
            "iteration 13010 : loss : 0.021537, loss_ce: 0.008221\n",
            "iteration 13011 : loss : 0.021876, loss_ce: 0.007312\n",
            "iteration 13012 : loss : 0.030018, loss_ce: 0.009952\n",
            "iteration 13013 : loss : 0.024318, loss_ce: 0.010218\n",
            "iteration 13014 : loss : 0.028737, loss_ce: 0.007237\n",
            "iteration 13015 : loss : 0.022817, loss_ce: 0.005814\n",
            "iteration 13016 : loss : 0.026821, loss_ce: 0.004336\n",
            "iteration 13017 : loss : 0.019996, loss_ce: 0.005711\n",
            "iteration 13018 : loss : 0.024308, loss_ce: 0.008905\n",
            "iteration 13019 : loss : 0.024421, loss_ce: 0.007632\n",
            "iteration 13020 : loss : 0.181372, loss_ce: 0.006498\n",
            " 93%|██████████████████████████▏ | 140/150 [5:06:07<21:48, 130.87s/it]iteration 13021 : loss : 0.022874, loss_ce: 0.008727\n",
            "iteration 13022 : loss : 0.025071, loss_ce: 0.006622\n",
            "iteration 13023 : loss : 0.032447, loss_ce: 0.008917\n",
            "iteration 13024 : loss : 0.019345, loss_ce: 0.005608\n",
            "iteration 13025 : loss : 0.023634, loss_ce: 0.008062\n",
            "iteration 13026 : loss : 0.027034, loss_ce: 0.009760\n",
            "iteration 13027 : loss : 0.021764, loss_ce: 0.008960\n",
            "iteration 13028 : loss : 0.023510, loss_ce: 0.008607\n",
            "iteration 13029 : loss : 0.020596, loss_ce: 0.006714\n",
            "iteration 13030 : loss : 0.021139, loss_ce: 0.008539\n",
            "iteration 13031 : loss : 0.017336, loss_ce: 0.005218\n",
            "iteration 13032 : loss : 0.072329, loss_ce: 0.005023\n",
            "iteration 13033 : loss : 0.029630, loss_ce: 0.009910\n",
            "iteration 13034 : loss : 0.074937, loss_ce: 0.006211\n",
            "iteration 13035 : loss : 0.023188, loss_ce: 0.003982\n",
            "iteration 13036 : loss : 0.026359, loss_ce: 0.008773\n",
            "iteration 13037 : loss : 0.019386, loss_ce: 0.005731\n",
            "iteration 13038 : loss : 0.021444, loss_ce: 0.007319\n",
            "iteration 13039 : loss : 0.027107, loss_ce: 0.008220\n",
            "iteration 13040 : loss : 0.019866, loss_ce: 0.008415\n",
            "iteration 13041 : loss : 0.019680, loss_ce: 0.006394\n",
            "iteration 13042 : loss : 0.024628, loss_ce: 0.006859\n",
            "iteration 13043 : loss : 0.023155, loss_ce: 0.011615\n",
            "iteration 13044 : loss : 0.023497, loss_ce: 0.008645\n",
            "iteration 13045 : loss : 0.022385, loss_ce: 0.008040\n",
            "iteration 13046 : loss : 0.022197, loss_ce: 0.008106\n",
            "iteration 13047 : loss : 0.021219, loss_ce: 0.004723\n",
            "iteration 13048 : loss : 0.023498, loss_ce: 0.008098\n",
            "iteration 13049 : loss : 0.026460, loss_ce: 0.008549\n",
            "iteration 13050 : loss : 0.026775, loss_ce: 0.008986\n",
            "iteration 13051 : loss : 0.022577, loss_ce: 0.008170\n",
            "iteration 13052 : loss : 0.021757, loss_ce: 0.008237\n",
            "iteration 13053 : loss : 0.078030, loss_ce: 0.007107\n",
            "iteration 13054 : loss : 0.021830, loss_ce: 0.005523\n",
            "iteration 13055 : loss : 0.028196, loss_ce: 0.010256\n",
            "iteration 13056 : loss : 0.024183, loss_ce: 0.012013\n",
            "iteration 13057 : loss : 0.022643, loss_ce: 0.008475\n",
            "iteration 13058 : loss : 0.027912, loss_ce: 0.007871\n",
            "iteration 13059 : loss : 0.021549, loss_ce: 0.005699\n",
            "iteration 13060 : loss : 0.023489, loss_ce: 0.009609\n",
            "iteration 13061 : loss : 0.022150, loss_ce: 0.007206\n",
            "iteration 13062 : loss : 0.022788, loss_ce: 0.006597\n",
            "iteration 13063 : loss : 0.026309, loss_ce: 0.007962\n",
            "iteration 13064 : loss : 0.022394, loss_ce: 0.006409\n",
            "iteration 13065 : loss : 0.024465, loss_ce: 0.007200\n",
            "iteration 13066 : loss : 0.025519, loss_ce: 0.005979\n",
            "iteration 13067 : loss : 0.021843, loss_ce: 0.008544\n",
            "iteration 13068 : loss : 0.024399, loss_ce: 0.006692\n",
            "iteration 13069 : loss : 0.020756, loss_ce: 0.006636\n",
            "iteration 13070 : loss : 0.020427, loss_ce: 0.007833\n",
            "iteration 13071 : loss : 0.026563, loss_ce: 0.013244\n",
            "iteration 13072 : loss : 0.024272, loss_ce: 0.010695\n",
            "iteration 13073 : loss : 0.022874, loss_ce: 0.010422\n",
            "iteration 13074 : loss : 0.026376, loss_ce: 0.004501\n",
            "iteration 13075 : loss : 0.071050, loss_ce: 0.004200\n",
            "iteration 13076 : loss : 0.024354, loss_ce: 0.006376\n",
            "iteration 13077 : loss : 0.026585, loss_ce: 0.014224\n",
            "iteration 13078 : loss : 0.020914, loss_ce: 0.008053\n",
            "iteration 13079 : loss : 0.029482, loss_ce: 0.007275\n",
            "iteration 13080 : loss : 0.025313, loss_ce: 0.005764\n",
            "iteration 13081 : loss : 0.022696, loss_ce: 0.005694\n",
            "iteration 13082 : loss : 0.025385, loss_ce: 0.009125\n",
            "iteration 13083 : loss : 0.022595, loss_ce: 0.004825\n",
            "iteration 13084 : loss : 0.020497, loss_ce: 0.008593\n",
            "iteration 13085 : loss : 0.028400, loss_ce: 0.006853\n",
            "iteration 13086 : loss : 0.023511, loss_ce: 0.010297\n",
            "iteration 13087 : loss : 0.021762, loss_ce: 0.007662\n",
            "iteration 13088 : loss : 0.023629, loss_ce: 0.007338\n",
            "iteration 13089 : loss : 0.023525, loss_ce: 0.009756\n",
            "iteration 13090 : loss : 0.021300, loss_ce: 0.005626\n",
            "iteration 13091 : loss : 0.023939, loss_ce: 0.006836\n",
            "iteration 13092 : loss : 0.022249, loss_ce: 0.010171\n",
            "iteration 13093 : loss : 0.024211, loss_ce: 0.010799\n",
            "iteration 13094 : loss : 0.028415, loss_ce: 0.008429\n",
            "iteration 13095 : loss : 0.024438, loss_ce: 0.006712\n",
            "iteration 13096 : loss : 0.020583, loss_ce: 0.006921\n",
            "iteration 13097 : loss : 0.022605, loss_ce: 0.006361\n",
            "iteration 13098 : loss : 0.022340, loss_ce: 0.008089\n",
            "iteration 13099 : loss : 0.022846, loss_ce: 0.008277\n",
            "iteration 13100 : loss : 0.026879, loss_ce: 0.012999\n",
            "iteration 13101 : loss : 0.021505, loss_ce: 0.008489\n",
            "iteration 13102 : loss : 0.022311, loss_ce: 0.006483\n",
            "iteration 13103 : loss : 0.025570, loss_ce: 0.007223\n",
            "iteration 13104 : loss : 0.021690, loss_ce: 0.008020\n",
            "iteration 13105 : loss : 0.026825, loss_ce: 0.010312\n",
            "iteration 13106 : loss : 0.019547, loss_ce: 0.007588\n",
            "iteration 13107 : loss : 0.023819, loss_ce: 0.005553\n",
            "iteration 13108 : loss : 0.022635, loss_ce: 0.010936\n",
            "iteration 13109 : loss : 0.024268, loss_ce: 0.007644\n",
            "iteration 13110 : loss : 0.038301, loss_ce: 0.010770\n",
            "iteration 13111 : loss : 0.023900, loss_ce: 0.009536\n",
            "iteration 13112 : loss : 0.018508, loss_ce: 0.006921\n",
            "iteration 13113 : loss : 0.078113, loss_ce: 0.010752\n",
            " 94%|██████████████████████████▎ | 141/150 [5:08:17<19:35, 130.62s/it]iteration 13114 : loss : 0.025862, loss_ce: 0.005150\n",
            "iteration 13115 : loss : 0.022491, loss_ce: 0.006943\n",
            "iteration 13116 : loss : 0.020994, loss_ce: 0.004855\n",
            "iteration 13117 : loss : 0.018884, loss_ce: 0.005088\n",
            "iteration 13118 : loss : 0.021905, loss_ce: 0.006440\n",
            "iteration 13119 : loss : 0.028184, loss_ce: 0.008226\n",
            "iteration 13120 : loss : 0.085429, loss_ce: 0.007201\n",
            "iteration 13121 : loss : 0.022746, loss_ce: 0.005353\n",
            "iteration 13122 : loss : 0.022630, loss_ce: 0.006719\n",
            "iteration 13123 : loss : 0.024344, loss_ce: 0.009608\n",
            "iteration 13124 : loss : 0.021814, loss_ce: 0.006899\n",
            "iteration 13125 : loss : 0.127500, loss_ce: 0.003945\n",
            "iteration 13126 : loss : 0.024009, loss_ce: 0.011085\n",
            "iteration 13127 : loss : 0.021234, loss_ce: 0.007222\n",
            "iteration 13128 : loss : 0.022297, loss_ce: 0.008507\n",
            "iteration 13129 : loss : 0.020790, loss_ce: 0.008482\n",
            "iteration 13130 : loss : 0.024365, loss_ce: 0.010994\n",
            "iteration 13131 : loss : 0.020292, loss_ce: 0.007547\n",
            "iteration 13132 : loss : 0.024424, loss_ce: 0.008001\n",
            "iteration 13133 : loss : 0.027599, loss_ce: 0.005869\n",
            "iteration 13134 : loss : 0.031233, loss_ce: 0.007212\n",
            "iteration 13135 : loss : 0.021895, loss_ce: 0.004947\n",
            "iteration 13136 : loss : 0.020866, loss_ce: 0.005276\n",
            "iteration 13137 : loss : 0.024878, loss_ce: 0.006101\n",
            "iteration 13138 : loss : 0.074364, loss_ce: 0.006009\n",
            "iteration 13139 : loss : 0.027517, loss_ce: 0.005061\n",
            "iteration 13140 : loss : 0.021804, loss_ce: 0.007804\n",
            "iteration 13141 : loss : 0.025786, loss_ce: 0.009911\n",
            "iteration 13142 : loss : 0.021692, loss_ce: 0.007523\n",
            "iteration 13143 : loss : 0.071518, loss_ce: 0.004557\n",
            "iteration 13144 : loss : 0.022350, loss_ce: 0.006856\n",
            "iteration 13145 : loss : 0.021134, loss_ce: 0.004707\n",
            "iteration 13146 : loss : 0.024791, loss_ce: 0.009263\n",
            "iteration 13147 : loss : 0.025512, loss_ce: 0.009443\n",
            "iteration 13148 : loss : 0.019438, loss_ce: 0.004712\n",
            "iteration 13149 : loss : 0.021693, loss_ce: 0.009613\n",
            "iteration 13150 : loss : 0.024260, loss_ce: 0.011913\n",
            "iteration 13151 : loss : 0.023406, loss_ce: 0.010469\n",
            "iteration 13152 : loss : 0.075845, loss_ce: 0.008783\n",
            "iteration 13153 : loss : 0.023084, loss_ce: 0.008003\n",
            "iteration 13154 : loss : 0.025458, loss_ce: 0.011274\n",
            "iteration 13155 : loss : 0.020970, loss_ce: 0.007176\n",
            "iteration 13156 : loss : 0.022270, loss_ce: 0.008971\n",
            "iteration 13157 : loss : 0.021184, loss_ce: 0.007257\n",
            "iteration 13158 : loss : 0.023060, loss_ce: 0.006356\n",
            "iteration 13159 : loss : 0.021981, loss_ce: 0.011205\n",
            "iteration 13160 : loss : 0.021224, loss_ce: 0.009982\n",
            "iteration 13161 : loss : 0.023351, loss_ce: 0.007652\n",
            "iteration 13162 : loss : 0.073706, loss_ce: 0.005408\n",
            "iteration 13163 : loss : 0.022914, loss_ce: 0.007906\n",
            "iteration 13164 : loss : 0.026383, loss_ce: 0.010819\n",
            "iteration 13165 : loss : 0.022220, loss_ce: 0.006318\n",
            "iteration 13166 : loss : 0.026147, loss_ce: 0.008044\n",
            "iteration 13167 : loss : 0.021913, loss_ce: 0.006853\n",
            "iteration 13168 : loss : 0.025512, loss_ce: 0.009634\n",
            "iteration 13169 : loss : 0.020173, loss_ce: 0.007770\n",
            "iteration 13170 : loss : 0.023742, loss_ce: 0.008413\n",
            "iteration 13171 : loss : 0.022143, loss_ce: 0.010033\n",
            "iteration 13172 : loss : 0.024459, loss_ce: 0.008740\n",
            "iteration 13173 : loss : 0.052333, loss_ce: 0.005714\n",
            "iteration 13174 : loss : 0.024351, loss_ce: 0.010350\n",
            "iteration 13175 : loss : 0.018982, loss_ce: 0.006433\n",
            "iteration 13176 : loss : 0.023249, loss_ce: 0.008216\n",
            "iteration 13177 : loss : 0.023701, loss_ce: 0.009487\n",
            "iteration 13178 : loss : 0.022897, loss_ce: 0.008493\n",
            "iteration 13179 : loss : 0.078146, loss_ce: 0.006301\n",
            "iteration 13180 : loss : 0.029601, loss_ce: 0.013765\n",
            "iteration 13181 : loss : 0.026563, loss_ce: 0.009086\n",
            "iteration 13182 : loss : 0.020551, loss_ce: 0.010198\n",
            "iteration 13183 : loss : 0.020853, loss_ce: 0.008316\n",
            "iteration 13184 : loss : 0.025596, loss_ce: 0.009082\n",
            "iteration 13185 : loss : 0.025310, loss_ce: 0.011687\n",
            "iteration 13186 : loss : 0.028130, loss_ce: 0.007709\n",
            "iteration 13187 : loss : 0.021518, loss_ce: 0.009255\n",
            "iteration 13188 : loss : 0.021611, loss_ce: 0.007652\n",
            "iteration 13189 : loss : 0.022124, loss_ce: 0.008017\n",
            "iteration 13190 : loss : 0.076142, loss_ce: 0.009658\n",
            "iteration 13191 : loss : 0.020585, loss_ce: 0.006438\n",
            "iteration 13192 : loss : 0.018855, loss_ce: 0.006019\n",
            "iteration 13193 : loss : 0.020297, loss_ce: 0.007763\n",
            "iteration 13194 : loss : 0.024485, loss_ce: 0.006416\n",
            "iteration 13195 : loss : 0.022339, loss_ce: 0.008852\n",
            "iteration 13196 : loss : 0.023663, loss_ce: 0.008076\n",
            "iteration 13197 : loss : 0.021592, loss_ce: 0.008507\n",
            "iteration 13198 : loss : 0.024212, loss_ce: 0.009269\n",
            "iteration 13199 : loss : 0.025413, loss_ce: 0.009434\n",
            "iteration 13200 : loss : 0.023187, loss_ce: 0.008195\n",
            "iteration 13201 : loss : 0.074378, loss_ce: 0.006449\n",
            "iteration 13202 : loss : 0.023169, loss_ce: 0.008210\n",
            "iteration 13203 : loss : 0.022122, loss_ce: 0.008731\n",
            "iteration 13204 : loss : 0.027191, loss_ce: 0.008903\n",
            "iteration 13205 : loss : 0.025894, loss_ce: 0.008011\n",
            "iteration 13206 : loss : 0.283572, loss_ce: 0.001927\n",
            " 95%|██████████████████████████▌ | 142/150 [5:10:28<17:25, 130.64s/it]iteration 13207 : loss : 0.020999, loss_ce: 0.006740\n",
            "iteration 13208 : loss : 0.022977, loss_ce: 0.008388\n",
            "iteration 13209 : loss : 0.026044, loss_ce: 0.008041\n",
            "iteration 13210 : loss : 0.025176, loss_ce: 0.010639\n",
            "iteration 13211 : loss : 0.025523, loss_ce: 0.005993\n",
            "iteration 13212 : loss : 0.017659, loss_ce: 0.006102\n",
            "iteration 13213 : loss : 0.021403, loss_ce: 0.008319\n",
            "iteration 13214 : loss : 0.024564, loss_ce: 0.008346\n",
            "iteration 13215 : loss : 0.022308, loss_ce: 0.007045\n",
            "iteration 13216 : loss : 0.020400, loss_ce: 0.005936\n",
            "iteration 13217 : loss : 0.028189, loss_ce: 0.009296\n",
            "iteration 13218 : loss : 0.024890, loss_ce: 0.005770\n",
            "iteration 13219 : loss : 0.022924, loss_ce: 0.007741\n",
            "iteration 13220 : loss : 0.022629, loss_ce: 0.008310\n",
            "iteration 13221 : loss : 0.030389, loss_ce: 0.005073\n",
            "iteration 13222 : loss : 0.025265, loss_ce: 0.010288\n",
            "iteration 13223 : loss : 0.023365, loss_ce: 0.009951\n",
            "iteration 13224 : loss : 0.020295, loss_ce: 0.006578\n",
            "iteration 13225 : loss : 0.037060, loss_ce: 0.007846\n",
            "iteration 13226 : loss : 0.026378, loss_ce: 0.012636\n",
            "iteration 13227 : loss : 0.023147, loss_ce: 0.009649\n",
            "iteration 13228 : loss : 0.024319, loss_ce: 0.008859\n",
            "iteration 13229 : loss : 0.023910, loss_ce: 0.009296\n",
            "iteration 13230 : loss : 0.021382, loss_ce: 0.007017\n",
            "iteration 13231 : loss : 0.021317, loss_ce: 0.009160\n",
            "iteration 13232 : loss : 0.020677, loss_ce: 0.007701\n",
            "iteration 13233 : loss : 0.020221, loss_ce: 0.007527\n",
            "iteration 13234 : loss : 0.032115, loss_ce: 0.006834\n",
            "iteration 13235 : loss : 0.024364, loss_ce: 0.006947\n",
            "iteration 13236 : loss : 0.022354, loss_ce: 0.009014\n",
            "iteration 13237 : loss : 0.024683, loss_ce: 0.012811\n",
            "iteration 13238 : loss : 0.018009, loss_ce: 0.006565\n",
            "iteration 13239 : loss : 0.029450, loss_ce: 0.007134\n",
            "iteration 13240 : loss : 0.021872, loss_ce: 0.007659\n",
            "iteration 13241 : loss : 0.023733, loss_ce: 0.006979\n",
            "iteration 13242 : loss : 0.026404, loss_ce: 0.011699\n",
            "iteration 13243 : loss : 0.024961, loss_ce: 0.009701\n",
            "iteration 13244 : loss : 0.025148, loss_ce: 0.008959\n",
            "iteration 13245 : loss : 0.025247, loss_ce: 0.008475\n",
            "iteration 13246 : loss : 0.074049, loss_ce: 0.003124\n",
            "iteration 13247 : loss : 0.031925, loss_ce: 0.007106\n",
            "iteration 13248 : loss : 0.022219, loss_ce: 0.006652\n",
            "iteration 13249 : loss : 0.020274, loss_ce: 0.008636\n",
            "iteration 13250 : loss : 0.032240, loss_ce: 0.009439\n",
            "iteration 13251 : loss : 0.022640, loss_ce: 0.008603\n",
            "iteration 13252 : loss : 0.021728, loss_ce: 0.008587\n",
            "iteration 13253 : loss : 0.023090, loss_ce: 0.010034\n",
            "iteration 13254 : loss : 0.019137, loss_ce: 0.006877\n",
            "iteration 13255 : loss : 0.022957, loss_ce: 0.007946\n",
            "iteration 13256 : loss : 0.024717, loss_ce: 0.011888\n",
            "iteration 13257 : loss : 0.019513, loss_ce: 0.004804\n",
            "iteration 13258 : loss : 0.024243, loss_ce: 0.005938\n",
            "iteration 13259 : loss : 0.025622, loss_ce: 0.009019\n",
            "iteration 13260 : loss : 0.025795, loss_ce: 0.009862\n",
            "iteration 13261 : loss : 0.022165, loss_ce: 0.009182\n",
            "iteration 13262 : loss : 0.030576, loss_ce: 0.008388\n",
            "iteration 13263 : loss : 0.026505, loss_ce: 0.014546\n",
            "iteration 13264 : loss : 0.025769, loss_ce: 0.008457\n",
            "iteration 13265 : loss : 0.019187, loss_ce: 0.005663\n",
            "iteration 13266 : loss : 0.023044, loss_ce: 0.008671\n",
            "iteration 13267 : loss : 0.022699, loss_ce: 0.010415\n",
            "iteration 13268 : loss : 0.023412, loss_ce: 0.010225\n",
            "iteration 13269 : loss : 0.023911, loss_ce: 0.006752\n",
            "iteration 13270 : loss : 0.022950, loss_ce: 0.007616\n",
            "iteration 13271 : loss : 0.023312, loss_ce: 0.007818\n",
            "iteration 13272 : loss : 0.020316, loss_ce: 0.006568\n",
            "iteration 13273 : loss : 0.025307, loss_ce: 0.007927\n",
            "iteration 13274 : loss : 0.026847, loss_ce: 0.010547\n",
            "iteration 13275 : loss : 0.045773, loss_ce: 0.010425\n",
            "iteration 13276 : loss : 0.024267, loss_ce: 0.009822\n",
            "iteration 13277 : loss : 0.021933, loss_ce: 0.006133\n",
            "iteration 13278 : loss : 0.026106, loss_ce: 0.008547\n",
            "iteration 13279 : loss : 0.022959, loss_ce: 0.007596\n",
            "iteration 13280 : loss : 0.023549, loss_ce: 0.006622\n",
            "iteration 13281 : loss : 0.022742, loss_ce: 0.005381\n",
            "iteration 13282 : loss : 0.074187, loss_ce: 0.007337\n",
            "iteration 13283 : loss : 0.021162, loss_ce: 0.006943\n",
            "iteration 13284 : loss : 0.021991, loss_ce: 0.008087\n",
            "iteration 13285 : loss : 0.026613, loss_ce: 0.008517\n",
            "iteration 13286 : loss : 0.025617, loss_ce: 0.004480\n",
            "iteration 13287 : loss : 0.025061, loss_ce: 0.007751\n",
            "iteration 13288 : loss : 0.023695, loss_ce: 0.006961\n",
            "iteration 13289 : loss : 0.024114, loss_ce: 0.009149\n",
            "iteration 13290 : loss : 0.033859, loss_ce: 0.005853\n",
            "iteration 13291 : loss : 0.018754, loss_ce: 0.005494\n",
            "iteration 13292 : loss : 0.024223, loss_ce: 0.007601\n",
            "iteration 13293 : loss : 0.021595, loss_ce: 0.007757\n",
            "iteration 13294 : loss : 0.021670, loss_ce: 0.004551\n",
            "iteration 13295 : loss : 0.038867, loss_ce: 0.010288\n",
            "iteration 13296 : loss : 0.018407, loss_ce: 0.006492\n",
            "iteration 13297 : loss : 0.020169, loss_ce: 0.005823\n",
            "iteration 13298 : loss : 0.019962, loss_ce: 0.006785\n",
            "iteration 13299 : loss : 0.442821, loss_ce: 0.000425\n",
            " 95%|██████████████████████████▋ | 143/150 [5:12:39<15:15, 130.74s/it]iteration 13300 : loss : 0.073633, loss_ce: 0.007519\n",
            "iteration 13301 : loss : 0.024919, loss_ce: 0.008912\n",
            "iteration 13302 : loss : 0.022972, loss_ce: 0.008983\n",
            "iteration 13303 : loss : 0.025595, loss_ce: 0.003108\n",
            "iteration 13304 : loss : 0.024926, loss_ce: 0.009179\n",
            "iteration 13305 : loss : 0.019296, loss_ce: 0.008056\n",
            "iteration 13306 : loss : 0.021442, loss_ce: 0.008747\n",
            "iteration 13307 : loss : 0.023696, loss_ce: 0.005981\n",
            "iteration 13308 : loss : 0.027086, loss_ce: 0.008360\n",
            "iteration 13309 : loss : 0.072953, loss_ce: 0.006916\n",
            "iteration 13310 : loss : 0.020329, loss_ce: 0.005320\n",
            "iteration 13311 : loss : 0.029067, loss_ce: 0.012476\n",
            "iteration 13312 : loss : 0.027872, loss_ce: 0.007645\n",
            "iteration 13313 : loss : 0.020943, loss_ce: 0.005604\n",
            "iteration 13314 : loss : 0.023004, loss_ce: 0.007834\n",
            "iteration 13315 : loss : 0.022129, loss_ce: 0.007727\n",
            "iteration 13316 : loss : 0.022777, loss_ce: 0.005728\n",
            "iteration 13317 : loss : 0.023789, loss_ce: 0.007887\n",
            "iteration 13318 : loss : 0.023673, loss_ce: 0.010026\n",
            "iteration 13319 : loss : 0.024249, loss_ce: 0.009168\n",
            "iteration 13320 : loss : 0.025684, loss_ce: 0.011327\n",
            "iteration 13321 : loss : 0.018674, loss_ce: 0.006901\n",
            "iteration 13322 : loss : 0.078563, loss_ce: 0.006131\n",
            "iteration 13323 : loss : 0.017714, loss_ce: 0.006118\n",
            "iteration 13324 : loss : 0.023337, loss_ce: 0.005379\n",
            "iteration 13325 : loss : 0.019576, loss_ce: 0.006029\n",
            "iteration 13326 : loss : 0.024889, loss_ce: 0.008326\n",
            "iteration 13327 : loss : 0.035202, loss_ce: 0.008231\n",
            "iteration 13328 : loss : 0.027234, loss_ce: 0.004657\n",
            "iteration 13329 : loss : 0.027695, loss_ce: 0.008599\n",
            "iteration 13330 : loss : 0.035112, loss_ce: 0.007960\n",
            "iteration 13331 : loss : 0.020591, loss_ce: 0.007531\n",
            "iteration 13332 : loss : 0.027278, loss_ce: 0.009653\n",
            "iteration 13333 : loss : 0.026145, loss_ce: 0.003899\n",
            "iteration 13334 : loss : 0.024135, loss_ce: 0.006095\n",
            "iteration 13335 : loss : 0.023042, loss_ce: 0.008021\n",
            "iteration 13336 : loss : 0.075999, loss_ce: 0.002773\n",
            "iteration 13337 : loss : 0.024274, loss_ce: 0.005939\n",
            "iteration 13338 : loss : 0.024894, loss_ce: 0.008254\n",
            "iteration 13339 : loss : 0.076241, loss_ce: 0.007679\n",
            "iteration 13340 : loss : 0.072007, loss_ce: 0.004506\n",
            "iteration 13341 : loss : 0.025846, loss_ce: 0.013245\n",
            "iteration 13342 : loss : 0.021944, loss_ce: 0.008227\n",
            "iteration 13343 : loss : 0.028216, loss_ce: 0.007202\n",
            "iteration 13344 : loss : 0.018751, loss_ce: 0.006652\n",
            "iteration 13345 : loss : 0.027296, loss_ce: 0.012796\n",
            "iteration 13346 : loss : 0.019520, loss_ce: 0.007800\n",
            "iteration 13347 : loss : 0.021680, loss_ce: 0.010862\n",
            "iteration 13348 : loss : 0.021060, loss_ce: 0.008766\n",
            "iteration 13349 : loss : 0.024054, loss_ce: 0.007312\n",
            "iteration 13350 : loss : 0.023943, loss_ce: 0.008323\n",
            "iteration 13351 : loss : 0.020940, loss_ce: 0.009199\n",
            "iteration 13352 : loss : 0.020166, loss_ce: 0.007913\n",
            "iteration 13353 : loss : 0.022416, loss_ce: 0.007279\n",
            "iteration 13354 : loss : 0.026872, loss_ce: 0.007656\n",
            "iteration 13355 : loss : 0.022205, loss_ce: 0.009458\n",
            "iteration 13356 : loss : 0.022122, loss_ce: 0.009138\n",
            "iteration 13357 : loss : 0.023905, loss_ce: 0.008338\n",
            "iteration 13358 : loss : 0.023279, loss_ce: 0.013296\n",
            "iteration 13359 : loss : 0.024220, loss_ce: 0.009281\n",
            "iteration 13360 : loss : 0.025270, loss_ce: 0.007785\n",
            "iteration 13361 : loss : 0.023607, loss_ce: 0.010284\n",
            "iteration 13362 : loss : 0.021424, loss_ce: 0.004909\n",
            "iteration 13363 : loss : 0.020794, loss_ce: 0.006328\n",
            "iteration 13364 : loss : 0.018434, loss_ce: 0.005422\n",
            "iteration 13365 : loss : 0.025207, loss_ce: 0.010471\n",
            "iteration 13366 : loss : 0.025916, loss_ce: 0.009552\n",
            "iteration 13367 : loss : 0.034578, loss_ce: 0.009687\n",
            "iteration 13368 : loss : 0.026354, loss_ce: 0.008914\n",
            "iteration 13369 : loss : 0.025588, loss_ce: 0.011407\n",
            "iteration 13370 : loss : 0.021117, loss_ce: 0.005752\n",
            "iteration 13371 : loss : 0.074406, loss_ce: 0.007263\n",
            "iteration 13372 : loss : 0.027668, loss_ce: 0.008988\n",
            "iteration 13373 : loss : 0.022295, loss_ce: 0.008834\n",
            "iteration 13374 : loss : 0.023884, loss_ce: 0.007033\n",
            "iteration 13375 : loss : 0.075964, loss_ce: 0.004902\n",
            "iteration 13376 : loss : 0.019525, loss_ce: 0.005388\n",
            "iteration 13377 : loss : 0.075592, loss_ce: 0.008346\n",
            "iteration 13378 : loss : 0.024638, loss_ce: 0.012288\n",
            "iteration 13379 : loss : 0.021429, loss_ce: 0.007917\n",
            "iteration 13380 : loss : 0.024329, loss_ce: 0.009868\n",
            "iteration 13381 : loss : 0.023992, loss_ce: 0.011230\n",
            "iteration 13382 : loss : 0.022543, loss_ce: 0.007724\n",
            "iteration 13383 : loss : 0.021511, loss_ce: 0.006509\n",
            "iteration 13384 : loss : 0.026237, loss_ce: 0.005025\n",
            "iteration 13385 : loss : 0.024947, loss_ce: 0.009412\n",
            "iteration 13386 : loss : 0.024177, loss_ce: 0.007639\n",
            "iteration 13387 : loss : 0.021990, loss_ce: 0.006498\n",
            "iteration 13388 : loss : 0.023960, loss_ce: 0.013064\n",
            "iteration 13389 : loss : 0.020669, loss_ce: 0.006715\n",
            "iteration 13390 : loss : 0.021532, loss_ce: 0.008925\n",
            "iteration 13391 : loss : 0.125040, loss_ce: 0.004060\n",
            "iteration 13392 : loss : 0.090196, loss_ce: 0.023719\n",
            " 96%|██████████████████████████▉ | 144/150 [5:14:49<13:03, 130.56s/it]iteration 13393 : loss : 0.019546, loss_ce: 0.006529\n",
            "iteration 13394 : loss : 0.022372, loss_ce: 0.009553\n",
            "iteration 13395 : loss : 0.024752, loss_ce: 0.008878\n",
            "iteration 13396 : loss : 0.023959, loss_ce: 0.007532\n",
            "iteration 13397 : loss : 0.029824, loss_ce: 0.004720\n",
            "iteration 13398 : loss : 0.024281, loss_ce: 0.007836\n",
            "iteration 13399 : loss : 0.077342, loss_ce: 0.006784\n",
            "iteration 13400 : loss : 0.019734, loss_ce: 0.009703\n",
            "iteration 13401 : loss : 0.022833, loss_ce: 0.008886\n",
            "iteration 13402 : loss : 0.023443, loss_ce: 0.006797\n",
            "iteration 13403 : loss : 0.022522, loss_ce: 0.009499\n",
            "iteration 13404 : loss : 0.027291, loss_ce: 0.012410\n",
            "iteration 13405 : loss : 0.022459, loss_ce: 0.006563\n",
            "iteration 13406 : loss : 0.021281, loss_ce: 0.005857\n",
            "iteration 13407 : loss : 0.023455, loss_ce: 0.006343\n",
            "iteration 13408 : loss : 0.024755, loss_ce: 0.004312\n",
            "iteration 13409 : loss : 0.025113, loss_ce: 0.008302\n",
            "iteration 13410 : loss : 0.023569, loss_ce: 0.011222\n",
            "iteration 13411 : loss : 0.026344, loss_ce: 0.006077\n",
            "iteration 13412 : loss : 0.023519, loss_ce: 0.007382\n",
            "iteration 13413 : loss : 0.024522, loss_ce: 0.010245\n",
            "iteration 13414 : loss : 0.024897, loss_ce: 0.006954\n",
            "iteration 13415 : loss : 0.021130, loss_ce: 0.007885\n",
            "iteration 13416 : loss : 0.022769, loss_ce: 0.009202\n",
            "iteration 13417 : loss : 0.023616, loss_ce: 0.009253\n",
            "iteration 13418 : loss : 0.020704, loss_ce: 0.004734\n",
            "iteration 13419 : loss : 0.020292, loss_ce: 0.006158\n",
            "iteration 13420 : loss : 0.027771, loss_ce: 0.009351\n",
            "iteration 13421 : loss : 0.027846, loss_ce: 0.014354\n",
            "iteration 13422 : loss : 0.024658, loss_ce: 0.010080\n",
            "iteration 13423 : loss : 0.019381, loss_ce: 0.005825\n",
            "iteration 13424 : loss : 0.020980, loss_ce: 0.008868\n",
            "iteration 13425 : loss : 0.025223, loss_ce: 0.010332\n",
            "iteration 13426 : loss : 0.020845, loss_ce: 0.005023\n",
            "iteration 13427 : loss : 0.020240, loss_ce: 0.007242\n",
            "iteration 13428 : loss : 0.024638, loss_ce: 0.004722\n",
            "iteration 13429 : loss : 0.020873, loss_ce: 0.008757\n",
            "iteration 13430 : loss : 0.020882, loss_ce: 0.005313\n",
            "iteration 13431 : loss : 0.022227, loss_ce: 0.006155\n",
            "iteration 13432 : loss : 0.025099, loss_ce: 0.009421\n",
            "iteration 13433 : loss : 0.019470, loss_ce: 0.006499\n",
            "iteration 13434 : loss : 0.022531, loss_ce: 0.007990\n",
            "iteration 13435 : loss : 0.024533, loss_ce: 0.009295\n",
            "iteration 13436 : loss : 0.025298, loss_ce: 0.009522\n",
            "iteration 13437 : loss : 0.020596, loss_ce: 0.008085\n",
            "iteration 13438 : loss : 0.026185, loss_ce: 0.011751\n",
            "iteration 13439 : loss : 0.019146, loss_ce: 0.005839\n",
            "iteration 13440 : loss : 0.023215, loss_ce: 0.008787\n",
            "iteration 13441 : loss : 0.021564, loss_ce: 0.009734\n",
            "iteration 13442 : loss : 0.023244, loss_ce: 0.007082\n",
            "iteration 13443 : loss : 0.028113, loss_ce: 0.008822\n",
            "iteration 13444 : loss : 0.026320, loss_ce: 0.012927\n",
            "iteration 13445 : loss : 0.023818, loss_ce: 0.007791\n",
            "iteration 13446 : loss : 0.072059, loss_ce: 0.004244\n",
            "iteration 13447 : loss : 0.021384, loss_ce: 0.006962\n",
            "iteration 13448 : loss : 0.017514, loss_ce: 0.005403\n",
            "iteration 13449 : loss : 0.026253, loss_ce: 0.008151\n",
            "iteration 13450 : loss : 0.021491, loss_ce: 0.005235\n",
            "iteration 13451 : loss : 0.024550, loss_ce: 0.009380\n",
            "iteration 13452 : loss : 0.023278, loss_ce: 0.008777\n",
            "iteration 13453 : loss : 0.036863, loss_ce: 0.005471\n",
            "iteration 13454 : loss : 0.022258, loss_ce: 0.009036\n",
            "iteration 13455 : loss : 0.023980, loss_ce: 0.010881\n",
            "iteration 13456 : loss : 0.023581, loss_ce: 0.009927\n",
            "iteration 13457 : loss : 0.022455, loss_ce: 0.007058\n",
            "iteration 13458 : loss : 0.027882, loss_ce: 0.007441\n",
            "iteration 13459 : loss : 0.027677, loss_ce: 0.011209\n",
            "iteration 13460 : loss : 0.026034, loss_ce: 0.011233\n",
            "iteration 13461 : loss : 0.019559, loss_ce: 0.006474\n",
            "iteration 13462 : loss : 0.026418, loss_ce: 0.007865\n",
            "iteration 13463 : loss : 0.021609, loss_ce: 0.006078\n",
            "iteration 13464 : loss : 0.032375, loss_ce: 0.006116\n",
            "iteration 13465 : loss : 0.018383, loss_ce: 0.005752\n",
            "iteration 13466 : loss : 0.020251, loss_ce: 0.009500\n",
            "iteration 13467 : loss : 0.024758, loss_ce: 0.008542\n",
            "iteration 13468 : loss : 0.031819, loss_ce: 0.003656\n",
            "iteration 13469 : loss : 0.071639, loss_ce: 0.004569\n",
            "iteration 13470 : loss : 0.024706, loss_ce: 0.006794\n",
            "iteration 13471 : loss : 0.020391, loss_ce: 0.005595\n",
            "iteration 13472 : loss : 0.022634, loss_ce: 0.007801\n",
            "iteration 13473 : loss : 0.021211, loss_ce: 0.011094\n",
            "iteration 13474 : loss : 0.026428, loss_ce: 0.007169\n",
            "iteration 13475 : loss : 0.020667, loss_ce: 0.007798\n",
            "iteration 13476 : loss : 0.022981, loss_ce: 0.009393\n",
            "iteration 13477 : loss : 0.021754, loss_ce: 0.010185\n",
            "iteration 13478 : loss : 0.029556, loss_ce: 0.009732\n",
            "iteration 13479 : loss : 0.025289, loss_ce: 0.006165\n",
            "iteration 13480 : loss : 0.025326, loss_ce: 0.006248\n",
            "iteration 13481 : loss : 0.029397, loss_ce: 0.006580\n",
            "iteration 13482 : loss : 0.022221, loss_ce: 0.008174\n",
            "iteration 13483 : loss : 0.072951, loss_ce: 0.007015\n",
            "iteration 13484 : loss : 0.073711, loss_ce: 0.009363\n",
            "iteration 13485 : loss : 0.239543, loss_ce: 0.019187\n",
            " 97%|███████████████████████████ | 145/150 [5:16:59<10:52, 130.51s/it]iteration 13486 : loss : 0.022038, loss_ce: 0.008900\n",
            "iteration 13487 : loss : 0.077477, loss_ce: 0.007821\n",
            "iteration 13488 : loss : 0.061379, loss_ce: 0.006818\n",
            "iteration 13489 : loss : 0.019009, loss_ce: 0.006700\n",
            "iteration 13490 : loss : 0.022809, loss_ce: 0.012277\n",
            "iteration 13491 : loss : 0.079558, loss_ce: 0.004219\n",
            "iteration 13492 : loss : 0.023745, loss_ce: 0.008475\n",
            "iteration 13493 : loss : 0.018884, loss_ce: 0.005008\n",
            "iteration 13494 : loss : 0.076503, loss_ce: 0.003871\n",
            "iteration 13495 : loss : 0.021392, loss_ce: 0.007205\n",
            "iteration 13496 : loss : 0.021341, loss_ce: 0.007181\n",
            "iteration 13497 : loss : 0.033645, loss_ce: 0.006966\n",
            "iteration 13498 : loss : 0.023381, loss_ce: 0.005133\n",
            "iteration 13499 : loss : 0.021945, loss_ce: 0.006814\n",
            "iteration 13500 : loss : 0.025303, loss_ce: 0.003782\n",
            "iteration 13501 : loss : 0.021887, loss_ce: 0.006654\n",
            "iteration 13502 : loss : 0.025711, loss_ce: 0.012186\n",
            "iteration 13503 : loss : 0.025564, loss_ce: 0.007259\n",
            "iteration 13504 : loss : 0.022325, loss_ce: 0.007452\n",
            "iteration 13505 : loss : 0.019896, loss_ce: 0.006308\n",
            "iteration 13506 : loss : 0.021248, loss_ce: 0.008385\n",
            "iteration 13507 : loss : 0.025119, loss_ce: 0.008668\n",
            "iteration 13508 : loss : 0.031553, loss_ce: 0.008067\n",
            "iteration 13509 : loss : 0.020902, loss_ce: 0.010186\n",
            "iteration 13510 : loss : 0.024193, loss_ce: 0.007378\n",
            "iteration 13511 : loss : 0.022485, loss_ce: 0.011771\n",
            "iteration 13512 : loss : 0.027452, loss_ce: 0.009430\n",
            "iteration 13513 : loss : 0.024483, loss_ce: 0.006342\n",
            "iteration 13514 : loss : 0.023302, loss_ce: 0.010929\n",
            "iteration 13515 : loss : 0.024236, loss_ce: 0.010087\n",
            "iteration 13516 : loss : 0.018128, loss_ce: 0.007345\n",
            "iteration 13517 : loss : 0.025320, loss_ce: 0.007196\n",
            "iteration 13518 : loss : 0.025638, loss_ce: 0.006706\n",
            "iteration 13519 : loss : 0.020257, loss_ce: 0.006066\n",
            "iteration 13520 : loss : 0.021495, loss_ce: 0.009255\n",
            "iteration 13521 : loss : 0.022625, loss_ce: 0.006308\n",
            "iteration 13522 : loss : 0.025225, loss_ce: 0.010385\n",
            "iteration 13523 : loss : 0.027898, loss_ce: 0.007441\n",
            "iteration 13524 : loss : 0.021281, loss_ce: 0.009379\n",
            "iteration 13525 : loss : 0.020946, loss_ce: 0.008667\n",
            "iteration 13526 : loss : 0.025307, loss_ce: 0.010508\n",
            "iteration 13527 : loss : 0.024817, loss_ce: 0.006934\n",
            "iteration 13528 : loss : 0.023746, loss_ce: 0.009144\n",
            "iteration 13529 : loss : 0.027087, loss_ce: 0.009820\n",
            "iteration 13530 : loss : 0.023800, loss_ce: 0.006425\n",
            "iteration 13531 : loss : 0.026942, loss_ce: 0.009050\n",
            "iteration 13532 : loss : 0.020985, loss_ce: 0.007256\n",
            "iteration 13533 : loss : 0.072966, loss_ce: 0.008242\n",
            "iteration 13534 : loss : 0.023498, loss_ce: 0.009969\n",
            "iteration 13535 : loss : 0.023206, loss_ce: 0.011385\n",
            "iteration 13536 : loss : 0.022808, loss_ce: 0.010456\n",
            "iteration 13537 : loss : 0.023368, loss_ce: 0.007822\n",
            "iteration 13538 : loss : 0.026338, loss_ce: 0.005054\n",
            "iteration 13539 : loss : 0.024571, loss_ce: 0.011061\n",
            "iteration 13540 : loss : 0.076742, loss_ce: 0.007490\n",
            "iteration 13541 : loss : 0.024219, loss_ce: 0.010182\n",
            "iteration 13542 : loss : 0.024354, loss_ce: 0.010176\n",
            "iteration 13543 : loss : 0.026640, loss_ce: 0.010952\n",
            "iteration 13544 : loss : 0.025049, loss_ce: 0.006685\n",
            "iteration 13545 : loss : 0.026314, loss_ce: 0.007853\n",
            "iteration 13546 : loss : 0.020104, loss_ce: 0.006069\n",
            "iteration 13547 : loss : 0.024042, loss_ce: 0.005975\n",
            "iteration 13548 : loss : 0.022331, loss_ce: 0.004745\n",
            "iteration 13549 : loss : 0.025265, loss_ce: 0.006879\n",
            "iteration 13550 : loss : 0.085445, loss_ce: 0.004847\n",
            "iteration 13551 : loss : 0.022898, loss_ce: 0.006366\n",
            "iteration 13552 : loss : 0.024920, loss_ce: 0.009325\n",
            "iteration 13553 : loss : 0.025985, loss_ce: 0.009668\n",
            "iteration 13554 : loss : 0.023740, loss_ce: 0.011176\n",
            "iteration 13555 : loss : 0.025636, loss_ce: 0.010920\n",
            "iteration 13556 : loss : 0.023794, loss_ce: 0.010236\n",
            "iteration 13557 : loss : 0.023200, loss_ce: 0.007721\n",
            "iteration 13558 : loss : 0.024419, loss_ce: 0.011127\n",
            "iteration 13559 : loss : 0.022594, loss_ce: 0.008477\n",
            "iteration 13560 : loss : 0.020806, loss_ce: 0.008372\n",
            "iteration 13561 : loss : 0.021852, loss_ce: 0.011154\n",
            "iteration 13562 : loss : 0.020940, loss_ce: 0.004549\n",
            "iteration 13563 : loss : 0.073224, loss_ce: 0.005045\n",
            "iteration 13564 : loss : 0.022227, loss_ce: 0.006671\n",
            "iteration 13565 : loss : 0.024622, loss_ce: 0.008349\n",
            "iteration 13566 : loss : 0.030474, loss_ce: 0.008118\n",
            "iteration 13567 : loss : 0.023472, loss_ce: 0.008613\n",
            "iteration 13568 : loss : 0.017517, loss_ce: 0.005278\n",
            "iteration 13569 : loss : 0.024321, loss_ce: 0.007129\n",
            "iteration 13570 : loss : 0.020522, loss_ce: 0.006619\n",
            "iteration 13571 : loss : 0.073809, loss_ce: 0.006698\n",
            "iteration 13572 : loss : 0.025694, loss_ce: 0.003621\n",
            "iteration 13573 : loss : 0.023682, loss_ce: 0.005608\n",
            "iteration 13574 : loss : 0.022390, loss_ce: 0.008312\n",
            "iteration 13575 : loss : 0.024430, loss_ce: 0.010097\n",
            "iteration 13576 : loss : 0.022586, loss_ce: 0.007638\n",
            "iteration 13577 : loss : 0.021449, loss_ce: 0.008279\n",
            "iteration 13578 : loss : 0.085012, loss_ce: 0.007019\n",
            " 97%|███████████████████████████▎| 146/150 [5:19:11<08:43, 130.92s/it]iteration 13579 : loss : 0.070057, loss_ce: 0.004463\n",
            "iteration 13580 : loss : 0.042762, loss_ce: 0.011939\n",
            "iteration 13581 : loss : 0.024978, loss_ce: 0.008455\n",
            "iteration 13582 : loss : 0.019859, loss_ce: 0.006034\n",
            "iteration 13583 : loss : 0.024175, loss_ce: 0.008240\n",
            "iteration 13584 : loss : 0.023166, loss_ce: 0.008366\n",
            "iteration 13585 : loss : 0.041792, loss_ce: 0.009925\n",
            "iteration 13586 : loss : 0.020749, loss_ce: 0.011398\n",
            "iteration 13587 : loss : 0.021828, loss_ce: 0.005023\n",
            "iteration 13588 : loss : 0.025742, loss_ce: 0.009107\n",
            "iteration 13589 : loss : 0.021962, loss_ce: 0.006894\n",
            "iteration 13590 : loss : 0.024632, loss_ce: 0.009550\n",
            "iteration 13591 : loss : 0.025546, loss_ce: 0.008946\n",
            "iteration 13592 : loss : 0.023117, loss_ce: 0.009415\n",
            "iteration 13593 : loss : 0.023355, loss_ce: 0.009718\n",
            "iteration 13594 : loss : 0.019495, loss_ce: 0.005366\n",
            "iteration 13595 : loss : 0.021966, loss_ce: 0.006725\n",
            "iteration 13596 : loss : 0.022144, loss_ce: 0.005663\n",
            "iteration 13597 : loss : 0.027428, loss_ce: 0.006892\n",
            "iteration 13598 : loss : 0.022769, loss_ce: 0.011079\n",
            "iteration 13599 : loss : 0.029750, loss_ce: 0.005891\n",
            "iteration 13600 : loss : 0.123812, loss_ce: 0.006089\n",
            "iteration 13601 : loss : 0.021942, loss_ce: 0.007782\n",
            "iteration 13602 : loss : 0.075108, loss_ce: 0.007271\n",
            "iteration 13603 : loss : 0.021587, loss_ce: 0.007773\n",
            "iteration 13604 : loss : 0.027644, loss_ce: 0.007222\n",
            "iteration 13605 : loss : 0.025477, loss_ce: 0.006471\n",
            "iteration 13606 : loss : 0.022127, loss_ce: 0.010086\n",
            "iteration 13607 : loss : 0.019571, loss_ce: 0.005173\n",
            "iteration 13608 : loss : 0.023997, loss_ce: 0.008029\n",
            "iteration 13609 : loss : 0.018759, loss_ce: 0.007930\n",
            "iteration 13610 : loss : 0.021689, loss_ce: 0.006716\n",
            "iteration 13611 : loss : 0.026158, loss_ce: 0.007690\n",
            "iteration 13612 : loss : 0.023405, loss_ce: 0.009917\n",
            "iteration 13613 : loss : 0.021285, loss_ce: 0.007983\n",
            "iteration 13614 : loss : 0.076372, loss_ce: 0.008013\n",
            "iteration 13615 : loss : 0.023140, loss_ce: 0.008014\n",
            "iteration 13616 : loss : 0.023059, loss_ce: 0.007116\n",
            "iteration 13617 : loss : 0.025434, loss_ce: 0.009260\n",
            "iteration 13618 : loss : 0.053178, loss_ce: 0.006311\n",
            "iteration 13619 : loss : 0.022338, loss_ce: 0.008258\n",
            "iteration 13620 : loss : 0.026857, loss_ce: 0.007431\n",
            "iteration 13621 : loss : 0.019498, loss_ce: 0.005688\n",
            "iteration 13622 : loss : 0.020674, loss_ce: 0.007763\n",
            "iteration 13623 : loss : 0.021840, loss_ce: 0.007241\n",
            "iteration 13624 : loss : 0.021272, loss_ce: 0.004490\n",
            "iteration 13625 : loss : 0.022575, loss_ce: 0.007813\n",
            "iteration 13626 : loss : 0.082213, loss_ce: 0.003780\n",
            "iteration 13627 : loss : 0.027360, loss_ce: 0.007847\n",
            "iteration 13628 : loss : 0.026254, loss_ce: 0.006537\n",
            "iteration 13629 : loss : 0.021206, loss_ce: 0.006451\n",
            "iteration 13630 : loss : 0.024934, loss_ce: 0.010844\n",
            "iteration 13631 : loss : 0.020982, loss_ce: 0.006716\n",
            "iteration 13632 : loss : 0.022728, loss_ce: 0.012236\n",
            "iteration 13633 : loss : 0.021559, loss_ce: 0.006804\n",
            "iteration 13634 : loss : 0.028504, loss_ce: 0.008248\n",
            "iteration 13635 : loss : 0.030481, loss_ce: 0.007744\n",
            "iteration 13636 : loss : 0.074904, loss_ce: 0.005712\n",
            "iteration 13637 : loss : 0.024405, loss_ce: 0.007758\n",
            "iteration 13638 : loss : 0.030276, loss_ce: 0.014775\n",
            "iteration 13639 : loss : 0.030743, loss_ce: 0.007869\n",
            "iteration 13640 : loss : 0.026641, loss_ce: 0.009631\n",
            "iteration 13641 : loss : 0.029472, loss_ce: 0.005309\n",
            "iteration 13642 : loss : 0.019387, loss_ce: 0.007529\n",
            "iteration 13643 : loss : 0.028914, loss_ce: 0.004405\n",
            "iteration 13644 : loss : 0.027604, loss_ce: 0.008202\n",
            "iteration 13645 : loss : 0.018411, loss_ce: 0.007500\n",
            "iteration 13646 : loss : 0.026312, loss_ce: 0.011519\n",
            "iteration 13647 : loss : 0.020609, loss_ce: 0.007109\n",
            "iteration 13648 : loss : 0.017816, loss_ce: 0.006046\n",
            "iteration 13649 : loss : 0.019188, loss_ce: 0.003413\n",
            "iteration 13650 : loss : 0.026182, loss_ce: 0.003428\n",
            "iteration 13651 : loss : 0.021078, loss_ce: 0.004293\n",
            "iteration 13652 : loss : 0.028537, loss_ce: 0.011034\n",
            "iteration 13653 : loss : 0.025441, loss_ce: 0.007724\n",
            "iteration 13654 : loss : 0.021361, loss_ce: 0.008859\n",
            "iteration 13655 : loss : 0.023157, loss_ce: 0.009470\n",
            "iteration 13656 : loss : 0.025574, loss_ce: 0.010660\n",
            "iteration 13657 : loss : 0.023023, loss_ce: 0.010296\n",
            "iteration 13658 : loss : 0.023829, loss_ce: 0.008854\n",
            "iteration 13659 : loss : 0.029247, loss_ce: 0.006310\n",
            "iteration 13660 : loss : 0.020582, loss_ce: 0.008542\n",
            "iteration 13661 : loss : 0.028033, loss_ce: 0.015577\n",
            "iteration 13662 : loss : 0.076648, loss_ce: 0.006747\n",
            "iteration 13663 : loss : 0.027489, loss_ce: 0.015121\n",
            "iteration 13664 : loss : 0.025979, loss_ce: 0.008981\n",
            "iteration 13665 : loss : 0.026533, loss_ce: 0.007371\n",
            "iteration 13666 : loss : 0.022449, loss_ce: 0.005835\n",
            "iteration 13667 : loss : 0.025095, loss_ce: 0.007910\n",
            "iteration 13668 : loss : 0.021657, loss_ce: 0.010777\n",
            "iteration 13669 : loss : 0.025530, loss_ce: 0.008925\n",
            "iteration 13670 : loss : 0.021455, loss_ce: 0.006969\n",
            "iteration 13671 : loss : 0.230763, loss_ce: 0.008823\n",
            " 98%|███████████████████████████▍| 147/150 [5:21:21<06:31, 130.59s/it]iteration 13672 : loss : 0.018350, loss_ce: 0.005977\n",
            "iteration 13673 : loss : 0.071399, loss_ce: 0.005138\n",
            "iteration 13674 : loss : 0.026521, loss_ce: 0.007347\n",
            "iteration 13675 : loss : 0.022680, loss_ce: 0.011199\n",
            "iteration 13676 : loss : 0.022947, loss_ce: 0.010579\n",
            "iteration 13677 : loss : 0.035566, loss_ce: 0.005459\n",
            "iteration 13678 : loss : 0.024182, loss_ce: 0.008904\n",
            "iteration 13679 : loss : 0.019892, loss_ce: 0.006759\n",
            "iteration 13680 : loss : 0.025077, loss_ce: 0.007984\n",
            "iteration 13681 : loss : 0.022382, loss_ce: 0.010354\n",
            "iteration 13682 : loss : 0.074335, loss_ce: 0.008104\n",
            "iteration 13683 : loss : 0.025099, loss_ce: 0.005065\n",
            "iteration 13684 : loss : 0.022675, loss_ce: 0.007983\n",
            "iteration 13685 : loss : 0.028601, loss_ce: 0.007515\n",
            "iteration 13686 : loss : 0.026725, loss_ce: 0.004711\n",
            "iteration 13687 : loss : 0.022883, loss_ce: 0.009622\n",
            "iteration 13688 : loss : 0.021908, loss_ce: 0.007936\n",
            "iteration 13689 : loss : 0.021082, loss_ce: 0.007379\n",
            "iteration 13690 : loss : 0.024263, loss_ce: 0.006434\n",
            "iteration 13691 : loss : 0.022808, loss_ce: 0.010202\n",
            "iteration 13692 : loss : 0.022983, loss_ce: 0.006771\n",
            "iteration 13693 : loss : 0.020890, loss_ce: 0.009900\n",
            "iteration 13694 : loss : 0.027098, loss_ce: 0.005871\n",
            "iteration 13695 : loss : 0.019485, loss_ce: 0.005555\n",
            "iteration 13696 : loss : 0.027309, loss_ce: 0.007200\n",
            "iteration 13697 : loss : 0.027337, loss_ce: 0.012606\n",
            "iteration 13698 : loss : 0.024448, loss_ce: 0.006256\n",
            "iteration 13699 : loss : 0.025875, loss_ce: 0.011045\n",
            "iteration 13700 : loss : 0.035139, loss_ce: 0.008933\n",
            "iteration 13701 : loss : 0.022640, loss_ce: 0.008800\n",
            "iteration 13702 : loss : 0.019992, loss_ce: 0.005907\n",
            "iteration 13703 : loss : 0.025225, loss_ce: 0.005682\n",
            "iteration 13704 : loss : 0.019821, loss_ce: 0.008242\n",
            "iteration 13705 : loss : 0.026763, loss_ce: 0.006780\n",
            "iteration 13706 : loss : 0.022419, loss_ce: 0.007448\n",
            "iteration 13707 : loss : 0.021193, loss_ce: 0.005032\n",
            "iteration 13708 : loss : 0.023625, loss_ce: 0.011428\n",
            "iteration 13709 : loss : 0.023846, loss_ce: 0.005828\n",
            "iteration 13710 : loss : 0.024492, loss_ce: 0.007564\n",
            "iteration 13711 : loss : 0.024993, loss_ce: 0.011951\n",
            "iteration 13712 : loss : 0.024377, loss_ce: 0.008047\n",
            "iteration 13713 : loss : 0.027348, loss_ce: 0.007785\n",
            "iteration 13714 : loss : 0.020961, loss_ce: 0.004625\n",
            "iteration 13715 : loss : 0.025663, loss_ce: 0.010214\n",
            "iteration 13716 : loss : 0.022451, loss_ce: 0.006741\n",
            "iteration 13717 : loss : 0.025651, loss_ce: 0.008901\n",
            "iteration 13718 : loss : 0.021473, loss_ce: 0.007741\n",
            "iteration 13719 : loss : 0.021081, loss_ce: 0.005179\n",
            "iteration 13720 : loss : 0.019885, loss_ce: 0.004940\n",
            "iteration 13721 : loss : 0.039586, loss_ce: 0.005817\n",
            "iteration 13722 : loss : 0.020441, loss_ce: 0.006704\n",
            "iteration 13723 : loss : 0.044406, loss_ce: 0.009295\n",
            "iteration 13724 : loss : 0.079364, loss_ce: 0.005707\n",
            "iteration 13725 : loss : 0.026754, loss_ce: 0.008867\n",
            "iteration 13726 : loss : 0.018867, loss_ce: 0.006070\n",
            "iteration 13727 : loss : 0.025134, loss_ce: 0.007311\n",
            "iteration 13728 : loss : 0.022734, loss_ce: 0.009818\n",
            "iteration 13729 : loss : 0.022841, loss_ce: 0.010733\n",
            "iteration 13730 : loss : 0.025779, loss_ce: 0.006945\n",
            "iteration 13731 : loss : 0.027982, loss_ce: 0.010243\n",
            "iteration 13732 : loss : 0.023773, loss_ce: 0.004152\n",
            "iteration 13733 : loss : 0.021557, loss_ce: 0.009029\n",
            "iteration 13734 : loss : 0.026747, loss_ce: 0.010000\n",
            "iteration 13735 : loss : 0.019711, loss_ce: 0.008088\n",
            "iteration 13736 : loss : 0.026862, loss_ce: 0.007230\n",
            "iteration 13737 : loss : 0.025042, loss_ce: 0.005448\n",
            "iteration 13738 : loss : 0.020531, loss_ce: 0.004585\n",
            "iteration 13739 : loss : 0.022514, loss_ce: 0.012241\n",
            "iteration 13740 : loss : 0.029582, loss_ce: 0.006095\n",
            "iteration 13741 : loss : 0.022526, loss_ce: 0.006489\n",
            "iteration 13742 : loss : 0.023566, loss_ce: 0.009756\n",
            "iteration 13743 : loss : 0.080098, loss_ce: 0.009149\n",
            "iteration 13744 : loss : 0.021906, loss_ce: 0.009219\n",
            "iteration 13745 : loss : 0.022542, loss_ce: 0.003994\n",
            "iteration 13746 : loss : 0.021798, loss_ce: 0.009257\n",
            "iteration 13747 : loss : 0.022437, loss_ce: 0.006823\n",
            "iteration 13748 : loss : 0.018659, loss_ce: 0.006417\n",
            "iteration 13749 : loss : 0.022156, loss_ce: 0.008070\n",
            "iteration 13750 : loss : 0.020779, loss_ce: 0.008536\n",
            "iteration 13751 : loss : 0.025766, loss_ce: 0.008860\n",
            "iteration 13752 : loss : 0.024491, loss_ce: 0.009245\n",
            "iteration 13753 : loss : 0.024743, loss_ce: 0.013214\n",
            "iteration 13754 : loss : 0.025623, loss_ce: 0.010472\n",
            "iteration 13755 : loss : 0.022429, loss_ce: 0.006842\n",
            "iteration 13756 : loss : 0.025528, loss_ce: 0.010408\n",
            "iteration 13757 : loss : 0.020471, loss_ce: 0.007322\n",
            "iteration 13758 : loss : 0.024430, loss_ce: 0.009660\n",
            "iteration 13759 : loss : 0.025268, loss_ce: 0.009055\n",
            "iteration 13760 : loss : 0.023074, loss_ce: 0.010366\n",
            "iteration 13761 : loss : 0.024582, loss_ce: 0.008868\n",
            "iteration 13762 : loss : 0.025547, loss_ce: 0.008570\n",
            "iteration 13763 : loss : 0.025209, loss_ce: 0.007507\n",
            "iteration 13764 : loss : 0.089671, loss_ce: 0.016282\n",
            " 99%|███████████████████████████▋| 148/150 [5:23:31<04:20, 130.46s/it]iteration 13765 : loss : 0.022312, loss_ce: 0.005815\n",
            "iteration 13766 : loss : 0.026102, loss_ce: 0.007926\n",
            "iteration 13767 : loss : 0.076460, loss_ce: 0.006033\n",
            "iteration 13768 : loss : 0.028022, loss_ce: 0.015151\n",
            "iteration 13769 : loss : 0.025131, loss_ce: 0.010182\n",
            "iteration 13770 : loss : 0.022306, loss_ce: 0.009405\n",
            "iteration 13771 : loss : 0.021228, loss_ce: 0.005106\n",
            "iteration 13772 : loss : 0.022553, loss_ce: 0.006806\n",
            "iteration 13773 : loss : 0.021280, loss_ce: 0.007618\n",
            "iteration 13774 : loss : 0.021252, loss_ce: 0.007810\n",
            "iteration 13775 : loss : 0.125377, loss_ce: 0.004246\n",
            "iteration 13776 : loss : 0.035027, loss_ce: 0.010397\n",
            "iteration 13777 : loss : 0.025764, loss_ce: 0.008270\n",
            "iteration 13778 : loss : 0.023118, loss_ce: 0.009110\n",
            "iteration 13779 : loss : 0.023185, loss_ce: 0.005667\n",
            "iteration 13780 : loss : 0.022611, loss_ce: 0.010915\n",
            "iteration 13781 : loss : 0.023809, loss_ce: 0.008641\n",
            "iteration 13782 : loss : 0.023866, loss_ce: 0.007747\n",
            "iteration 13783 : loss : 0.024510, loss_ce: 0.010465\n",
            "iteration 13784 : loss : 0.021507, loss_ce: 0.008902\n",
            "iteration 13785 : loss : 0.018403, loss_ce: 0.004086\n",
            "iteration 13786 : loss : 0.019822, loss_ce: 0.006622\n",
            "iteration 13787 : loss : 0.026975, loss_ce: 0.006765\n",
            "iteration 13788 : loss : 0.031577, loss_ce: 0.012395\n",
            "iteration 13789 : loss : 0.020317, loss_ce: 0.007261\n",
            "iteration 13790 : loss : 0.024214, loss_ce: 0.006321\n",
            "iteration 13791 : loss : 0.017945, loss_ce: 0.005945\n",
            "iteration 13792 : loss : 0.022742, loss_ce: 0.008338\n",
            "iteration 13793 : loss : 0.023969, loss_ce: 0.009068\n",
            "iteration 13794 : loss : 0.028262, loss_ce: 0.006302\n",
            "iteration 13795 : loss : 0.025579, loss_ce: 0.009276\n",
            "iteration 13796 : loss : 0.021590, loss_ce: 0.007058\n",
            "iteration 13797 : loss : 0.020954, loss_ce: 0.004464\n",
            "iteration 13798 : loss : 0.025381, loss_ce: 0.008414\n",
            "iteration 13799 : loss : 0.021872, loss_ce: 0.008004\n",
            "iteration 13800 : loss : 0.019938, loss_ce: 0.007280\n",
            "iteration 13801 : loss : 0.021558, loss_ce: 0.007680\n",
            "iteration 13802 : loss : 0.018986, loss_ce: 0.004756\n",
            "iteration 13803 : loss : 0.020013, loss_ce: 0.008810\n",
            "iteration 13804 : loss : 0.027336, loss_ce: 0.012335\n",
            "iteration 13805 : loss : 0.026251, loss_ce: 0.006068\n",
            "iteration 13806 : loss : 0.024264, loss_ce: 0.008202\n",
            "iteration 13807 : loss : 0.022271, loss_ce: 0.009371\n",
            "iteration 13808 : loss : 0.020859, loss_ce: 0.007741\n",
            "iteration 13809 : loss : 0.025936, loss_ce: 0.007455\n",
            "iteration 13810 : loss : 0.019873, loss_ce: 0.006077\n",
            "iteration 13811 : loss : 0.023230, loss_ce: 0.007027\n",
            "iteration 13812 : loss : 0.023083, loss_ce: 0.010924\n",
            "iteration 13813 : loss : 0.025117, loss_ce: 0.007239\n",
            "iteration 13814 : loss : 0.028657, loss_ce: 0.012869\n",
            "iteration 13815 : loss : 0.025622, loss_ce: 0.006639\n",
            "iteration 13816 : loss : 0.022635, loss_ce: 0.007604\n",
            "iteration 13817 : loss : 0.022820, loss_ce: 0.009346\n",
            "iteration 13818 : loss : 0.018366, loss_ce: 0.004301\n",
            "iteration 13819 : loss : 0.021333, loss_ce: 0.008122\n",
            "iteration 13820 : loss : 0.020974, loss_ce: 0.005671\n",
            "iteration 13821 : loss : 0.024146, loss_ce: 0.008622\n",
            "iteration 13822 : loss : 0.018559, loss_ce: 0.003958\n",
            "iteration 13823 : loss : 0.023156, loss_ce: 0.007648\n",
            "iteration 13824 : loss : 0.074668, loss_ce: 0.008164\n",
            "iteration 13825 : loss : 0.025673, loss_ce: 0.009383\n",
            "iteration 13826 : loss : 0.072161, loss_ce: 0.003734\n",
            "iteration 13827 : loss : 0.019765, loss_ce: 0.004119\n",
            "iteration 13828 : loss : 0.025545, loss_ce: 0.007681\n",
            "iteration 13829 : loss : 0.021184, loss_ce: 0.008047\n",
            "iteration 13830 : loss : 0.027118, loss_ce: 0.010110\n",
            "iteration 13831 : loss : 0.025652, loss_ce: 0.012406\n",
            "iteration 13832 : loss : 0.026297, loss_ce: 0.012156\n",
            "iteration 13833 : loss : 0.020276, loss_ce: 0.006198\n",
            "iteration 13834 : loss : 0.022469, loss_ce: 0.010330\n",
            "iteration 13835 : loss : 0.073373, loss_ce: 0.003331\n",
            "iteration 13836 : loss : 0.021493, loss_ce: 0.007360\n",
            "iteration 13837 : loss : 0.025020, loss_ce: 0.007419\n",
            "iteration 13838 : loss : 0.022106, loss_ce: 0.007654\n",
            "iteration 13839 : loss : 0.024194, loss_ce: 0.012001\n",
            "iteration 13840 : loss : 0.023113, loss_ce: 0.007606\n",
            "iteration 13841 : loss : 0.022800, loss_ce: 0.007063\n",
            "iteration 13842 : loss : 0.076060, loss_ce: 0.008194\n",
            "iteration 13843 : loss : 0.022636, loss_ce: 0.004360\n",
            "iteration 13844 : loss : 0.022482, loss_ce: 0.008188\n",
            "iteration 13845 : loss : 0.021528, loss_ce: 0.007018\n",
            "iteration 13846 : loss : 0.025896, loss_ce: 0.008147\n",
            "iteration 13847 : loss : 0.028903, loss_ce: 0.014999\n",
            "iteration 13848 : loss : 0.025593, loss_ce: 0.008440\n",
            "iteration 13849 : loss : 0.021905, loss_ce: 0.007766\n",
            "iteration 13850 : loss : 0.020250, loss_ce: 0.008011\n",
            "iteration 13851 : loss : 0.023121, loss_ce: 0.010547\n",
            "iteration 13852 : loss : 0.022241, loss_ce: 0.010341\n",
            "iteration 13853 : loss : 0.020302, loss_ce: 0.005606\n",
            "iteration 13854 : loss : 0.073303, loss_ce: 0.004015\n",
            "iteration 13855 : loss : 0.023352, loss_ce: 0.009978\n",
            "iteration 13856 : loss : 0.029127, loss_ce: 0.006225\n",
            "iteration 13857 : loss : 0.172388, loss_ce: 0.005220\n",
            " 99%|███████████████████████████▊| 149/150 [5:25:43<02:10, 130.84s/it]iteration 13858 : loss : 0.019467, loss_ce: 0.004683\n",
            "iteration 13859 : loss : 0.022453, loss_ce: 0.008878\n",
            "iteration 13860 : loss : 0.073517, loss_ce: 0.003768\n",
            "iteration 13861 : loss : 0.019411, loss_ce: 0.008457\n",
            "iteration 13862 : loss : 0.019699, loss_ce: 0.006140\n",
            "iteration 13863 : loss : 0.022283, loss_ce: 0.007930\n",
            "iteration 13864 : loss : 0.023636, loss_ce: 0.007490\n",
            "iteration 13865 : loss : 0.021905, loss_ce: 0.007387\n",
            "iteration 13866 : loss : 0.025223, loss_ce: 0.009242\n",
            "iteration 13867 : loss : 0.027933, loss_ce: 0.007580\n",
            "iteration 13868 : loss : 0.043566, loss_ce: 0.009387\n",
            "iteration 13869 : loss : 0.024520, loss_ce: 0.007083\n",
            "iteration 13870 : loss : 0.021446, loss_ce: 0.006349\n",
            "iteration 13871 : loss : 0.023091, loss_ce: 0.009327\n",
            "iteration 13872 : loss : 0.022757, loss_ce: 0.009760\n",
            "iteration 13873 : loss : 0.025953, loss_ce: 0.006724\n",
            "iteration 13874 : loss : 0.020640, loss_ce: 0.006751\n",
            "iteration 13875 : loss : 0.022474, loss_ce: 0.006751\n",
            "iteration 13876 : loss : 0.028220, loss_ce: 0.006723\n",
            "iteration 13877 : loss : 0.024144, loss_ce: 0.011766\n",
            "iteration 13878 : loss : 0.023161, loss_ce: 0.009176\n",
            "iteration 13879 : loss : 0.019546, loss_ce: 0.007161\n",
            "iteration 13880 : loss : 0.026037, loss_ce: 0.009491\n",
            "iteration 13881 : loss : 0.019969, loss_ce: 0.005694\n",
            "iteration 13882 : loss : 0.021317, loss_ce: 0.007864\n",
            "iteration 13883 : loss : 0.023933, loss_ce: 0.009281\n",
            "iteration 13884 : loss : 0.022619, loss_ce: 0.010228\n",
            "iteration 13885 : loss : 0.043639, loss_ce: 0.007829\n",
            "iteration 13886 : loss : 0.026334, loss_ce: 0.007950\n",
            "iteration 13887 : loss : 0.022227, loss_ce: 0.007419\n",
            "iteration 13888 : loss : 0.020154, loss_ce: 0.004881\n",
            "iteration 13889 : loss : 0.024144, loss_ce: 0.006992\n",
            "iteration 13890 : loss : 0.024029, loss_ce: 0.009463\n",
            "iteration 13891 : loss : 0.018539, loss_ce: 0.006560\n",
            "iteration 13892 : loss : 0.026278, loss_ce: 0.006187\n",
            "iteration 13893 : loss : 0.022099, loss_ce: 0.007463\n",
            "iteration 13894 : loss : 0.022824, loss_ce: 0.007349\n",
            "iteration 13895 : loss : 0.022501, loss_ce: 0.005973\n",
            "iteration 13896 : loss : 0.025148, loss_ce: 0.008115\n",
            "iteration 13897 : loss : 0.023320, loss_ce: 0.011751\n",
            "iteration 13898 : loss : 0.022678, loss_ce: 0.009798\n",
            "iteration 13899 : loss : 0.022130, loss_ce: 0.009533\n",
            "iteration 13900 : loss : 0.023286, loss_ce: 0.007241\n",
            "iteration 13901 : loss : 0.022348, loss_ce: 0.008925\n",
            "iteration 13902 : loss : 0.021919, loss_ce: 0.004970\n",
            "iteration 13903 : loss : 0.025590, loss_ce: 0.009929\n",
            "iteration 13904 : loss : 0.020522, loss_ce: 0.003636\n",
            "iteration 13905 : loss : 0.029014, loss_ce: 0.007407\n",
            "iteration 13906 : loss : 0.023825, loss_ce: 0.007613\n",
            "iteration 13907 : loss : 0.018447, loss_ce: 0.005850\n",
            "iteration 13908 : loss : 0.017314, loss_ce: 0.004997\n",
            "iteration 13909 : loss : 0.020428, loss_ce: 0.008796\n",
            "iteration 13910 : loss : 0.017081, loss_ce: 0.004652\n",
            "iteration 13911 : loss : 0.074662, loss_ce: 0.004653\n",
            "iteration 13912 : loss : 0.073411, loss_ce: 0.006518\n",
            "iteration 13913 : loss : 0.025564, loss_ce: 0.007206\n",
            "iteration 13914 : loss : 0.069690, loss_ce: 0.002628\n",
            "iteration 13915 : loss : 0.031996, loss_ce: 0.005048\n",
            "iteration 13916 : loss : 0.023647, loss_ce: 0.012283\n",
            "iteration 13917 : loss : 0.032715, loss_ce: 0.010983\n",
            "iteration 13918 : loss : 0.022715, loss_ce: 0.008814\n",
            "iteration 13919 : loss : 0.025401, loss_ce: 0.006952\n",
            "iteration 13920 : loss : 0.026061, loss_ce: 0.011755\n",
            "iteration 13921 : loss : 0.020074, loss_ce: 0.006831\n",
            "iteration 13922 : loss : 0.074774, loss_ce: 0.009048\n",
            "iteration 13923 : loss : 0.025611, loss_ce: 0.012848\n",
            "iteration 13924 : loss : 0.021978, loss_ce: 0.008241\n",
            "iteration 13925 : loss : 0.071847, loss_ce: 0.006275\n",
            "iteration 13926 : loss : 0.025334, loss_ce: 0.011392\n",
            "iteration 13927 : loss : 0.022945, loss_ce: 0.007950\n",
            "iteration 13928 : loss : 0.021657, loss_ce: 0.010201\n",
            "iteration 13929 : loss : 0.023543, loss_ce: 0.010951\n",
            "iteration 13930 : loss : 0.026375, loss_ce: 0.007045\n",
            "iteration 13931 : loss : 0.020878, loss_ce: 0.006371\n",
            "iteration 13932 : loss : 0.022074, loss_ce: 0.007688\n",
            "iteration 13933 : loss : 0.022991, loss_ce: 0.006033\n",
            "iteration 13934 : loss : 0.078894, loss_ce: 0.009822\n",
            "iteration 13935 : loss : 0.023846, loss_ce: 0.013598\n",
            "iteration 13936 : loss : 0.022399, loss_ce: 0.013009\n",
            "iteration 13937 : loss : 0.024194, loss_ce: 0.009088\n",
            "iteration 13938 : loss : 0.021237, loss_ce: 0.007839\n",
            "iteration 13939 : loss : 0.025240, loss_ce: 0.007216\n",
            "iteration 13940 : loss : 0.078183, loss_ce: 0.008360\n",
            "iteration 13941 : loss : 0.028413, loss_ce: 0.009421\n",
            "iteration 13942 : loss : 0.019305, loss_ce: 0.006569\n",
            "iteration 13943 : loss : 0.074728, loss_ce: 0.008495\n",
            "iteration 13944 : loss : 0.023706, loss_ce: 0.008154\n",
            "iteration 13945 : loss : 0.020558, loss_ce: 0.007719\n",
            "iteration 13946 : loss : 0.036051, loss_ce: 0.008010\n",
            "iteration 13947 : loss : 0.024307, loss_ce: 0.005790\n",
            "iteration 13948 : loss : 0.021413, loss_ce: 0.004979\n",
            "iteration 13949 : loss : 0.037169, loss_ce: 0.005722\n",
            "iteration 13950 : loss : 0.231811, loss_ce: 0.008815\n",
            "save model to ../model/TU_Synapse224/TU_pretrain_R50-ViT-B_16_skip3_epo150_bs24_224/epoch_149.pth\n",
            "save model to ../model/TU_Synapse224/TU_pretrain_R50-ViT-B_16_skip3_epo150_bs24_224/epoch_149.pth\n",
            " 99%|███████████████████████████▊| 149/150 [5:27:55<02:12, 132.05s/it]\n"
          ]
        }
      ],
      "source": [
        "# Full training\n",
        "# Takes ~120s per epoch\n",
        "!CUDA_VISIBLE_DEVICES=0 python train.py --dataset Synapse --vit_name R50-ViT-B_16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alZCbXd-4jIW"
      },
      "outputs": [],
      "source": [
        "# Partial training over 10 epochs\n",
        "# !CUDA_VISIBLE_DEVICES=0 python train.py --dataset Synapse --vit_name R50-ViT-B_16 --max_epochs 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8S0fp3oyhFh"
      },
      "source": [
        "# Testing the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uWeka31ymnx",
        "outputId": "afcd43b5-bbc7-455b-c479-127d2194ad94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(volume_path='../data/Synapse/test_vol_h5', dataset='Synapse', num_classes=9, list_dir='./lists/lists_Synapse', max_iterations=20000, max_epochs=150, batch_size=24, img_size=224, is_savenii=False, n_skip=3, vit_name='R50-ViT-B_16', test_save_dir='../predictions', deterministic=1, base_lr=0.01, seed=1234, vit_patches_size=16, Dataset=<class 'datasets.dataset_synapse.Synapse_dataset'>, z_spacing=1, is_pretrain=True, exp='TU_Synapse224')\n",
            "TU_pretrain_R50-ViT-B_16_skip3_epo150_bs24_224\n",
            "12 test iterations per epoch\n",
            "0it [00:00, ?it/s]idx 0 case case0008 mean_dice 0.637608 mean_hd95 15.919877\n",
            "1it [02:26, 146.47s/it]idx 1 case case0022 mean_dice 0.896007 mean_hd95 4.763628\n",
            "2it [03:48, 108.66s/it]idx 2 case case0038 mean_dice 0.708595 mean_hd95 57.092045\n",
            "3it [05:18, 100.29s/it]idx 3 case case0036 mean_dice 0.826432 mean_hd95 10.946298\n",
            "4it [08:24, 133.91s/it]idx 4 case case0032 mean_dice 0.791814 mean_hd95 16.591825\n",
            "5it [10:40, 134.78s/it]idx 5 case case0002 mean_dice 0.869098 mean_hd95 5.842838\n",
            "6it [12:45, 131.52s/it]idx 6 case case0029 mean_dice 0.659001 mean_hd95 41.228967\n",
            "7it [14:12, 116.70s/it]idx 7 case case0003 mean_dice 0.559990 mean_hd95 115.845497\n",
            "8it [17:27, 141.73s/it]idx 8 case case0001 mean_dice 0.729560 mean_hd95 42.934377\n",
            "9it [19:44, 140.25s/it]idx 9 case case0004 mean_dice 0.746282 mean_hd95 31.836942\n",
            "10it [21:52, 136.52s/it]idx 10 case case0025 mean_dice 0.753442 mean_hd95 49.402512\n",
            "11it [23:07, 117.61s/it]idx 11 case case0035 mean_dice 0.891983 mean_hd95 3.212520\n",
            "12it [24:20, 121.72s/it]\n",
            "Mean class 1 mean_dice 0.864887 mean_hd95 15.941857\n",
            "Mean class 2 mean_dice 0.593769 mean_hd95 49.221256\n",
            "Mean class 3 mean_dice 0.802309 mean_hd95 53.142331\n",
            "Mean class 4 mean_dice 0.712529 mean_hd95 24.471064\n",
            "Mean class 5 mean_dice 0.937578 mean_hd95 25.631345\n",
            "Mean class 6 mean_dice 0.557004 mean_hd95 13.126641\n",
            "Mean class 7 mean_dice 0.827775 mean_hd95 65.123979\n",
            "Mean class 8 mean_dice 0.750690 mean_hd95 17.086411\n",
            "Testing performance in best val model: mean_dice : 0.755818 mean_hd95 : 32.968110\n"
          ]
        }
      ],
      "source": [
        "!python test.py --dataset Synapse --vit_name R50-ViT-B_16 --max_epochs 150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-RojrXT-lxV"
      },
      "outputs": [],
      "source": [
        "# Testing the partialy trained network\n",
        "# !python test.py --dataset Synapse --vit_name R50-ViT-B_16 --max_epochs 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "2wqcQh90d5NM",
        "outputId": "45adcf30-e0b2-4fa2-bbe9-26bd3fd0ab3c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_abf20067-b732-4849-8820-a707ac74a958\", \"TU_pretrain_R50-ViT-B_16_skip3_epo150_bs24_224.txt\", 2090)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Download log files\n",
        "\n",
        "from google.colab import files\n",
        "# files.download('/content/model/TU_Synapse224/TU_pretrain_R50-ViT-B_16_skip3_epo150_bs24_224/log.txt')\n",
        "files.download('/content/TransUNet/test_log/test_log_TU_Synapse224/TU_pretrain_R50-ViT-B_16_skip3_epo150_bs24_224.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "9oe4NTWVSnAW",
        "outputId": "9e584f49-1f68-473f-fc47-be6a1dbf7ace"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_99129303-9018-4e43-9fbd-34cb70573741\", \"epoch_149.pth\", 421284933)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "files.download('/content/model/TU_Synapse224/TU_pretrain_R50-ViT-B_16_skip3_epo150_bs24_224/epoch_149.pth')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
